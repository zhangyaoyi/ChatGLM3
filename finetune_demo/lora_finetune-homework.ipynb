{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "id": "89b89f64d8f8053d",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB及以上（推荐使用30系或A10等sm80架构以上的NVIDIA显卡进行尝试）\n",
    "内存：16GB\n",
    "RAM: 2.9 /16 GB\n",
    "GPU RAM: 15.5/16.0 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "id": "a7bd9a514ed09ea6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 0. 环境检查\n",
    "首先，先检查代码的运行地址，确保运行地址处于 `finetune_demo` 中。\n",
    "并且，确保已经安装了 `requirements.txt`中的依赖。\n",
    "\n",
    "> 本 demo 中，不需要使用 deepspeed, mpi4py 两个依赖，如果您安装这两个依赖遇到问题，可以不安装这两个依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7703109d1443346",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/overman/Workspace/ChatGLM3/finetune_demo\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50e92810011977",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "cellView": "form",
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "id": "a1b7a99923349056",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c87410a24d844f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17c87410a24d844f",
    "outputId": "e347fc7d-875e-40c9-c682-3e064100476b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:00<00:00,  7.14it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.0312\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 1764308.97 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 727611.10 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 707536.70 examples/s]\n",
      "Map (num_proc=16): 100%|█████| 114599/114599 [00:01<00:00, 100790.44 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 3365.20 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 3221.37 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 68,760\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.8336, 'grad_norm': 2.093691825866699, 'learning_rate': 4.999272833042466e-05, 'epoch': 0.0}\n",
      "{'loss': 4.5891, 'grad_norm': 2.917391300201416, 'learning_rate': 4.9985456660849336e-05, 'epoch': 0.0}\n",
      "{'loss': 4.3961, 'grad_norm': 3.1664488315582275, 'learning_rate': 4.9978184991273996e-05, 'epoch': 0.0}\n",
      "{'loss': 4.2402, 'grad_norm': 2.6534783840179443, 'learning_rate': 4.997091332169866e-05, 'epoch': 0.0}\n",
      "{'loss': 3.9445, 'grad_norm': 2.8851776123046875, 'learning_rate': 4.996364165212333e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8852, 'grad_norm': 2.815131664276123, 'learning_rate': 4.9956369982548e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7463, 'grad_norm': 2.8149569034576416, 'learning_rate': 4.994909831297266e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7396, 'grad_norm': 2.8513388633728027, 'learning_rate': 4.9941826643397324e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7693, 'grad_norm': 2.775526285171509, 'learning_rate': 4.993455497382199e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7563, 'grad_norm': 3.1939644813537598, 'learning_rate': 4.992728330424666e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7139, 'grad_norm': 3.4346749782562256, 'learning_rate': 4.992001163467132e-05, 'epoch': 0.0}\n",
      "{'loss': 3.749, 'grad_norm': 3.1055526733398438, 'learning_rate': 4.991273996509599e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7299, 'grad_norm': 3.4363486766815186, 'learning_rate': 4.990546829552065e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6309, 'grad_norm': 4.114476203918457, 'learning_rate': 4.989819662594532e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6055, 'grad_norm': 4.003161430358887, 'learning_rate': 4.9890924956369985e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6156, 'grad_norm': 3.866570472717285, 'learning_rate': 4.988365328679465e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5791, 'grad_norm': 3.8479714393615723, 'learning_rate': 4.987638161721931e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7318, 'grad_norm': 3.547825574874878, 'learning_rate': 4.986910994764398e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5137, 'grad_norm': 7.133440017700195, 'learning_rate': 4.9861838278068646e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5426, 'grad_norm': 4.693489074707031, 'learning_rate': 4.985456660849331e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5986, 'grad_norm': 4.561092376708984, 'learning_rate': 4.984729493891797e-05, 'epoch': 0.01}\n",
      "{'loss': 3.668, 'grad_norm': 4.150888919830322, 'learning_rate': 4.984002326934265e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6246, 'grad_norm': 4.912140369415283, 'learning_rate': 4.983275159976731e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5682, 'grad_norm': 5.232517719268799, 'learning_rate': 4.9825479930191974e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5002, 'grad_norm': 4.3241353034973145, 'learning_rate': 4.981820826061664e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6279, 'grad_norm': 5.068346977233887, 'learning_rate': 4.981093659104131e-05, 'epoch': 0.01}\n",
      "{'loss': 3.525, 'grad_norm': 5.463093280792236, 'learning_rate': 4.980366492146597e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4777, 'grad_norm': 4.929694175720215, 'learning_rate': 4.979639325189064e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6096, 'grad_norm': 5.236003398895264, 'learning_rate': 4.97891215823153e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3771, 'grad_norm': 4.920596599578857, 'learning_rate': 4.978184991273997e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5924, 'grad_norm': 5.2729949951171875, 'learning_rate': 4.9774578243164635e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5076, 'grad_norm': 4.907198905944824, 'learning_rate': 4.97673065735893e-05, 'epoch': 0.01}\n",
      "{'loss': 3.65, 'grad_norm': 4.722230434417725, 'learning_rate': 4.976003490401396e-05, 'epoch': 0.01}\n",
      "{'loss': 3.599, 'grad_norm': 5.0793890953063965, 'learning_rate': 4.975276323443863e-05, 'epoch': 0.01}\n",
      "{'loss': 3.523, 'grad_norm': 5.0796613693237305, 'learning_rate': 4.9745491564863296e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4789, 'grad_norm': 5.548161029815674, 'learning_rate': 4.973821989528796e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4967, 'grad_norm': 5.051771640777588, 'learning_rate': 4.973094822571262e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4764, 'grad_norm': 4.942626953125, 'learning_rate': 4.97236765561373e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4854, 'grad_norm': 4.897036075592041, 'learning_rate': 4.971640488656196e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6078, 'grad_norm': 5.188492298126221, 'learning_rate': 4.970913321698662e-05, 'epoch': 0.02}\n",
      "{'loss': 3.376, 'grad_norm': 5.415237903594971, 'learning_rate': 4.970186154741129e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6457, 'grad_norm': 5.576304912567139, 'learning_rate': 4.969458987783595e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5191, 'grad_norm': 7.108072757720947, 'learning_rate': 4.968731820826062e-05, 'epoch': 0.02}\n",
      "{'loss': 3.55, 'grad_norm': 4.865795135498047, 'learning_rate': 4.9680046538685285e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6666, 'grad_norm': 4.803467273712158, 'learning_rate': 4.967277486910995e-05, 'epoch': 0.02}\n",
      "{'loss': 3.458, 'grad_norm': 5.756401062011719, 'learning_rate': 4.966550319953461e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4215, 'grad_norm': 5.910860061645508, 'learning_rate': 4.965823152995928e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5168, 'grad_norm': 5.6420111656188965, 'learning_rate': 4.9650959860383946e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4596, 'grad_norm': 5.391194820404053, 'learning_rate': 4.964368819080861e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4629, 'grad_norm': 5.786153793334961, 'learning_rate': 4.963641652123327e-05, 'epoch': 0.02}\n",
      "  1%|▎                                   | 500/68760 [05:13<10:46:54,  1.76it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.27s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.70s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:23<00:00,  6.98s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.242 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 30.117997999999996, 'eval_rouge-2': 6.348287999999999, 'eval_rouge-l': 23.513014, 'eval_bleu-4': 0.030151639716657224, 'eval_runtime': 36.3088, 'eval_samples_per_second': 1.377, 'eval_steps_per_second': 0.11, 'epoch': 0.02}\n",
      "  1%|▎                                   | 500/68760 [05:49<10:46:54,  1.76it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:23<00:00,  6.98s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.4799, 'grad_norm': 5.680968761444092, 'learning_rate': 4.9629144851657946e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5859, 'grad_norm': 5.769601821899414, 'learning_rate': 4.9621873182082606e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5658, 'grad_norm': 5.741812705993652, 'learning_rate': 4.961460151250727e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4826, 'grad_norm': 5.664819717407227, 'learning_rate': 4.960732984293194e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3361, 'grad_norm': 5.036021709442139, 'learning_rate': 4.960005817335661e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3988, 'grad_norm': 5.43843412399292, 'learning_rate': 4.959278650378127e-05, 'epoch': 0.02}\n",
      "{'loss': 3.502, 'grad_norm': 7.060335159301758, 'learning_rate': 4.9585514834205934e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4744, 'grad_norm': 5.047677040100098, 'learning_rate': 4.95782431646306e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4889, 'grad_norm': 5.854788780212402, 'learning_rate': 4.957097149505527e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4414, 'grad_norm': 5.759346008300781, 'learning_rate': 4.956369982547993e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4709, 'grad_norm': 6.327971458435059, 'learning_rate': 4.95564281559046e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6832, 'grad_norm': 6.297789096832275, 'learning_rate': 4.954915648632926e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4127, 'grad_norm': 5.5926032066345215, 'learning_rate': 4.954188481675393e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5127, 'grad_norm': 5.782616138458252, 'learning_rate': 4.9534613147178596e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3156, 'grad_norm': 6.076821327209473, 'learning_rate': 4.952734147760326e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4479, 'grad_norm': 6.5388875007629395, 'learning_rate': 4.952006980802792e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4404, 'grad_norm': 6.766672611236572, 'learning_rate': 4.9512798138452596e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5336, 'grad_norm': 5.792845726013184, 'learning_rate': 4.9505526468877256e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5496, 'grad_norm': 5.697185039520264, 'learning_rate': 4.949825479930192e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3449, 'grad_norm': 5.755405902862549, 'learning_rate': 4.949098312972659e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5273, 'grad_norm': 8.426182746887207, 'learning_rate': 4.948371146015126e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3182, 'grad_norm': 7.14739990234375, 'learning_rate': 4.947643979057592e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5008, 'grad_norm': 6.713037014007568, 'learning_rate': 4.9469168121000584e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4877, 'grad_norm': 6.030078411102295, 'learning_rate': 4.946189645142525e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5041, 'grad_norm': 6.724524974822998, 'learning_rate': 4.945462478184992e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4422, 'grad_norm': 6.039632320404053, 'learning_rate': 4.944735311227458e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4701, 'grad_norm': 6.333420276641846, 'learning_rate': 4.944008144269925e-05, 'epoch': 0.03}\n",
      "{'loss': 3.465, 'grad_norm': 6.703145503997803, 'learning_rate': 4.943280977312391e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4043, 'grad_norm': 5.7120513916015625, 'learning_rate': 4.942553810354858e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4855, 'grad_norm': 7.041387557983398, 'learning_rate': 4.9418266433973246e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4324, 'grad_norm': 6.489725112915039, 'learning_rate': 4.9410994764397906e-05, 'epoch': 0.04}\n",
      "{'loss': 3.582, 'grad_norm': 7.124441623687744, 'learning_rate': 4.940372309482257e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5488, 'grad_norm': 5.520554542541504, 'learning_rate': 4.939645142524724e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4369, 'grad_norm': 7.578197956085205, 'learning_rate': 4.9389179755671906e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4234, 'grad_norm': 7.3131937980651855, 'learning_rate': 4.9381908086096567e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3969, 'grad_norm': 6.405727386474609, 'learning_rate': 4.9374636416521233e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5811, 'grad_norm': 6.254045486450195, 'learning_rate': 4.93673647469459e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4891, 'grad_norm': 6.078732490539551, 'learning_rate': 4.936009307737057e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3715, 'grad_norm': 6.081798076629639, 'learning_rate': 4.935282140779523e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5295, 'grad_norm': 7.235204219818115, 'learning_rate': 4.93455497382199e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4486, 'grad_norm': 5.997340679168701, 'learning_rate': 4.933827806864456e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3611, 'grad_norm': 6.7839741706848145, 'learning_rate': 4.933100639906923e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4246, 'grad_norm': 6.9906415939331055, 'learning_rate': 4.9323734729493895e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4324, 'grad_norm': 6.226603031158447, 'learning_rate': 4.931646305991856e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4795, 'grad_norm': 5.924614429473877, 'learning_rate': 4.930919139034322e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4156, 'grad_norm': 5.837088584899902, 'learning_rate': 4.930191972076789e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2574, 'grad_norm': 7.074845314025879, 'learning_rate': 4.9294648051192556e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3781, 'grad_norm': 6.971447467803955, 'learning_rate': 4.928737638161722e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4148, 'grad_norm': 7.428528785705566, 'learning_rate': 4.928010471204188e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4875, 'grad_norm': 6.371115207672119, 'learning_rate': 4.9272833042466556e-05, 'epoch': 0.04}\n",
      "  1%|▌                                  | 1000/68760 [10:58<11:56:38,  1.58it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.03s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:05,  5.56s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.960966, 'eval_rouge-2': 6.961432000000001, 'eval_rouge-l': 22.579656, 'eval_bleu-4': 0.03296180556938578, 'eval_runtime': 35.471, 'eval_samples_per_second': 1.41, 'eval_steps_per_second': 0.113, 'epoch': 0.04}\n",
      "  1%|▌                                  | 1000/68760 [11:33<11:56:38,  1.58it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:23<00:00,  6.89s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-1000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3791, 'grad_norm': 7.053378582000732, 'learning_rate': 4.9265561372891217e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4043, 'grad_norm': 7.8402419090271, 'learning_rate': 4.9258289703315883e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3895, 'grad_norm': 6.160865783691406, 'learning_rate': 4.925101803374055e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5104, 'grad_norm': 6.608728408813477, 'learning_rate': 4.924374636416522e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5332, 'grad_norm': 7.026474952697754, 'learning_rate': 4.923647469458988e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4256, 'grad_norm': 5.984251499176025, 'learning_rate': 4.9229203025014544e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3553, 'grad_norm': 6.981761455535889, 'learning_rate': 4.922193135543921e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4029, 'grad_norm': 6.568871021270752, 'learning_rate': 4.921465968586388e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3908, 'grad_norm': 7.734512805938721, 'learning_rate': 4.920738801628854e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4586, 'grad_norm': 6.117930889129639, 'learning_rate': 4.920011634671321e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4826, 'grad_norm': 7.319199085235596, 'learning_rate': 4.919284467713787e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3727, 'grad_norm': 5.84085750579834, 'learning_rate': 4.918557300756254e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3754, 'grad_norm': 6.362896919250488, 'learning_rate': 4.9178301337987206e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3189, 'grad_norm': 6.728439807891846, 'learning_rate': 4.917102966841187e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4508, 'grad_norm': 7.306115627288818, 'learning_rate': 4.916375799883653e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2629, 'grad_norm': 6.041460990905762, 'learning_rate': 4.9156486329261206e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4434, 'grad_norm': 7.150186061859131, 'learning_rate': 4.9149214659685867e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4182, 'grad_norm': 6.228764057159424, 'learning_rate': 4.9141942990110533e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4084, 'grad_norm': 5.5393452644348145, 'learning_rate': 4.91346713205352e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4783, 'grad_norm': 6.025214672088623, 'learning_rate': 4.912739965095986e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3516, 'grad_norm': 6.264692783355713, 'learning_rate': 4.912012798138453e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4836, 'grad_norm': 6.493244171142578, 'learning_rate': 4.9112856311809194e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4303, 'grad_norm': 6.844917297363281, 'learning_rate': 4.910558464223386e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4838, 'grad_norm': 6.8835062980651855, 'learning_rate': 4.909831297265852e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4283, 'grad_norm': 6.758045196533203, 'learning_rate': 4.909104130308319e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4426, 'grad_norm': 5.7979865074157715, 'learning_rate': 4.9083769633507855e-05, 'epoch': 0.05}\n",
      "{'loss': 3.6039, 'grad_norm': 7.246650218963623, 'learning_rate': 4.907649796393252e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4229, 'grad_norm': 6.100091457366943, 'learning_rate': 4.906922629435718e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3975, 'grad_norm': 6.513072490692139, 'learning_rate': 4.9061954624781856e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4469, 'grad_norm': 7.879304885864258, 'learning_rate': 4.9054682955206516e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3814, 'grad_norm': 6.7914934158325195, 'learning_rate': 4.904741128563118e-05, 'epoch': 0.06}\n",
      "{'loss': 3.433, 'grad_norm': 6.362341403961182, 'learning_rate': 4.904013961605585e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4186, 'grad_norm': 7.1620707511901855, 'learning_rate': 4.9032867946480517e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4193, 'grad_norm': 5.985991954803467, 'learning_rate': 4.902559627690518e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3813, 'grad_norm': 6.933530330657959, 'learning_rate': 4.9018324607329844e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5053, 'grad_norm': 6.331979274749756, 'learning_rate': 4.901105293775451e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4932, 'grad_norm': 6.974112033843994, 'learning_rate': 4.900378126817918e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4914, 'grad_norm': 6.332769870758057, 'learning_rate': 4.899650959860384e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4373, 'grad_norm': 6.938186168670654, 'learning_rate': 4.898923792902851e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4928, 'grad_norm': 6.52090311050415, 'learning_rate': 4.898196625945317e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4291, 'grad_norm': 6.6170268058776855, 'learning_rate': 4.897469458987784e-05, 'epoch': 0.06}\n",
      "{'loss': 3.426, 'grad_norm': 6.043107509613037, 'learning_rate': 4.8967422920302505e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4059, 'grad_norm': 7.320342540740967, 'learning_rate': 4.896015125072717e-05, 'epoch': 0.06}\n",
      "{'loss': 3.398, 'grad_norm': 7.204819679260254, 'learning_rate': 4.895287958115183e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3941, 'grad_norm': 7.335903167724609, 'learning_rate': 4.89456079115765e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4439, 'grad_norm': 7.020168304443359, 'learning_rate': 4.8938336242001166e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4715, 'grad_norm': 7.627744674682617, 'learning_rate': 4.893106457242583e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5375, 'grad_norm': 7.551634311676025, 'learning_rate': 4.892379290285049e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4375, 'grad_norm': 6.565927505493164, 'learning_rate': 4.8916521233275167e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2746, 'grad_norm': 5.970626354217529, 'learning_rate': 4.890924956369983e-05, 'epoch': 0.07}\n",
      "  2%|▊                                  | 1500/68760 [16:43<12:09:34,  1.54it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.14s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.62s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.563575999999998, 'eval_rouge-2': 6.58261, 'eval_rouge-l': 23.733891999999997, 'eval_bleu-4': 0.03165442365690711, 'eval_runtime': 35.6576, 'eval_samples_per_second': 1.402, 'eval_steps_per_second': 0.112, 'epoch': 0.07}\n",
      "  2%|▊                                  | 1500/68760 [17:19<12:09:34,  1.54it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:23<00:00,  6.93s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-1500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.4564, 'grad_norm': 6.835273742675781, 'learning_rate': 4.8901977894124494e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3693, 'grad_norm': 7.027013301849365, 'learning_rate': 4.889470622454916e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4047, 'grad_norm': 6.175076961517334, 'learning_rate': 4.888743455497383e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3984, 'grad_norm': 7.957047462463379, 'learning_rate': 4.888016288539849e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5086, 'grad_norm': 7.825512409210205, 'learning_rate': 4.887289121582316e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4092, 'grad_norm': 6.1163482666015625, 'learning_rate': 4.886561954624782e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3098, 'grad_norm': 6.941298961639404, 'learning_rate': 4.885834787667249e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3062, 'grad_norm': 8.104315757751465, 'learning_rate': 4.8851076207097155e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4299, 'grad_norm': 6.871484756469727, 'learning_rate': 4.884380453752182e-05, 'epoch': 0.07}\n",
      "{'loss': 3.49, 'grad_norm': 7.074862003326416, 'learning_rate': 4.883653286794648e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4527, 'grad_norm': 6.409709453582764, 'learning_rate': 4.882926119837115e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5369, 'grad_norm': 7.008164882659912, 'learning_rate': 4.8821989528795816e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4891, 'grad_norm': 7.441250324249268, 'learning_rate': 4.8814717859220476e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4793, 'grad_norm': 6.861883163452148, 'learning_rate': 4.880744618964514e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3539, 'grad_norm': 6.497973918914795, 'learning_rate': 4.880017452006981e-05, 'epoch': 0.07}\n",
      "{'loss': 3.526, 'grad_norm': 6.744744300842285, 'learning_rate': 4.879290285049448e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3816, 'grad_norm': 7.29751443862915, 'learning_rate': 4.878563118091914e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4273, 'grad_norm': 7.175651550292969, 'learning_rate': 4.877835951134381e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3934, 'grad_norm': 6.804452419281006, 'learning_rate': 4.877108784176847e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4455, 'grad_norm': 6.781115531921387, 'learning_rate': 4.876381617219314e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4066, 'grad_norm': 7.238293170928955, 'learning_rate': 4.8756544502617804e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3971, 'grad_norm': 7.112039089202881, 'learning_rate': 4.874927283304247e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4598, 'grad_norm': 7.435873508453369, 'learning_rate': 4.874200116346713e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5113, 'grad_norm': 6.989663124084473, 'learning_rate': 4.87347294938918e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4172, 'grad_norm': 6.967296600341797, 'learning_rate': 4.8727457824316465e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4121, 'grad_norm': 6.752455234527588, 'learning_rate': 4.872018615474113e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4723, 'grad_norm': 6.211374282836914, 'learning_rate': 4.871291448516579e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4939, 'grad_norm': 6.851359844207764, 'learning_rate': 4.8705642815590466e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3529, 'grad_norm': 6.903733730316162, 'learning_rate': 4.8698371146015126e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3289, 'grad_norm': 6.8046183586120605, 'learning_rate': 4.869109947643979e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3828, 'grad_norm': 6.755008697509766, 'learning_rate': 4.868382780686446e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4609, 'grad_norm': 6.072253227233887, 'learning_rate': 4.867655613728913e-05, 'epoch': 0.08}\n",
      "{'loss': 3.41, 'grad_norm': 7.129286766052246, 'learning_rate': 4.866928446771379e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3701, 'grad_norm': 6.670097827911377, 'learning_rate': 4.8662012798138454e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3268, 'grad_norm': 6.719322681427002, 'learning_rate': 4.865474112856312e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4096, 'grad_norm': 7.049243450164795, 'learning_rate': 4.864746945898779e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4014, 'grad_norm': 6.627460956573486, 'learning_rate': 4.864019778941245e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2711, 'grad_norm': 7.287693500518799, 'learning_rate': 4.863292611983712e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4682, 'grad_norm': 7.417572975158691, 'learning_rate': 4.862565445026178e-05, 'epoch': 0.08}\n",
      "{'loss': 3.375, 'grad_norm': 6.714954853057861, 'learning_rate': 4.861838278068645e-05, 'epoch': 0.08}\n",
      "{'loss': 3.401, 'grad_norm': 6.929347515106201, 'learning_rate': 4.8611111111111115e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4137, 'grad_norm': 6.248781204223633, 'learning_rate': 4.860383944153578e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4422, 'grad_norm': 6.615328311920166, 'learning_rate': 4.859656777196044e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4332, 'grad_norm': 6.803929328918457, 'learning_rate': 4.858929610238511e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2789, 'grad_norm': 7.028914451599121, 'learning_rate': 4.8582024432809776e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3645, 'grad_norm': 6.797562599182129, 'learning_rate': 4.857475276323444e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3426, 'grad_norm': 7.117684364318848, 'learning_rate': 4.85674810936591e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3891, 'grad_norm': 6.439541816711426, 'learning_rate': 4.856020942408378e-05, 'epoch': 0.09}\n",
      "{'loss': 3.417, 'grad_norm': 8.076313018798828, 'learning_rate': 4.855293775450844e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4672, 'grad_norm': 7.693373203277588, 'learning_rate': 4.85456660849331e-05, 'epoch': 0.09}\n",
      "  3%|█                                  | 2000/68760 [22:27<12:05:17,  1.53it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.339133999999994, 'eval_rouge-2': 6.847437999999999, 'eval_rouge-l': 24.737556000000005, 'eval_bleu-4': 0.03158453583218248, 'eval_runtime': 35.6542, 'eval_samples_per_second': 1.402, 'eval_steps_per_second': 0.112, 'epoch': 0.09}\n",
      "  3%|█                                  | 2000/68760 [23:03<12:05:17,  1.53it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  8.75s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.343, 'grad_norm': 6.63975191116333, 'learning_rate': 4.853839441535777e-05, 'epoch': 0.09}\n",
      "{'loss': 3.29, 'grad_norm': 6.878228187561035, 'learning_rate': 4.853112274578243e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3893, 'grad_norm': 7.019903182983398, 'learning_rate': 4.85238510762071e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3893, 'grad_norm': 6.923579692840576, 'learning_rate': 4.8516579406631764e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4338, 'grad_norm': 6.970373153686523, 'learning_rate': 4.850930773705643e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5414, 'grad_norm': 7.0776190757751465, 'learning_rate': 4.850203606748109e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4057, 'grad_norm': 6.844956874847412, 'learning_rate': 4.8494764397905765e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4814, 'grad_norm': 6.933566570281982, 'learning_rate': 4.8487492728330425e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3756, 'grad_norm': 7.136112213134766, 'learning_rate': 4.848022105875509e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5131, 'grad_norm': 6.437678337097168, 'learning_rate': 4.847294938917976e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5264, 'grad_norm': 6.355262279510498, 'learning_rate': 4.8465677719604426e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4025, 'grad_norm': 6.961944103240967, 'learning_rate': 4.8458406050029086e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3939, 'grad_norm': 8.107158660888672, 'learning_rate': 4.845113438045375e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3441, 'grad_norm': 6.640314102172852, 'learning_rate': 4.844386271087842e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4891, 'grad_norm': 6.688788890838623, 'learning_rate': 4.843659104130309e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4332, 'grad_norm': 7.824726581573486, 'learning_rate': 4.842931937172775e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4324, 'grad_norm': 7.245715141296387, 'learning_rate': 4.842204770215242e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3279, 'grad_norm': 6.852088451385498, 'learning_rate': 4.841477603257708e-05, 'epoch': 0.1}\n",
      "{'loss': 3.367, 'grad_norm': 8.976794242858887, 'learning_rate': 4.840750436300175e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5248, 'grad_norm': 7.2471723556518555, 'learning_rate': 4.8400232693426414e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4377, 'grad_norm': 6.8629865646362305, 'learning_rate': 4.839296102385108e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2764, 'grad_norm': 6.906515598297119, 'learning_rate': 4.838568935427574e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3543, 'grad_norm': 6.754329204559326, 'learning_rate': 4.837841768470041e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2479, 'grad_norm': 6.952877521514893, 'learning_rate': 4.8371146015125075e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4215, 'grad_norm': 6.733872890472412, 'learning_rate': 4.836387434554974e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3854, 'grad_norm': 7.439169883728027, 'learning_rate': 4.83566026759744e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4643, 'grad_norm': 6.243377685546875, 'learning_rate': 4.8349331006399076e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4775, 'grad_norm': 7.5297441482543945, 'learning_rate': 4.8342059336823736e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4275, 'grad_norm': 8.19321346282959, 'learning_rate': 4.83347876672484e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4027, 'grad_norm': 7.931632995605469, 'learning_rate': 4.832751599767307e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4129, 'grad_norm': 7.5268683433532715, 'learning_rate': 4.832024432809774e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3207, 'grad_norm': 7.410455226898193, 'learning_rate': 4.83129726585224e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3188, 'grad_norm': 6.942922115325928, 'learning_rate': 4.8305700988947064e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2723, 'grad_norm': 7.023056507110596, 'learning_rate': 4.829842931937173e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4043, 'grad_norm': 6.656778812408447, 'learning_rate': 4.82911576497964e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3246, 'grad_norm': 7.141319274902344, 'learning_rate': 4.828388598022106e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2764, 'grad_norm': 6.727743148803711, 'learning_rate': 4.827661431064573e-05, 'epoch': 0.1}\n",
      "{'loss': 3.474, 'grad_norm': 6.938635349273682, 'learning_rate': 4.826934264107039e-05, 'epoch': 0.1}\n",
      "{'loss': 3.515, 'grad_norm': 6.8845438957214355, 'learning_rate': 4.826207097149506e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3732, 'grad_norm': 6.331892013549805, 'learning_rate': 4.8254799301919725e-05, 'epoch': 0.1}\n",
      "{'loss': 3.5145, 'grad_norm': 7.190702438354492, 'learning_rate': 4.8247527632344385e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3904, 'grad_norm': 7.158609390258789, 'learning_rate': 4.824025596276905e-05, 'epoch': 0.11}\n",
      "{'loss': 3.326, 'grad_norm': 6.763055324554443, 'learning_rate': 4.823298429319372e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3469, 'grad_norm': 7.006174564361572, 'learning_rate': 4.8225712623618386e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5584, 'grad_norm': 6.86747407913208, 'learning_rate': 4.8218440954043046e-05, 'epoch': 0.11}\n",
      "{'loss': 3.1648, 'grad_norm': 7.2090935707092285, 'learning_rate': 4.821116928446772e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3529, 'grad_norm': 6.905623435974121, 'learning_rate': 4.820389761489238e-05, 'epoch': 0.11}\n",
      "{'loss': 3.427, 'grad_norm': 6.536060810089111, 'learning_rate': 4.819662594531705e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3471, 'grad_norm': 6.720921993255615, 'learning_rate': 4.8189354275741714e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4703, 'grad_norm': 6.2641987800598145, 'learning_rate': 4.818208260616638e-05, 'epoch': 0.11}\n",
      "  4%|█▎                                 | 2500/68760 [28:09<12:21:00,  1.49it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.50s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.82715, 'eval_rouge-2': 7.637525999999999, 'eval_rouge-l': 24.321616, 'eval_bleu-4': 0.03533343173257942, 'eval_runtime': 45.6188, 'eval_samples_per_second': 1.096, 'eval_steps_per_second': 0.088, 'epoch': 0.11}\n",
      "  4%|█▎                                 | 2500/68760 [28:54<12:21:00,  1.49it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  8.74s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2477, 'grad_norm': 6.8163957595825195, 'learning_rate': 4.817481093659104e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5123, 'grad_norm': 7.341535568237305, 'learning_rate': 4.816753926701571e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4299, 'grad_norm': 7.762011528015137, 'learning_rate': 4.8160267597440375e-05, 'epoch': 0.11}\n",
      "{'loss': 3.449, 'grad_norm': 6.774735927581787, 'learning_rate': 4.815299592786504e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3346, 'grad_norm': 6.491263389587402, 'learning_rate': 4.81457242582897e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4273, 'grad_norm': 7.195074558258057, 'learning_rate': 4.8138452588714375e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3936, 'grad_norm': 6.981831073760986, 'learning_rate': 4.8131180919139035e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5139, 'grad_norm': 7.692958831787109, 'learning_rate': 4.81239092495637e-05, 'epoch': 0.11}\n",
      "{'loss': 3.326, 'grad_norm': 7.282724857330322, 'learning_rate': 4.811663757998837e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3953, 'grad_norm': 7.832827091217041, 'learning_rate': 4.8109365910413036e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4057, 'grad_norm': 7.269961357116699, 'learning_rate': 4.8102094240837696e-05, 'epoch': 0.11}\n",
      "{'loss': 3.443, 'grad_norm': 6.289412021636963, 'learning_rate': 4.809482257126236e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2578, 'grad_norm': 6.707058429718018, 'learning_rate': 4.808755090168703e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4248, 'grad_norm': 6.446294784545898, 'learning_rate': 4.80802792321117e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3545, 'grad_norm': 6.991726398468018, 'learning_rate': 4.807300756253636e-05, 'epoch': 0.12}\n",
      "{'loss': 3.334, 'grad_norm': 7.122893810272217, 'learning_rate': 4.806573589296103e-05, 'epoch': 0.12}\n",
      "{'loss': 3.366, 'grad_norm': 7.218725204467773, 'learning_rate': 4.805846422338569e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3543, 'grad_norm': 7.5447998046875, 'learning_rate': 4.805119255381036e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4623, 'grad_norm': 6.9656243324279785, 'learning_rate': 4.8043920884235025e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3682, 'grad_norm': 6.388377666473389, 'learning_rate': 4.803664921465969e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3352, 'grad_norm': 6.432741641998291, 'learning_rate': 4.802937754508435e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3668, 'grad_norm': 6.30816125869751, 'learning_rate': 4.802210587550902e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2424, 'grad_norm': 6.881883144378662, 'learning_rate': 4.8014834205933685e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3623, 'grad_norm': 7.044820308685303, 'learning_rate': 4.800756253635835e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3441, 'grad_norm': 7.32511568069458, 'learning_rate': 4.800029086678301e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3297, 'grad_norm': 6.909946441650391, 'learning_rate': 4.7993019197207686e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3525, 'grad_norm': 7.648974418640137, 'learning_rate': 4.7985747527632346e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3014, 'grad_norm': 6.646653652191162, 'learning_rate': 4.797847585805701e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3465, 'grad_norm': 8.000992774963379, 'learning_rate': 4.797120418848168e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4787, 'grad_norm': 7.090011119842529, 'learning_rate': 4.796393251890634e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4021, 'grad_norm': 6.852939605712891, 'learning_rate': 4.795666084933101e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3037, 'grad_norm': 6.181131839752197, 'learning_rate': 4.7949389179755674e-05, 'epoch': 0.12}\n",
      "{'loss': 3.384, 'grad_norm': 8.071118354797363, 'learning_rate': 4.794211751018034e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3309, 'grad_norm': 7.215324401855469, 'learning_rate': 4.7934845840605e-05, 'epoch': 0.12}\n",
      "{'loss': 3.241, 'grad_norm': 6.798108100891113, 'learning_rate': 4.792757417102967e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4221, 'grad_norm': 6.821751594543457, 'learning_rate': 4.7920302501454335e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3645, 'grad_norm': 6.9087347984313965, 'learning_rate': 4.7913030831879e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2182, 'grad_norm': 6.887081623077393, 'learning_rate': 4.790575916230366e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3535, 'grad_norm': 7.993666648864746, 'learning_rate': 4.7898487492728335e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4971, 'grad_norm': 7.449388027191162, 'learning_rate': 4.7891215823152995e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3232, 'grad_norm': 7.147407531738281, 'learning_rate': 4.788394415357766e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3061, 'grad_norm': 7.884832382202148, 'learning_rate': 4.787667248400233e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2176, 'grad_norm': 6.8776350021362305, 'learning_rate': 4.7869400814426996e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3973, 'grad_norm': 6.619953632354736, 'learning_rate': 4.7862129144851656e-05, 'epoch': 0.13}\n",
      "{'loss': 3.459, 'grad_norm': 7.285758018493652, 'learning_rate': 4.785485747527633e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3797, 'grad_norm': 7.473917484283447, 'learning_rate': 4.784758580570099e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4207, 'grad_norm': 7.137749671936035, 'learning_rate': 4.784031413612566e-05, 'epoch': 0.13}\n",
      "{'loss': 3.307, 'grad_norm': 6.811392784118652, 'learning_rate': 4.7833042466550324e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3209, 'grad_norm': 6.836728572845459, 'learning_rate': 4.782577079697499e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2906, 'grad_norm': 6.837697505950928, 'learning_rate': 4.781849912739965e-05, 'epoch': 0.13}\n",
      "  4%|█▌                                 | 3000/68760 [34:01<11:19:17,  1.61it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.23s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.085016, 'eval_rouge-2': 6.548446, 'eval_rouge-l': 23.35397, 'eval_bleu-4': 0.030518216992163616, 'eval_runtime': 35.318, 'eval_samples_per_second': 1.416, 'eval_steps_per_second': 0.113, 'epoch': 0.13}\n",
      "  4%|█▌                                 | 3000/68760 [34:37<11:19:17,  1.61it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:23<00:00,  6.06s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-3000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2158, 'grad_norm': 12.237098693847656, 'learning_rate': 4.781122745782432e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2607, 'grad_norm': 8.582145690917969, 'learning_rate': 4.7803955788248985e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4541, 'grad_norm': 6.875422954559326, 'learning_rate': 4.779668411867365e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3529, 'grad_norm': 7.162674903869629, 'learning_rate': 4.778941244909831e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3512, 'grad_norm': 7.005746841430664, 'learning_rate': 4.7782140779522985e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3549, 'grad_norm': 7.461012840270996, 'learning_rate': 4.7774869109947645e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3453, 'grad_norm': 7.687946796417236, 'learning_rate': 4.776759744037231e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2902, 'grad_norm': 7.09230899810791, 'learning_rate': 4.776032577079698e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3189, 'grad_norm': 7.158580780029297, 'learning_rate': 4.7753054101221646e-05, 'epoch': 0.13}\n",
      "{'loss': 3.5854, 'grad_norm': 7.218592166900635, 'learning_rate': 4.7745782431646306e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4775, 'grad_norm': 6.501031875610352, 'learning_rate': 4.773851076207097e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4361, 'grad_norm': 6.429056167602539, 'learning_rate': 4.773123909249564e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3004, 'grad_norm': 6.989495754241943, 'learning_rate': 4.772396742292031e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4045, 'grad_norm': 6.3669891357421875, 'learning_rate': 4.771669575334497e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4533, 'grad_norm': 6.987752914428711, 'learning_rate': 4.770942408376964e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4137, 'grad_norm': 7.084104061126709, 'learning_rate': 4.77021524141943e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3465, 'grad_norm': 7.9263014793396, 'learning_rate': 4.769488074461897e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4766, 'grad_norm': 6.608340740203857, 'learning_rate': 4.7687609075043635e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3389, 'grad_norm': 7.164591312408447, 'learning_rate': 4.76803374054683e-05, 'epoch': 0.14}\n",
      "{'loss': 3.473, 'grad_norm': 7.826939105987549, 'learning_rate': 4.767306573589296e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2811, 'grad_norm': 7.608525276184082, 'learning_rate': 4.766579406631763e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4406, 'grad_norm': 6.7073655128479, 'learning_rate': 4.7658522396742295e-05, 'epoch': 0.14}\n",
      "{'loss': 3.432, 'grad_norm': 7.6155242919921875, 'learning_rate': 4.7651250727166956e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2914, 'grad_norm': 6.429361343383789, 'learning_rate': 4.764397905759162e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3324, 'grad_norm': 6.432245254516602, 'learning_rate': 4.763670738801629e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2539, 'grad_norm': 7.571222305297852, 'learning_rate': 4.7629435718440956e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4643, 'grad_norm': 7.808412075042725, 'learning_rate': 4.7622164048865616e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3684, 'grad_norm': 7.297715663909912, 'learning_rate': 4.761489237929029e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3871, 'grad_norm': 6.925282955169678, 'learning_rate': 4.760762070971495e-05, 'epoch': 0.14}\n",
      "{'loss': 3.325, 'grad_norm': 7.297329425811768, 'learning_rate': 4.760034904013962e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4311, 'grad_norm': 7.095937728881836, 'learning_rate': 4.7593077370564284e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3303, 'grad_norm': 6.86182975769043, 'learning_rate': 4.758580570098895e-05, 'epoch': 0.14}\n",
      "{'loss': 3.348, 'grad_norm': 7.483709812164307, 'learning_rate': 4.757853403141361e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2715, 'grad_norm': 7.046223163604736, 'learning_rate': 4.7571262361838285e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4176, 'grad_norm': 8.842360496520996, 'learning_rate': 4.7563990692262945e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4318, 'grad_norm': 6.862270832061768, 'learning_rate': 4.755671902268761e-05, 'epoch': 0.15}\n",
      "{'loss': 3.365, 'grad_norm': 8.50274658203125, 'learning_rate': 4.754944735311228e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2732, 'grad_norm': 5.794495105743408, 'learning_rate': 4.7542175683536945e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4996, 'grad_norm': 8.401252746582031, 'learning_rate': 4.7534904013961606e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4387, 'grad_norm': 6.661204814910889, 'learning_rate': 4.752763234438627e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3449, 'grad_norm': 7.131821632385254, 'learning_rate': 4.752036067481094e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2664, 'grad_norm': 7.283858776092529, 'learning_rate': 4.7513089005235606e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2221, 'grad_norm': 8.03541088104248, 'learning_rate': 4.7505817335660266e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4053, 'grad_norm': 7.394880771636963, 'learning_rate': 4.749854566608494e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3395, 'grad_norm': 6.968741416931152, 'learning_rate': 4.74912739965096e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2586, 'grad_norm': 7.177121162414551, 'learning_rate': 4.748400232693427e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3391, 'grad_norm': 7.0274176597595215, 'learning_rate': 4.7476730657358934e-05, 'epoch': 0.15}\n",
      "{'loss': 3.302, 'grad_norm': 7.035677433013916, 'learning_rate': 4.74694589877836e-05, 'epoch': 0.15}\n",
      "{'loss': 3.301, 'grad_norm': 10.850223541259766, 'learning_rate': 4.746218731820826e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4254, 'grad_norm': 6.8544921875, 'learning_rate': 4.745491564863293e-05, 'epoch': 0.15}\n",
      "  5%|█▊                                 | 3500/68760 [39:47<11:28:03,  1.58it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.16s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.46s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.994534, 'eval_rouge-2': 7.28964, 'eval_rouge-l': 24.355742, 'eval_bleu-4': 0.03745479685664476, 'eval_runtime': 17.6222, 'eval_samples_per_second': 2.837, 'eval_steps_per_second': 0.227, 'epoch': 0.15}\n",
      "  5%|█▊                                 | 3500/68760 [40:04<11:28:03,  1.58it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.30s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-3500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2605, 'grad_norm': 8.177186012268066, 'learning_rate': 4.7447643979057595e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3383, 'grad_norm': 7.003015041351318, 'learning_rate': 4.744037230948226e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3389, 'grad_norm': 8.412468910217285, 'learning_rate': 4.743310063990692e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2705, 'grad_norm': 7.494683265686035, 'learning_rate': 4.7425828970331595e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3813, 'grad_norm': 6.997287273406982, 'learning_rate': 4.7418557300756256e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2264, 'grad_norm': 7.4643144607543945, 'learning_rate': 4.741128563118092e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3674, 'grad_norm': 6.5692949295043945, 'learning_rate': 4.740401396160559e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4908, 'grad_norm': 7.7701520919799805, 'learning_rate': 4.7396742292030256e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3354, 'grad_norm': 7.115901947021484, 'learning_rate': 4.7389470622454916e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3717, 'grad_norm': 7.285871505737305, 'learning_rate': 4.738219895287958e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4266, 'grad_norm': 6.98978328704834, 'learning_rate': 4.737492728330425e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2678, 'grad_norm': 6.77274227142334, 'learning_rate': 4.736765561372891e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3789, 'grad_norm': 6.636729717254639, 'learning_rate': 4.736038394415358e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2629, 'grad_norm': 7.509377479553223, 'learning_rate': 4.7353112274578244e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3707, 'grad_norm': 6.955242156982422, 'learning_rate': 4.734584060500291e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3789, 'grad_norm': 7.498287677764893, 'learning_rate': 4.733856893542757e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3041, 'grad_norm': 7.376264572143555, 'learning_rate': 4.7331297265852245e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3473, 'grad_norm': 7.502767086029053, 'learning_rate': 4.7324025596276905e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2846, 'grad_norm': 6.886200428009033, 'learning_rate': 4.731675392670157e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3855, 'grad_norm': 6.405886173248291, 'learning_rate': 4.730948225712624e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3889, 'grad_norm': 7.229759216308594, 'learning_rate': 4.7302210587550906e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3266, 'grad_norm': 6.81824254989624, 'learning_rate': 4.7294938917975566e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4326, 'grad_norm': 7.825042724609375, 'learning_rate': 4.728766724840023e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4631, 'grad_norm': 7.167402267456055, 'learning_rate': 4.72803955788249e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4627, 'grad_norm': 6.385161876678467, 'learning_rate': 4.7273123909249566e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4992, 'grad_norm': 7.524393081665039, 'learning_rate': 4.7265852239674227e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4166, 'grad_norm': 6.673911094665527, 'learning_rate': 4.72585805700989e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3873, 'grad_norm': 6.624701023101807, 'learning_rate': 4.725130890052356e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3516, 'grad_norm': 7.200399875640869, 'learning_rate': 4.724403723094823e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3682, 'grad_norm': 7.458515644073486, 'learning_rate': 4.7236765561372894e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3178, 'grad_norm': 8.153778076171875, 'learning_rate': 4.722949389179756e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4033, 'grad_norm': 7.066944599151611, 'learning_rate': 4.722222222222222e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3836, 'grad_norm': 7.089659214019775, 'learning_rate': 4.7214950552646895e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4137, 'grad_norm': 7.632340431213379, 'learning_rate': 4.7207678883071555e-05, 'epoch': 0.17}\n",
      "{'loss': 3.5764, 'grad_norm': 7.790324687957764, 'learning_rate': 4.720040721349622e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1439, 'grad_norm': 7.2042741775512695, 'learning_rate': 4.719313554392089e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2643, 'grad_norm': 8.243280410766602, 'learning_rate': 4.7185863874345556e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3807, 'grad_norm': 10.113865852355957, 'learning_rate': 4.7178592204770216e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2467, 'grad_norm': 7.440348148345947, 'learning_rate': 4.717132053519488e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2771, 'grad_norm': 7.0031867027282715, 'learning_rate': 4.716404886561955e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4881, 'grad_norm': 7.471083164215088, 'learning_rate': 4.7156777196044216e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3127, 'grad_norm': 7.613226890563965, 'learning_rate': 4.7149505526468876e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2178, 'grad_norm': 7.412527084350586, 'learning_rate': 4.714223385689355e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3666, 'grad_norm': 7.2932939529418945, 'learning_rate': 4.713496218731821e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3111, 'grad_norm': 6.663497447967529, 'learning_rate': 4.712769051774288e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3424, 'grad_norm': 7.959456920623779, 'learning_rate': 4.7120418848167544e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4707, 'grad_norm': 7.628739356994629, 'learning_rate': 4.711314717859221e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3086, 'grad_norm': 6.780134201049805, 'learning_rate': 4.710587550901687e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3305, 'grad_norm': 6.8333306312561035, 'learning_rate': 4.709860383944154e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1406, 'grad_norm': 6.875617980957031, 'learning_rate': 4.7091332169866205e-05, 'epoch': 0.17}\n",
      "  6%|██                                 | 4000/68760 [45:14<10:38:42,  1.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.50s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.121076, 'eval_rouge-2': 6.660486, 'eval_rouge-l': 21.32947, 'eval_bleu-4': 0.029839434949502183, 'eval_runtime': 45.6009, 'eval_samples_per_second': 1.096, 'eval_steps_per_second': 0.088, 'epoch': 0.17}\n",
      "  6%|██                                 | 4000/68760 [45:59<10:38:42,  1.69it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  8.74s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-4000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.4084, 'grad_norm': 7.343353748321533, 'learning_rate': 4.7084060500290865e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3527, 'grad_norm': 6.482705593109131, 'learning_rate': 4.707678883071553e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2434, 'grad_norm': 6.528530120849609, 'learning_rate': 4.70695171611402e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3283, 'grad_norm': 7.980737686157227, 'learning_rate': 4.7062245491564866e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3766, 'grad_norm': 6.684381484985352, 'learning_rate': 4.7054973821989526e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4613, 'grad_norm': 6.722583770751953, 'learning_rate': 4.70477021524142e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4363, 'grad_norm': 6.572778224945068, 'learning_rate': 4.704043048283886e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3311, 'grad_norm': 6.476711750030518, 'learning_rate': 4.7033158813263526e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3393, 'grad_norm': 8.113285064697266, 'learning_rate': 4.7025887143688193e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3271, 'grad_norm': 7.176059722900391, 'learning_rate': 4.701861547411286e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2898, 'grad_norm': 6.958792686462402, 'learning_rate': 4.701134380453752e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2959, 'grad_norm': 7.28585147857666, 'learning_rate': 4.700407213496219e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3305, 'grad_norm': 7.513507843017578, 'learning_rate': 4.6996800465386854e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2941, 'grad_norm': 7.704373359680176, 'learning_rate': 4.698952879581152e-05, 'epoch': 0.18}\n",
      "{'loss': 3.408, 'grad_norm': 6.870911121368408, 'learning_rate': 4.698225712623618e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3449, 'grad_norm': 6.763436794281006, 'learning_rate': 4.6974985456660855e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4555, 'grad_norm': 7.128466606140137, 'learning_rate': 4.6967713787085515e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3252, 'grad_norm': 6.973842620849609, 'learning_rate': 4.696044211751018e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4182, 'grad_norm': 7.757055282592773, 'learning_rate': 4.695317044793485e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3016, 'grad_norm': 6.6762871742248535, 'learning_rate': 4.6945898778359516e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2412, 'grad_norm': 7.505995750427246, 'learning_rate': 4.6938627108784176e-05, 'epoch': 0.18}\n",
      "{'loss': 3.1523, 'grad_norm': 6.669270038604736, 'learning_rate': 4.693135543920885e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2252, 'grad_norm': 7.374346733093262, 'learning_rate': 4.692408376963351e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2092, 'grad_norm': 6.980013370513916, 'learning_rate': 4.6916812100058176e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3055, 'grad_norm': 7.86083984375, 'learning_rate': 4.690954043048284e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3361, 'grad_norm': 7.276488780975342, 'learning_rate': 4.690226876090751e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2867, 'grad_norm': 8.322100639343262, 'learning_rate': 4.689499709133217e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2543, 'grad_norm': 7.56488037109375, 'learning_rate': 4.688772542175684e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2771, 'grad_norm': 6.733166217803955, 'learning_rate': 4.6880453752181504e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2848, 'grad_norm': 7.026002883911133, 'learning_rate': 4.687318208260617e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2809, 'grad_norm': 7.036928653717041, 'learning_rate': 4.686591041303083e-05, 'epoch': 0.19}\n",
      "{'loss': 3.323, 'grad_norm': 7.34930419921875, 'learning_rate': 4.6858638743455505e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4207, 'grad_norm': 6.570087909698486, 'learning_rate': 4.6851367073880165e-05, 'epoch': 0.19}\n",
      "{'loss': 3.26, 'grad_norm': 7.475534439086914, 'learning_rate': 4.684409540430483e-05, 'epoch': 0.19}\n",
      "{'loss': 3.359, 'grad_norm': 7.650315761566162, 'learning_rate': 4.68368237347295e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3436, 'grad_norm': 7.0070695877075195, 'learning_rate': 4.6829552065154166e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3912, 'grad_norm': 7.216175556182861, 'learning_rate': 4.6822280395578826e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2631, 'grad_norm': 7.300492286682129, 'learning_rate': 4.681500872600349e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3143, 'grad_norm': 7.663378715515137, 'learning_rate': 4.680773705642816e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2852, 'grad_norm': 7.364168167114258, 'learning_rate': 4.680046538685282e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4008, 'grad_norm': 7.116032123565674, 'learning_rate': 4.6793193717277487e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3508, 'grad_norm': 7.1777544021606445, 'learning_rate': 4.6785922047702154e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4801, 'grad_norm': 6.707097053527832, 'learning_rate': 4.677865037812682e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2582, 'grad_norm': 7.27829647064209, 'learning_rate': 4.677137870855148e-05, 'epoch': 0.19}\n",
      "{'loss': 3.274, 'grad_norm': 7.404837131500244, 'learning_rate': 4.6764107038976154e-05, 'epoch': 0.19}\n",
      "{'loss': 3.276, 'grad_norm': 8.229657173156738, 'learning_rate': 4.6756835369400814e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3203, 'grad_norm': 7.76650333404541, 'learning_rate': 4.674956369982548e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3229, 'grad_norm': 7.145764350891113, 'learning_rate': 4.674229203025015e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3479, 'grad_norm': 9.316917419433594, 'learning_rate': 4.6735020360674815e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2734, 'grad_norm': 7.474826335906982, 'learning_rate': 4.6727748691099475e-05, 'epoch': 0.2}\n",
      "  7%|██▎                                | 4500/68760 [51:10<11:09:50,  1.60it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.50s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.308088, 'eval_rouge-2': 7.211534, 'eval_rouge-l': 24.499762, 'eval_bleu-4': 0.030470648962915497, 'eval_runtime': 37.4346, 'eval_samples_per_second': 1.336, 'eval_steps_per_second': 0.107, 'epoch': 0.2}\n",
      "  7%|██▎                                | 4500/68760 [51:47<11:09:50,  1.60it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.70s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-4500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.349, 'grad_norm': 6.487696170806885, 'learning_rate': 4.672047702152414e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3703, 'grad_norm': 7.298358917236328, 'learning_rate': 4.671320535194881e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4285, 'grad_norm': 6.861961841583252, 'learning_rate': 4.6705933682373476e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2869, 'grad_norm': 7.353546619415283, 'learning_rate': 4.6698662012798136e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4488, 'grad_norm': 7.275052070617676, 'learning_rate': 4.669139034322281e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3076, 'grad_norm': 8.579574584960938, 'learning_rate': 4.668411867364747e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4367, 'grad_norm': 7.291879177093506, 'learning_rate': 4.6676847004072137e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3434, 'grad_norm': 7.262322902679443, 'learning_rate': 4.6669575334496804e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3287, 'grad_norm': 7.20402717590332, 'learning_rate': 4.666230366492147e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3936, 'grad_norm': 7.644272327423096, 'learning_rate': 4.665503199534613e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3613, 'grad_norm': 7.153761863708496, 'learning_rate': 4.66477603257708e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3066, 'grad_norm': 7.541435241699219, 'learning_rate': 4.6640488656195464e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3561, 'grad_norm': 7.632855415344238, 'learning_rate': 4.663321698662013e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4148, 'grad_norm': 7.082825183868408, 'learning_rate': 4.662594531704479e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2811, 'grad_norm': 6.5390448570251465, 'learning_rate': 4.6618673647469465e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2232, 'grad_norm': 8.139557838439941, 'learning_rate': 4.6611401977894125e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2369, 'grad_norm': 7.29149866104126, 'learning_rate': 4.660413030831879e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3516, 'grad_norm': 7.186008930206299, 'learning_rate': 4.659685863874346e-05, 'epoch': 0.2}\n",
      "{'loss': 3.1609, 'grad_norm': 6.927206993103027, 'learning_rate': 4.6589586969168126e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2502, 'grad_norm': 7.119161128997803, 'learning_rate': 4.6582315299592786e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2457, 'grad_norm': 7.493477821350098, 'learning_rate': 4.657504363001746e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2902, 'grad_norm': 7.0765180587768555, 'learning_rate': 4.656777196044212e-05, 'epoch': 0.21}\n",
      "{'loss': 3.477, 'grad_norm': 7.359493255615234, 'learning_rate': 4.6560500290866787e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3895, 'grad_norm': 6.84971284866333, 'learning_rate': 4.6553228621291453e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3957, 'grad_norm': 7.243264198303223, 'learning_rate': 4.654595695171612e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4502, 'grad_norm': 6.9752373695373535, 'learning_rate': 4.653868528214078e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4188, 'grad_norm': 7.529500961303711, 'learning_rate': 4.653141361256545e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3785, 'grad_norm': 7.41131067276001, 'learning_rate': 4.6524141942990114e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3602, 'grad_norm': 8.548637390136719, 'learning_rate': 4.651687027341478e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4258, 'grad_norm': 7.383105754852295, 'learning_rate': 4.650959860383944e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3117, 'grad_norm': 7.649941444396973, 'learning_rate': 4.650232693426411e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2188, 'grad_norm': 6.888808727264404, 'learning_rate': 4.6495055264688775e-05, 'epoch': 0.21}\n",
      "{'loss': 3.1748, 'grad_norm': 6.930405616760254, 'learning_rate': 4.6487783595113435e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3953, 'grad_norm': 7.136369705200195, 'learning_rate': 4.648051192553811e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2762, 'grad_norm': 7.5030412673950195, 'learning_rate': 4.647324025596277e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3248, 'grad_norm': 7.1863555908203125, 'learning_rate': 4.6465968586387436e-05, 'epoch': 0.21}\n",
      "{'loss': 3.358, 'grad_norm': 6.859156131744385, 'learning_rate': 4.64586969168121e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3742, 'grad_norm': 6.985790729522705, 'learning_rate': 4.645142524723677e-05, 'epoch': 0.21}\n",
      "{'loss': 3.227, 'grad_norm': 7.551669120788574, 'learning_rate': 4.644415357766143e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3605, 'grad_norm': 6.716835021972656, 'learning_rate': 4.64368819080861e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2662, 'grad_norm': 9.24878215789795, 'learning_rate': 4.6429610238510764e-05, 'epoch': 0.21}\n",
      "{'loss': 3.442, 'grad_norm': 7.364518642425537, 'learning_rate': 4.642233856893543e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4039, 'grad_norm': 7.2508158683776855, 'learning_rate': 4.641506689936009e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2494, 'grad_norm': 7.913571834564209, 'learning_rate': 4.6407795229784764e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3211, 'grad_norm': 6.669483184814453, 'learning_rate': 4.6400523560209424e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2697, 'grad_norm': 7.073995113372803, 'learning_rate': 4.639325189063409e-05, 'epoch': 0.22}\n",
      "{'loss': 3.233, 'grad_norm': 7.378519535064697, 'learning_rate': 4.638598022105876e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3078, 'grad_norm': 6.807032108306885, 'learning_rate': 4.6378708551483425e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4131, 'grad_norm': 7.366269111633301, 'learning_rate': 4.6371436881908085e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3076, 'grad_norm': 7.59051513671875, 'learning_rate': 4.636416521233275e-05, 'epoch': 0.22}\n",
      "  7%|██▌                                | 5000/68760 [57:02<11:23:05,  1.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.144754000000006, 'eval_rouge-2': 7.868040000000001, 'eval_rouge-l': 25.172235999999998, 'eval_bleu-4': 0.03718344086662419, 'eval_runtime': 27.209, 'eval_samples_per_second': 1.838, 'eval_steps_per_second': 0.147, 'epoch': 0.22}\n",
      "  7%|██▌                                | 5000/68760 [57:30<11:23:05,  1.56it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:24<00:00,  5.63s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-5000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.343, 'grad_norm': 8.195801734924316, 'learning_rate': 4.635689354275742e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3496, 'grad_norm': 7.308911323547363, 'learning_rate': 4.6349621873182086e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3189, 'grad_norm': 7.172280311584473, 'learning_rate': 4.6342350203606746e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2434, 'grad_norm': 7.200687408447266, 'learning_rate': 4.633507853403142e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3996, 'grad_norm': 7.1529669761657715, 'learning_rate': 4.632780686445608e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3707, 'grad_norm': 6.660386085510254, 'learning_rate': 4.632053519488075e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3566, 'grad_norm': 6.974921226501465, 'learning_rate': 4.6313263525305414e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3637, 'grad_norm': 8.268531799316406, 'learning_rate': 4.630599185573008e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4006, 'grad_norm': 7.022994518280029, 'learning_rate': 4.629872018615474e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4285, 'grad_norm': 7.995883941650391, 'learning_rate': 4.6291448516579414e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4855, 'grad_norm': 7.212204933166504, 'learning_rate': 4.6284176847004074e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3225, 'grad_norm': 7.040669918060303, 'learning_rate': 4.627690517742874e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3027, 'grad_norm': 8.059640884399414, 'learning_rate': 4.626963350785341e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3473, 'grad_norm': 7.621701240539551, 'learning_rate': 4.6262361838278075e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4174, 'grad_norm': 6.693846225738525, 'learning_rate': 4.6255090168702735e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3473, 'grad_norm': 6.683714866638184, 'learning_rate': 4.62478184991274e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2623, 'grad_norm': 8.260007858276367, 'learning_rate': 4.624054682955207e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4535, 'grad_norm': 7.046111106872559, 'learning_rate': 4.6233275159976736e-05, 'epoch': 0.23}\n",
      "{'loss': 3.367, 'grad_norm': 6.884995937347412, 'learning_rate': 4.6226003490401396e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3219, 'grad_norm': 7.2091569900512695, 'learning_rate': 4.621873182082606e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2119, 'grad_norm': 7.258078575134277, 'learning_rate': 4.621146015125073e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4041, 'grad_norm': 6.489983558654785, 'learning_rate': 4.620418848167539e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2381, 'grad_norm': 7.762601852416992, 'learning_rate': 4.6196916812100064e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3672, 'grad_norm': 7.022415637969971, 'learning_rate': 4.6189645142524724e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2889, 'grad_norm': 7.988768100738525, 'learning_rate': 4.618237347294939e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4281, 'grad_norm': 6.664470672607422, 'learning_rate': 4.617510180337406e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3805, 'grad_norm': 7.3820600509643555, 'learning_rate': 4.6167830133798724e-05, 'epoch': 0.23}\n",
      "{'loss': 3.368, 'grad_norm': 7.366847991943359, 'learning_rate': 4.6160558464223385e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3404, 'grad_norm': 7.057491302490234, 'learning_rate': 4.615328679464805e-05, 'epoch': 0.23}\n",
      "{'loss': 3.183, 'grad_norm': 7.673201560974121, 'learning_rate': 4.614601512507272e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3227, 'grad_norm': 6.869627475738525, 'learning_rate': 4.6138743455497385e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2504, 'grad_norm': 7.633050441741943, 'learning_rate': 4.6131471785922045e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2289, 'grad_norm': 8.04894733428955, 'learning_rate': 4.612420011634672e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3062, 'grad_norm': 8.100444793701172, 'learning_rate': 4.611692844677138e-05, 'epoch': 0.23}\n",
      "{'loss': 3.368, 'grad_norm': 7.625421047210693, 'learning_rate': 4.6109656777196046e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2484, 'grad_norm': 6.741760730743408, 'learning_rate': 4.610238510762071e-05, 'epoch': 0.23}\n",
      "{'loss': 3.3348, 'grad_norm': 7.712244510650635, 'learning_rate': 4.609511343804538e-05, 'epoch': 0.23}\n",
      "{'loss': 3.4309, 'grad_norm': 7.338762283325195, 'learning_rate': 4.608784176847004e-05, 'epoch': 0.23}\n",
      "{'loss': 3.2768, 'grad_norm': 7.050266265869141, 'learning_rate': 4.608057009889471e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3125, 'grad_norm': 7.835916996002197, 'learning_rate': 4.6073298429319374e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3227, 'grad_norm': 6.934577465057373, 'learning_rate': 4.606602675974404e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3145, 'grad_norm': 6.616237163543701, 'learning_rate': 4.60587550901687e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2691, 'grad_norm': 7.076300621032715, 'learning_rate': 4.6051483420593374e-05, 'epoch': 0.24}\n",
      "{'loss': 3.293, 'grad_norm': 7.323471546173096, 'learning_rate': 4.6044211751018035e-05, 'epoch': 0.24}\n",
      "{'loss': 3.415, 'grad_norm': 7.2134246826171875, 'learning_rate': 4.60369400814427e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3531, 'grad_norm': 7.241875171661377, 'learning_rate': 4.602966841186737e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2455, 'grad_norm': 7.20354700088501, 'learning_rate': 4.6022396742292035e-05, 'epoch': 0.24}\n",
      "{'loss': 3.5148, 'grad_norm': 7.687924861907959, 'learning_rate': 4.6015125072716695e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3846, 'grad_norm': 7.684531211853027, 'learning_rate': 4.600785340314136e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2174, 'grad_norm': 7.229401588439941, 'learning_rate': 4.600058173356603e-05, 'epoch': 0.24}\n",
      "  8%|██▋                              | 5500/68760 [1:02:39<11:02:13,  1.59it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.22s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.835426, 'eval_rouge-2': 7.351133999999999, 'eval_rouge-l': 25.423168000000004, 'eval_bleu-4': 0.03531202245629313, 'eval_runtime': 16.7086, 'eval_samples_per_second': 2.992, 'eval_steps_per_second': 0.239, 'epoch': 0.24}\n",
      "  8%|██▋                              | 5500/68760 [1:02:55<11:02:13,  1.59it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.07s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-5500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3225, 'grad_norm': 7.608778953552246, 'learning_rate': 4.5993310063990696e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2008, 'grad_norm': 7.58711576461792, 'learning_rate': 4.5986038394415356e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3682, 'grad_norm': 6.983951568603516, 'learning_rate': 4.597876672484003e-05, 'epoch': 0.24}\n",
      "{'loss': 3.293, 'grad_norm': 9.765800476074219, 'learning_rate': 4.597149505526469e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2439, 'grad_norm': 7.097065448760986, 'learning_rate': 4.596422338568936e-05, 'epoch': 0.24}\n",
      "{'loss': 3.248, 'grad_norm': 7.1172332763671875, 'learning_rate': 4.5956951716114024e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3547, 'grad_norm': 7.5540642738342285, 'learning_rate': 4.594968004653869e-05, 'epoch': 0.24}\n",
      "{'loss': 3.4082, 'grad_norm': 7.650186538696289, 'learning_rate': 4.594240837696335e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2732, 'grad_norm': 7.226182460784912, 'learning_rate': 4.5935136707388024e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3516, 'grad_norm': 6.568320274353027, 'learning_rate': 4.5927865037812685e-05, 'epoch': 0.24}\n",
      "{'loss': 3.5869, 'grad_norm': 6.790306568145752, 'learning_rate': 4.5920593368237345e-05, 'epoch': 0.24}\n",
      "{'loss': 3.3158, 'grad_norm': 6.406524658203125, 'learning_rate': 4.591332169866202e-05, 'epoch': 0.25}\n",
      "{'loss': 3.0875, 'grad_norm': 6.699483871459961, 'learning_rate': 4.590605002908668e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3359, 'grad_norm': 7.024326324462891, 'learning_rate': 4.5898778359511345e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3047, 'grad_norm': 7.292318344116211, 'learning_rate': 4.589150668993601e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2922, 'grad_norm': 7.042468070983887, 'learning_rate': 4.588423502036068e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3643, 'grad_norm': 8.138795852661133, 'learning_rate': 4.587696335078534e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2803, 'grad_norm': 6.781575679779053, 'learning_rate': 4.5869691681210006e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2744, 'grad_norm': 7.556521415710449, 'learning_rate': 4.586242001163467e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3756, 'grad_norm': 7.415856838226318, 'learning_rate': 4.585514834205934e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3766, 'grad_norm': 6.908609867095947, 'learning_rate': 4.5847876672484e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3053, 'grad_norm': 6.79080867767334, 'learning_rate': 4.5840605002908674e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2982, 'grad_norm': 7.227607727050781, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3842, 'grad_norm': 7.1537370681762695, 'learning_rate': 4.5826061663758e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3787, 'grad_norm': 7.1751203536987305, 'learning_rate': 4.581878999418267e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2572, 'grad_norm': 6.57660436630249, 'learning_rate': 4.5811518324607335e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2066, 'grad_norm': 7.454694747924805, 'learning_rate': 4.5804246655031995e-05, 'epoch': 0.25}\n",
      "{'loss': 3.15, 'grad_norm': 7.410243988037109, 'learning_rate': 4.579697498545666e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2113, 'grad_norm': 7.806134223937988, 'learning_rate': 4.578970331588133e-05, 'epoch': 0.25}\n",
      "{'loss': 3.4043, 'grad_norm': 7.070493698120117, 'learning_rate': 4.5782431646305995e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3203, 'grad_norm': 7.02327823638916, 'learning_rate': 4.5775159976730655e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3295, 'grad_norm': 7.381233215332031, 'learning_rate': 4.576788830715533e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3654, 'grad_norm': 6.869640350341797, 'learning_rate': 4.576061663757999e-05, 'epoch': 0.25}\n",
      "{'loss': 3.2799, 'grad_norm': 6.935670375823975, 'learning_rate': 4.5753344968004656e-05, 'epoch': 0.25}\n",
      "{'loss': 3.3156, 'grad_norm': 7.698729038238525, 'learning_rate': 4.574607329842932e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3846, 'grad_norm': 7.120029449462891, 'learning_rate': 4.573880162885399e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4115, 'grad_norm': 7.15385627746582, 'learning_rate': 4.573152995927865e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2182, 'grad_norm': 7.104674816131592, 'learning_rate': 4.572425828970332e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2191, 'grad_norm': 7.2633209228515625, 'learning_rate': 4.5716986620127984e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2928, 'grad_norm': 7.826387405395508, 'learning_rate': 4.570971495055265e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2156, 'grad_norm': 7.041449069976807, 'learning_rate': 4.570244328097731e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2852, 'grad_norm': 7.331432819366455, 'learning_rate': 4.5695171611401985e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3754, 'grad_norm': 6.8698344230651855, 'learning_rate': 4.5687899941826645e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3246, 'grad_norm': 6.581549644470215, 'learning_rate': 4.568062827225131e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3369, 'grad_norm': 7.3092474937438965, 'learning_rate': 4.567335660267598e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2896, 'grad_norm': 6.923274040222168, 'learning_rate': 4.5666084933100645e-05, 'epoch': 0.26}\n",
      "{'loss': 3.202, 'grad_norm': 6.946536540985107, 'learning_rate': 4.5658813263525305e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2707, 'grad_norm': 7.0447998046875, 'learning_rate': 4.565154159394998e-05, 'epoch': 0.26}\n",
      "{'loss': 3.2553, 'grad_norm': 6.902606964111328, 'learning_rate': 4.564426992437464e-05, 'epoch': 0.26}\n",
      "{'loss': 3.0699, 'grad_norm': 7.345515251159668, 'learning_rate': 4.56369982547993e-05, 'epoch': 0.26}\n",
      "  9%|██▉                               | 6000/68760 [1:08:08<9:51:22,  1.77it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.04it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.19s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.882490000000004, 'eval_rouge-2': 7.574552, 'eval_rouge-l': 26.483031999999998, 'eval_bleu-4': 0.037719708477371, 'eval_runtime': 6.6001, 'eval_samples_per_second': 7.576, 'eval_steps_per_second': 0.606, 'epoch': 0.26}\n",
      "  9%|██▉                               | 6000/68760 [1:08:14<9:51:22,  1.77it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.10s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-6000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.284, 'grad_norm': 7.413783550262451, 'learning_rate': 4.562972658522397e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3164, 'grad_norm': 6.734677791595459, 'learning_rate': 4.562245491564863e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3857, 'grad_norm': 7.169139385223389, 'learning_rate': 4.56151832460733e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3898, 'grad_norm': 6.973381042480469, 'learning_rate': 4.560791157649797e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3533, 'grad_norm': 6.7496209144592285, 'learning_rate': 4.5600639906922634e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3184, 'grad_norm': 8.503349304199219, 'learning_rate': 4.5593368237347294e-05, 'epoch': 0.26}\n",
      "{'loss': 3.3316, 'grad_norm': 6.988097190856934, 'learning_rate': 4.558609656777196e-05, 'epoch': 0.26}\n",
      "{'loss': 3.4025, 'grad_norm': 7.067635536193848, 'learning_rate': 4.557882489819663e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2629, 'grad_norm': 7.990600109100342, 'learning_rate': 4.5571553228621295e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3035, 'grad_norm': 8.847268104553223, 'learning_rate': 4.5564281559045955e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2768, 'grad_norm': 6.910191535949707, 'learning_rate': 4.555700988947063e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2777, 'grad_norm': 7.581271171569824, 'learning_rate': 4.554973821989529e-05, 'epoch': 0.27}\n",
      "{'loss': 3.292, 'grad_norm': 7.676822185516357, 'learning_rate': 4.5542466550319955e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3441, 'grad_norm': 7.339413642883301, 'learning_rate': 4.553519488074462e-05, 'epoch': 0.27}\n",
      "{'loss': 3.282, 'grad_norm': 7.325389385223389, 'learning_rate': 4.552792321116929e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3215, 'grad_norm': 7.194974899291992, 'learning_rate': 4.552065154159395e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3453, 'grad_norm': 6.983905792236328, 'learning_rate': 4.5513379872018616e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3123, 'grad_norm': 8.191595077514648, 'learning_rate': 4.550610820244328e-05, 'epoch': 0.27}\n",
      "{'loss': 3.298, 'grad_norm': 8.448797225952148, 'learning_rate': 4.549883653286795e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3967, 'grad_norm': 6.94887638092041, 'learning_rate': 4.549156486329261e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3271, 'grad_norm': 7.810079574584961, 'learning_rate': 4.5484293193717284e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2865, 'grad_norm': 7.332852840423584, 'learning_rate': 4.5477021524141944e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2426, 'grad_norm': 7.403159141540527, 'learning_rate': 4.546974985456661e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2186, 'grad_norm': 6.918664455413818, 'learning_rate': 4.546247818499128e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4078, 'grad_norm': 7.359975337982178, 'learning_rate': 4.5455206515415945e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3225, 'grad_norm': 6.954730987548828, 'learning_rate': 4.5447934845840605e-05, 'epoch': 0.27}\n",
      "{'loss': 3.3561, 'grad_norm': 7.4565300941467285, 'learning_rate': 4.544066317626527e-05, 'epoch': 0.27}\n",
      "{'loss': 3.4273, 'grad_norm': 7.628251552581787, 'learning_rate': 4.543339150668994e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2291, 'grad_norm': 6.6122918128967285, 'learning_rate': 4.5426119837114605e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2066, 'grad_norm': 7.1061248779296875, 'learning_rate': 4.5418848167539266e-05, 'epoch': 0.27}\n",
      "{'loss': 3.2506, 'grad_norm': 7.123733997344971, 'learning_rate': 4.541157649796394e-05, 'epoch': 0.28}\n",
      "{'loss': 3.266, 'grad_norm': 7.669885158538818, 'learning_rate': 4.54043048283886e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4342, 'grad_norm': 7.0816473960876465, 'learning_rate': 4.5397033158813266e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2635, 'grad_norm': 7.487034797668457, 'learning_rate': 4.538976148923793e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2861, 'grad_norm': 7.286117076873779, 'learning_rate': 4.53824898196626e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3062, 'grad_norm': 7.403431415557861, 'learning_rate': 4.537521815008726e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1973, 'grad_norm': 7.233095169067383, 'learning_rate': 4.536794648051193e-05, 'epoch': 0.28}\n",
      "{'loss': 3.4613, 'grad_norm': 7.458419322967529, 'learning_rate': 4.5360674810936594e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1742, 'grad_norm': 7.796756744384766, 'learning_rate': 4.535340314136126e-05, 'epoch': 0.28}\n",
      "{'loss': 3.318, 'grad_norm': 7.368410587310791, 'learning_rate': 4.534613147178592e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3301, 'grad_norm': 7.1720380783081055, 'learning_rate': 4.533885980221059e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2402, 'grad_norm': 7.54795503616333, 'learning_rate': 4.5331588132635255e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1934, 'grad_norm': 7.633383750915527, 'learning_rate': 4.5324316463059915e-05, 'epoch': 0.28}\n",
      "{'loss': 3.1523, 'grad_norm': 8.116466522216797, 'learning_rate': 4.531704479348459e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3891, 'grad_norm': 7.787015914916992, 'learning_rate': 4.530977312390925e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3049, 'grad_norm': 7.967081069946289, 'learning_rate': 4.5302501454333916e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2734, 'grad_norm': 7.569537162780762, 'learning_rate': 4.529522978475858e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2508, 'grad_norm': 7.213386535644531, 'learning_rate': 4.528795811518325e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3438, 'grad_norm': 7.2098493576049805, 'learning_rate': 4.528068644560791e-05, 'epoch': 0.28}\n",
      "{'loss': 3.317, 'grad_norm': 6.9897003173828125, 'learning_rate': 4.527341477603258e-05, 'epoch': 0.28}\n",
      "  9%|███                              | 6500/68760 [1:13:24<11:33:18,  1.50it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.12s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.61s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 30.860464, 'eval_rouge-2': 6.749766000000001, 'eval_rouge-l': 24.418632, 'eval_bleu-4': 0.033701549726981764, 'eval_runtime': 17.8765, 'eval_samples_per_second': 2.797, 'eval_steps_per_second': 0.224, 'epoch': 0.28}\n",
      "  9%|███                              | 6500/68760 [1:13:42<11:33:18,  1.50it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  4.07s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-6500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3928, 'grad_norm': 7.214720249176025, 'learning_rate': 4.526614310645724e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2928, 'grad_norm': 7.478385925292969, 'learning_rate': 4.525887143688191e-05, 'epoch': 0.28}\n",
      "{'loss': 3.2266, 'grad_norm': 7.196005344390869, 'learning_rate': 4.525159976730658e-05, 'epoch': 0.28}\n",
      "{'loss': 3.3342, 'grad_norm': 8.311795234680176, 'learning_rate': 4.5244328097731244e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3256, 'grad_norm': 8.08540153503418, 'learning_rate': 4.5237056428155904e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3527, 'grad_norm': 8.129487991333008, 'learning_rate': 4.522978475858057e-05, 'epoch': 0.29}\n",
      "{'loss': 3.325, 'grad_norm': 7.3645243644714355, 'learning_rate': 4.522251308900524e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2465, 'grad_norm': 7.186620712280273, 'learning_rate': 4.5215241419429905e-05, 'epoch': 0.29}\n",
      "{'loss': 3.282, 'grad_norm': 7.4143242835998535, 'learning_rate': 4.5207969749854565e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3729, 'grad_norm': 7.730903625488281, 'learning_rate': 4.520069808027924e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3307, 'grad_norm': 7.627574443817139, 'learning_rate': 4.51934264107039e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2307, 'grad_norm': 7.245645046234131, 'learning_rate': 4.5186154741128566e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3123, 'grad_norm': 7.982807636260986, 'learning_rate': 4.517888307155323e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2852, 'grad_norm': 7.096236228942871, 'learning_rate': 4.51716114019779e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2969, 'grad_norm': 8.165536880493164, 'learning_rate': 4.516433973240256e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3404, 'grad_norm': 7.8432111740112305, 'learning_rate': 4.5157068062827226e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3822, 'grad_norm': 7.9521050453186035, 'learning_rate': 4.514979639325189e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2143, 'grad_norm': 6.8778076171875, 'learning_rate': 4.514252472367656e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2611, 'grad_norm': 7.477982044219971, 'learning_rate': 4.513525305410122e-05, 'epoch': 0.29}\n",
      "{'loss': 3.273, 'grad_norm': 7.886739730834961, 'learning_rate': 4.5127981384525894e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2703, 'grad_norm': 7.13758659362793, 'learning_rate': 4.5120709714950554e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2361, 'grad_norm': 7.526783466339111, 'learning_rate': 4.511343804537522e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2457, 'grad_norm': 8.153390884399414, 'learning_rate': 4.510616637579989e-05, 'epoch': 0.29}\n",
      "{'loss': 3.3381, 'grad_norm': 7.702274799346924, 'learning_rate': 4.5098894706224555e-05, 'epoch': 0.29}\n",
      "{'loss': 3.2984, 'grad_norm': 6.906006336212158, 'learning_rate': 4.5091623036649215e-05, 'epoch': 0.29}\n",
      "{'loss': 3.26, 'grad_norm': 7.020072937011719, 'learning_rate': 4.508435136707388e-05, 'epoch': 0.29}\n",
      "{'loss': 3.1367, 'grad_norm': 7.230121612548828, 'learning_rate': 4.507707969749855e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3342, 'grad_norm': 7.9601593017578125, 'learning_rate': 4.5069808027923216e-05, 'epoch': 0.3}\n",
      "{'loss': 3.2766, 'grad_norm': 7.400661468505859, 'learning_rate': 4.5062536358347876e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3098, 'grad_norm': 6.876334190368652, 'learning_rate': 4.505526468877254e-05, 'epoch': 0.3}\n",
      "{'loss': 3.234, 'grad_norm': 7.739489555358887, 'learning_rate': 4.504799301919721e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3299, 'grad_norm': 7.4504194259643555, 'learning_rate': 4.504072134962187e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3406, 'grad_norm': 7.534814357757568, 'learning_rate': 4.503344968004654e-05, 'epoch': 0.3}\n",
      "{'loss': 3.2178, 'grad_norm': 6.731123447418213, 'learning_rate': 4.50261780104712e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3184, 'grad_norm': 7.569042682647705, 'learning_rate': 4.501890634089587e-05, 'epoch': 0.3}\n",
      "{'loss': 3.2889, 'grad_norm': 7.48179292678833, 'learning_rate': 4.501163467132054e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3244, 'grad_norm': 7.536869525909424, 'learning_rate': 4.5004363001745204e-05, 'epoch': 0.3}\n",
      "{'loss': 3.2803, 'grad_norm': 7.3373212814331055, 'learning_rate': 4.4997091332169864e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3523, 'grad_norm': 7.51869535446167, 'learning_rate': 4.498981966259454e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3162, 'grad_norm': 6.980216979980469, 'learning_rate': 4.49825479930192e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3742, 'grad_norm': 7.391360759735107, 'learning_rate': 4.4975276323443865e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3137, 'grad_norm': 7.313505172729492, 'learning_rate': 4.496800465386853e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3164, 'grad_norm': 6.950220108032227, 'learning_rate': 4.49607329842932e-05, 'epoch': 0.3}\n",
      "{'loss': 3.2951, 'grad_norm': 7.154389381408691, 'learning_rate': 4.495346131471786e-05, 'epoch': 0.3}\n",
      "{'loss': 3.2719, 'grad_norm': 7.222228050231934, 'learning_rate': 4.4946189645142526e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3053, 'grad_norm': 7.195568561553955, 'learning_rate': 4.493891797556719e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3764, 'grad_norm': 7.638105392456055, 'learning_rate': 4.493164630599186e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3371, 'grad_norm': 7.841986179351807, 'learning_rate': 4.492437463641652e-05, 'epoch': 0.3}\n",
      "{'loss': 3.1379, 'grad_norm': 8.029505729675293, 'learning_rate': 4.491710296684119e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3887, 'grad_norm': 7.366346836090088, 'learning_rate': 4.490983129726585e-05, 'epoch': 0.31}\n",
      " 10%|███▎                             | 7000/68760 [1:18:52<11:39:14,  1.47it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.30s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.916382000000002, 'eval_rouge-2': 7.486232, 'eval_rouge-l': 23.732188, 'eval_bleu-4': 0.03327971850888186, 'eval_runtime': 35.5079, 'eval_samples_per_second': 1.408, 'eval_steps_per_second': 0.113, 'epoch': 0.31}\n",
      " 10%|███▎                             | 7000/68760 [1:19:27<11:39:14,  1.47it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:23<00:00,  6.10s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-7000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1715, 'grad_norm': 7.593285083770752, 'learning_rate': 4.490255962769052e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2945, 'grad_norm': 7.0028204917907715, 'learning_rate': 4.489528795811519e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2594, 'grad_norm': 7.113710403442383, 'learning_rate': 4.4888016288539854e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3682, 'grad_norm': 6.820965766906738, 'learning_rate': 4.4880744618964514e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3287, 'grad_norm': 6.785414218902588, 'learning_rate': 4.487347294938918e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2764, 'grad_norm': 6.992018699645996, 'learning_rate': 4.486620127981385e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2732, 'grad_norm': 7.310014724731445, 'learning_rate': 4.4858929610238515e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2441, 'grad_norm': 8.214975357055664, 'learning_rate': 4.4851657940663175e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3516, 'grad_norm': 7.2968525886535645, 'learning_rate': 4.484438627108785e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3689, 'grad_norm': 6.748539447784424, 'learning_rate': 4.483711460151251e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3309, 'grad_norm': 8.042869567871094, 'learning_rate': 4.4829842931937176e-05, 'epoch': 0.31}\n",
      "{'loss': 3.192, 'grad_norm': 7.2340407371521, 'learning_rate': 4.482257126236184e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2072, 'grad_norm': 7.852964878082275, 'learning_rate': 4.481529959278651e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3355, 'grad_norm': 7.3039703369140625, 'learning_rate': 4.480802792321117e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3605, 'grad_norm': 7.454050540924072, 'learning_rate': 4.4800756253635836e-05, 'epoch': 0.31}\n",
      "{'loss': 3.4504, 'grad_norm': 7.9356160163879395, 'learning_rate': 4.47934845840605e-05, 'epoch': 0.31}\n",
      "{'loss': 3.242, 'grad_norm': 6.988646030426025, 'learning_rate': 4.478621291448517e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3176, 'grad_norm': 7.019476890563965, 'learning_rate': 4.477894124490983e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2549, 'grad_norm': 7.290840148925781, 'learning_rate': 4.4771669575334504e-05, 'epoch': 0.31}\n",
      "{'loss': 3.3773, 'grad_norm': 7.839923858642578, 'learning_rate': 4.4764397905759164e-05, 'epoch': 0.31}\n",
      "{'loss': 3.2309, 'grad_norm': 6.9919304847717285, 'learning_rate': 4.4757126236183824e-05, 'epoch': 0.31}\n",
      "{'loss': 3.324, 'grad_norm': 6.929099082946777, 'learning_rate': 4.47498545666085e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2166, 'grad_norm': 8.652189254760742, 'learning_rate': 4.474258289703316e-05, 'epoch': 0.32}\n",
      "{'loss': 3.193, 'grad_norm': 8.014322280883789, 'learning_rate': 4.4735311227457825e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3457, 'grad_norm': 7.73017692565918, 'learning_rate': 4.472803955788249e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3453, 'grad_norm': 7.023691177368164, 'learning_rate': 4.472076788830716e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2904, 'grad_norm': 7.481243133544922, 'learning_rate': 4.471349621873182e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2955, 'grad_norm': 8.265152931213379, 'learning_rate': 4.4706224549156486e-05, 'epoch': 0.32}\n",
      "{'loss': 3.167, 'grad_norm': 7.559632301330566, 'learning_rate': 4.469895287958115e-05, 'epoch': 0.32}\n",
      "{'loss': 3.4, 'grad_norm': 6.84234619140625, 'learning_rate': 4.469168121000582e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2072, 'grad_norm': 8.032352447509766, 'learning_rate': 4.468440954043048e-05, 'epoch': 0.32}\n",
      "{'loss': 3.252, 'grad_norm': 7.385690689086914, 'learning_rate': 4.467713787085515e-05, 'epoch': 0.32}\n",
      "{'loss': 3.0699, 'grad_norm': 8.100035667419434, 'learning_rate': 4.4669866201279813e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3453, 'grad_norm': 7.850386619567871, 'learning_rate': 4.466259453170448e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2295, 'grad_norm': 6.588375091552734, 'learning_rate': 4.465532286212915e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3082, 'grad_norm': 7.281150817871094, 'learning_rate': 4.4648051192553814e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3391, 'grad_norm': 8.14915943145752, 'learning_rate': 4.4640779522978474e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2432, 'grad_norm': 7.615752220153809, 'learning_rate': 4.463350785340315e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2314, 'grad_norm': 7.46631383895874, 'learning_rate': 4.462623618382781e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2971, 'grad_norm': 8.17763614654541, 'learning_rate': 4.4618964514252475e-05, 'epoch': 0.32}\n",
      "{'loss': 3.258, 'grad_norm': 7.257068634033203, 'learning_rate': 4.461169284467714e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3479, 'grad_norm': 6.782087326049805, 'learning_rate': 4.460442117510181e-05, 'epoch': 0.32}\n",
      "{'loss': 3.2131, 'grad_norm': 7.035430431365967, 'learning_rate': 4.459714950552647e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3178, 'grad_norm': 7.24170446395874, 'learning_rate': 4.4589877835951136e-05, 'epoch': 0.32}\n",
      "{'loss': 3.3148, 'grad_norm': 7.4030914306640625, 'learning_rate': 4.45826061663758e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3273, 'grad_norm': 7.381016254425049, 'learning_rate': 4.457533449680047e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2801, 'grad_norm': 7.398068428039551, 'learning_rate': 4.456806282722513e-05, 'epoch': 0.33}\n",
      "{'loss': 3.1818, 'grad_norm': 7.570266246795654, 'learning_rate': 4.45607911576498e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3004, 'grad_norm': 7.728521347045898, 'learning_rate': 4.4553519488074463e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2816, 'grad_norm': 8.299247741699219, 'learning_rate': 4.454624781849913e-05, 'epoch': 0.33}\n",
      " 11%|███▌                             | 7500/68760 [1:24:37<10:34:21,  1.61it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.21s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.751944, 'eval_rouge-2': 7.891798000000001, 'eval_rouge-l': 25.918026, 'eval_bleu-4': 0.04073123125221523, 'eval_runtime': 16.8694, 'eval_samples_per_second': 2.964, 'eval_steps_per_second': 0.237, 'epoch': 0.33}\n",
      " 11%|███▌                             | 7500/68760 [1:24:54<10:34:21,  1.61it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.01s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-7500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.365, 'grad_norm': 7.612710475921631, 'learning_rate': 4.45389761489238e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2266, 'grad_norm': 8.336224555969238, 'learning_rate': 4.4531704479348464e-05, 'epoch': 0.33}\n",
      "{'loss': 3.1729, 'grad_norm': 8.98204231262207, 'learning_rate': 4.4524432809773124e-05, 'epoch': 0.33}\n",
      "{'loss': 3.1475, 'grad_norm': 7.552983283996582, 'learning_rate': 4.451716114019779e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3014, 'grad_norm': 7.953640937805176, 'learning_rate': 4.450988947062246e-05, 'epoch': 0.33}\n",
      "{'loss': 3.226, 'grad_norm': 7.244723320007324, 'learning_rate': 4.4502617801047125e-05, 'epoch': 0.33}\n",
      "{'loss': 3.293, 'grad_norm': 7.327325820922852, 'learning_rate': 4.4495346131471785e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2488, 'grad_norm': 7.530031204223633, 'learning_rate': 4.448807446189646e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2127, 'grad_norm': 7.404933929443359, 'learning_rate': 4.448080279232112e-05, 'epoch': 0.33}\n",
      "{'loss': 3.1912, 'grad_norm': 7.1495795249938965, 'learning_rate': 4.447353112274578e-05, 'epoch': 0.33}\n",
      "{'loss': 3.4336, 'grad_norm': 7.29098653793335, 'learning_rate': 4.446625945317045e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2094, 'grad_norm': 7.140941619873047, 'learning_rate': 4.445898778359511e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2684, 'grad_norm': 7.158839225769043, 'learning_rate': 4.445171611401978e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2131, 'grad_norm': 6.971324443817139, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3043, 'grad_norm': 6.517827033996582, 'learning_rate': 4.4437172774869113e-05, 'epoch': 0.33}\n",
      "{'loss': 3.2273, 'grad_norm': 7.030611515045166, 'learning_rate': 4.4429901105293774e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3607, 'grad_norm': 8.179930686950684, 'learning_rate': 4.442262943571844e-05, 'epoch': 0.33}\n",
      "{'loss': 3.3236, 'grad_norm': 6.831953525543213, 'learning_rate': 4.441535776614311e-05, 'epoch': 0.34}\n",
      "{'loss': 3.308, 'grad_norm': 6.936872959136963, 'learning_rate': 4.4408086096567774e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3162, 'grad_norm': 7.350867748260498, 'learning_rate': 4.4400814426992434e-05, 'epoch': 0.34}\n",
      "{'loss': 3.2836, 'grad_norm': 8.258139610290527, 'learning_rate': 4.439354275741711e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3318, 'grad_norm': 7.337821006774902, 'learning_rate': 4.438627108784177e-05, 'epoch': 0.34}\n",
      "{'loss': 3.293, 'grad_norm': 6.644591331481934, 'learning_rate': 4.4378999418266435e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3039, 'grad_norm': 6.7064080238342285, 'learning_rate': 4.43717277486911e-05, 'epoch': 0.34}\n",
      "{'loss': 3.266, 'grad_norm': 7.754260540008545, 'learning_rate': 4.436445607911577e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3359, 'grad_norm': 8.707462310791016, 'learning_rate': 4.435718440954043e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3537, 'grad_norm': 7.8723955154418945, 'learning_rate': 4.43499127399651e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3758, 'grad_norm': 7.132078170776367, 'learning_rate': 4.434264107038976e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4051, 'grad_norm': 7.4325480461120605, 'learning_rate': 4.433536940081443e-05, 'epoch': 0.34}\n",
      "{'loss': 3.348, 'grad_norm': 7.869150161743164, 'learning_rate': 4.4328097731239097e-05, 'epoch': 0.34}\n",
      "{'loss': 3.3863, 'grad_norm': 8.09893798828125, 'learning_rate': 4.4320826061663763e-05, 'epoch': 0.34}\n",
      "{'loss': 3.2729, 'grad_norm': 7.168398380279541, 'learning_rate': 4.4313554392088424e-05, 'epoch': 0.34}\n",
      "{'loss': 3.243, 'grad_norm': 7.488523006439209, 'learning_rate': 4.430628272251309e-05, 'epoch': 0.34}\n",
      "{'loss': 3.2129, 'grad_norm': 7.558568477630615, 'learning_rate': 4.429901105293776e-05, 'epoch': 0.34}\n",
      "{'loss': 3.301, 'grad_norm': 8.398404121398926, 'learning_rate': 4.4291739383362424e-05, 'epoch': 0.34}\n",
      "{'loss': 3.1377, 'grad_norm': 7.4289984703063965, 'learning_rate': 4.4284467713787084e-05, 'epoch': 0.34}\n",
      "{'loss': 3.2004, 'grad_norm': 6.764904022216797, 'learning_rate': 4.427719604421176e-05, 'epoch': 0.34}\n",
      "{'loss': 3.0457, 'grad_norm': 7.705386161804199, 'learning_rate': 4.426992437463642e-05, 'epoch': 0.34}\n",
      "{'loss': 3.1793, 'grad_norm': 7.8847880363464355, 'learning_rate': 4.4262652705061085e-05, 'epoch': 0.34}\n",
      "{'loss': 3.4131, 'grad_norm': 7.673657417297363, 'learning_rate': 4.425538103548575e-05, 'epoch': 0.34}\n",
      "{'loss': 3.185, 'grad_norm': 7.546092987060547, 'learning_rate': 4.424810936591042e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3051, 'grad_norm': 7.984483242034912, 'learning_rate': 4.424083769633508e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2811, 'grad_norm': 6.91810417175293, 'learning_rate': 4.4233566026759746e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3176, 'grad_norm': 7.458583831787109, 'learning_rate': 4.422629435718441e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2703, 'grad_norm': 8.214866638183594, 'learning_rate': 4.421902268760908e-05, 'epoch': 0.35}\n",
      "{'loss': 3.35, 'grad_norm': 6.778419017791748, 'learning_rate': 4.421175101803374e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3496, 'grad_norm': 7.626506805419922, 'learning_rate': 4.4204479348458413e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2525, 'grad_norm': 7.078497886657715, 'learning_rate': 4.4197207678883074e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2408, 'grad_norm': 8.302969932556152, 'learning_rate': 4.418993600930774e-05, 'epoch': 0.35}\n",
      "{'loss': 3.1227, 'grad_norm': 7.0584187507629395, 'learning_rate': 4.418266433973241e-05, 'epoch': 0.35}\n",
      " 12%|███▊                             | 8000/68760 [1:30:02<10:42:08,  1.58it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.05s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.28s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.699412, 'eval_rouge-2': 7.735691999999999, 'eval_rouge-l': 25.729038, 'eval_bleu-4': 0.03977365335844091, 'eval_runtime': 17.2859, 'eval_samples_per_second': 2.893, 'eval_steps_per_second': 0.231, 'epoch': 0.35}\n",
      " 12%|███▊                             | 8000/68760 [1:30:20<10:42:08,  1.58it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.26s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-8000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3277, 'grad_norm': 7.412379741668701, 'learning_rate': 4.417539267015707e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3656, 'grad_norm': 7.342109680175781, 'learning_rate': 4.4168121000581734e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2955, 'grad_norm': 6.904810428619385, 'learning_rate': 4.41608493310064e-05, 'epoch': 0.35}\n",
      "{'loss': 3.1936, 'grad_norm': 6.813215255737305, 'learning_rate': 4.415357766143107e-05, 'epoch': 0.35}\n",
      "{'loss': 3.273, 'grad_norm': 6.932103633880615, 'learning_rate': 4.414630599185573e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2365, 'grad_norm': 7.604308128356934, 'learning_rate': 4.4139034322280395e-05, 'epoch': 0.35}\n",
      "{'loss': 3.1865, 'grad_norm': 7.399198055267334, 'learning_rate': 4.413176265270506e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2777, 'grad_norm': 7.482645511627197, 'learning_rate': 4.412449098312973e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2783, 'grad_norm': 7.165125370025635, 'learning_rate': 4.411721931355439e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3129, 'grad_norm': 6.565908432006836, 'learning_rate': 4.410994764397906e-05, 'epoch': 0.35}\n",
      "{'loss': 3.3389, 'grad_norm': 7.403790473937988, 'learning_rate': 4.410267597440372e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2209, 'grad_norm': 7.325875759124756, 'learning_rate': 4.409540430482839e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2916, 'grad_norm': 6.910371780395508, 'learning_rate': 4.408813263525306e-05, 'epoch': 0.35}\n",
      "{'loss': 3.2379, 'grad_norm': 7.33497428894043, 'learning_rate': 4.4080860965677724e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2791, 'grad_norm': 6.957594394683838, 'learning_rate': 4.4073589296102384e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2967, 'grad_norm': 7.334203243255615, 'learning_rate': 4.406631762652705e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2314, 'grad_norm': 7.125458240509033, 'learning_rate': 4.405904595695172e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2904, 'grad_norm': 7.698392391204834, 'learning_rate': 4.4051774287376384e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3305, 'grad_norm': 6.91762638092041, 'learning_rate': 4.4044502617801045e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2785, 'grad_norm': 8.45223331451416, 'learning_rate': 4.403723094822572e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2004, 'grad_norm': 8.410500526428223, 'learning_rate': 4.402995927865038e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2627, 'grad_norm': 6.887122631072998, 'learning_rate': 4.4022687609075045e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3648, 'grad_norm': 7.520719051361084, 'learning_rate': 4.401541593949971e-05, 'epoch': 0.36}\n",
      "{'loss': 3.4143, 'grad_norm': 7.323500156402588, 'learning_rate': 4.400814426992438e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2107, 'grad_norm': 8.627363204956055, 'learning_rate': 4.400087260034904e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3566, 'grad_norm': 6.772165775299072, 'learning_rate': 4.399360093077371e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3318, 'grad_norm': 7.485940456390381, 'learning_rate': 4.398632926119837e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3576, 'grad_norm': 8.631155014038086, 'learning_rate': 4.397905759162304e-05, 'epoch': 0.36}\n",
      "{'loss': 3.1355, 'grad_norm': 6.9211931228637695, 'learning_rate': 4.397178592204771e-05, 'epoch': 0.36}\n",
      "{'loss': 3.307, 'grad_norm': 7.129084587097168, 'learning_rate': 4.3964514252472374e-05, 'epoch': 0.36}\n",
      "{'loss': 3.3064, 'grad_norm': 7.345803737640381, 'learning_rate': 4.3957242582897034e-05, 'epoch': 0.36}\n",
      "{'loss': 3.1688, 'grad_norm': 8.401378631591797, 'learning_rate': 4.39499709133217e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2699, 'grad_norm': 6.8919548988342285, 'learning_rate': 4.394269924374637e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2238, 'grad_norm': 6.8884735107421875, 'learning_rate': 4.3935427574171034e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2371, 'grad_norm': 6.765687942504883, 'learning_rate': 4.3928155904595695e-05, 'epoch': 0.36}\n",
      "{'loss': 3.2299, 'grad_norm': 7.584731101989746, 'learning_rate': 4.392088423502037e-05, 'epoch': 0.36}\n",
      "{'loss': 3.1861, 'grad_norm': 8.28421401977539, 'learning_rate': 4.391361256544503e-05, 'epoch': 0.37}\n",
      "{'loss': 3.402, 'grad_norm': 8.522652626037598, 'learning_rate': 4.3906340895869695e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3275, 'grad_norm': 8.363822937011719, 'learning_rate': 4.389906922629436e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3088, 'grad_norm': 7.247319221496582, 'learning_rate': 4.389179755671902e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2408, 'grad_norm': 7.537041664123535, 'learning_rate': 4.388452588714369e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2812, 'grad_norm': 7.2771782875061035, 'learning_rate': 4.3877254217568356e-05, 'epoch': 0.37}\n",
      "{'loss': 3.157, 'grad_norm': 7.389171600341797, 'learning_rate': 4.386998254799302e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2629, 'grad_norm': 7.627345561981201, 'learning_rate': 4.386271087841768e-05, 'epoch': 0.37}\n",
      "{'loss': 3.31, 'grad_norm': 7.881656169891357, 'learning_rate': 4.385543920884235e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2184, 'grad_norm': 7.426987648010254, 'learning_rate': 4.384816753926702e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2068, 'grad_norm': 7.277919292449951, 'learning_rate': 4.3840895869691684e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2904, 'grad_norm': 7.5973687171936035, 'learning_rate': 4.3833624200116344e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3258, 'grad_norm': 7.3271002769470215, 'learning_rate': 4.382635253054102e-05, 'epoch': 0.37}\n",
      "{'loss': 3.1986, 'grad_norm': 7.892650604248047, 'learning_rate': 4.381908086096568e-05, 'epoch': 0.37}\n",
      " 12%|████▏                             | 8500/68760 [1:35:28<9:44:00,  1.72it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.26s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.388186, 'eval_rouge-2': 7.4152580000000015, 'eval_rouge-l': 24.815806, 'eval_bleu-4': 0.03496067307325056, 'eval_runtime': 27.6281, 'eval_samples_per_second': 1.81, 'eval_steps_per_second': 0.145, 'epoch': 0.37}\n",
      " 12%|████▏                             | 8500/68760 [1:35:55<9:44:00,  1.72it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.18s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-8500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3049, 'grad_norm': 7.050774574279785, 'learning_rate': 4.3811809191390344e-05, 'epoch': 0.37}\n",
      "{'loss': 3.1453, 'grad_norm': 7.737212657928467, 'learning_rate': 4.380453752181501e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3066, 'grad_norm': 7.32334566116333, 'learning_rate': 4.379726585223968e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2619, 'grad_norm': 8.114749908447266, 'learning_rate': 4.378999418266434e-05, 'epoch': 0.37}\n",
      "{'loss': 3.1625, 'grad_norm': 7.307359218597412, 'learning_rate': 4.3782722513089005e-05, 'epoch': 0.37}\n",
      "{'loss': 3.3279, 'grad_norm': 7.962070941925049, 'learning_rate': 4.377545084351367e-05, 'epoch': 0.37}\n",
      "{'loss': 3.1617, 'grad_norm': 7.885305404663086, 'learning_rate': 4.376817917393834e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2639, 'grad_norm': 7.218836307525635, 'learning_rate': 4.3760907504363e-05, 'epoch': 0.37}\n",
      "{'loss': 3.148, 'grad_norm': 8.228680610656738, 'learning_rate': 4.375363583478767e-05, 'epoch': 0.37}\n",
      "{'loss': 3.2984, 'grad_norm': 21.303836822509766, 'learning_rate': 4.374636416521233e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2605, 'grad_norm': 7.206628799438477, 'learning_rate': 4.3739092495637e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2652, 'grad_norm': 8.463595390319824, 'learning_rate': 4.373182082606167e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2715, 'grad_norm': 7.384931564331055, 'learning_rate': 4.3724549156486334e-05, 'epoch': 0.38}\n",
      "{'loss': 3.4406, 'grad_norm': 7.188694000244141, 'learning_rate': 4.3717277486910994e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3211, 'grad_norm': 7.558350086212158, 'learning_rate': 4.371000581733567e-05, 'epoch': 0.38}\n",
      "{'loss': 3.199, 'grad_norm': 7.303985118865967, 'learning_rate': 4.370273414776033e-05, 'epoch': 0.38}\n",
      "{'loss': 3.241, 'grad_norm': 7.23896598815918, 'learning_rate': 4.3695462478184994e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2613, 'grad_norm': 7.743210792541504, 'learning_rate': 4.368819080860966e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2992, 'grad_norm': 8.349611282348633, 'learning_rate': 4.368091913903433e-05, 'epoch': 0.38}\n",
      "{'loss': 3.227, 'grad_norm': 8.26275634765625, 'learning_rate': 4.367364746945899e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2061, 'grad_norm': 7.2477030754089355, 'learning_rate': 4.3666375799883655e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3537, 'grad_norm': 7.950453281402588, 'learning_rate': 4.365910413030832e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2334, 'grad_norm': 8.522682189941406, 'learning_rate': 4.365183246073299e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2561, 'grad_norm': 7.336171627044678, 'learning_rate': 4.364456079115765e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2605, 'grad_norm': 7.528255939483643, 'learning_rate': 4.363728912158232e-05, 'epoch': 0.38}\n",
      "{'loss': 3.352, 'grad_norm': 7.32082986831665, 'learning_rate': 4.363001745200698e-05, 'epoch': 0.38}\n",
      "{'loss': 3.1588, 'grad_norm': 8.024406433105469, 'learning_rate': 4.362274578243165e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2113, 'grad_norm': 8.65680980682373, 'learning_rate': 4.361547411285632e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2621, 'grad_norm': 7.517584800720215, 'learning_rate': 4.3608202443280984e-05, 'epoch': 0.38}\n",
      "{'loss': 3.0373, 'grad_norm': 7.825660705566406, 'learning_rate': 4.3600930773705644e-05, 'epoch': 0.38}\n",
      "{'loss': 3.2174, 'grad_norm': 6.737121105194092, 'learning_rate': 4.359365910413031e-05, 'epoch': 0.38}\n",
      "{'loss': 3.3756, 'grad_norm': 7.951241493225098, 'learning_rate': 4.358638743455498e-05, 'epoch': 0.38}\n",
      "{'loss': 3.0932, 'grad_norm': 7.5059590339660645, 'learning_rate': 4.357911576497964e-05, 'epoch': 0.39}\n",
      "{'loss': 3.4207, 'grad_norm': 7.551892280578613, 'learning_rate': 4.3571844095404305e-05, 'epoch': 0.39}\n",
      "{'loss': 3.307, 'grad_norm': 7.0103759765625, 'learning_rate': 4.356457242582897e-05, 'epoch': 0.39}\n",
      "{'loss': 3.1613, 'grad_norm': 7.745114803314209, 'learning_rate': 4.355730075625364e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3359, 'grad_norm': 7.367232322692871, 'learning_rate': 4.35500290866783e-05, 'epoch': 0.39}\n",
      "{'loss': 3.259, 'grad_norm': 7.365668296813965, 'learning_rate': 4.354275741710297e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3414, 'grad_norm': 7.682211399078369, 'learning_rate': 4.353548574752763e-05, 'epoch': 0.39}\n",
      "{'loss': 3.2437, 'grad_norm': 7.411280155181885, 'learning_rate': 4.35282140779523e-05, 'epoch': 0.39}\n",
      "{'loss': 3.2025, 'grad_norm': 7.711309432983398, 'learning_rate': 4.3520942408376966e-05, 'epoch': 0.39}\n",
      "{'loss': 3.2256, 'grad_norm': 7.132777690887451, 'learning_rate': 4.351367073880163e-05, 'epoch': 0.39}\n",
      "{'loss': 3.2119, 'grad_norm': 8.01865005493164, 'learning_rate': 4.350639906922629e-05, 'epoch': 0.39}\n",
      "{'loss': 3.158, 'grad_norm': 7.362518787384033, 'learning_rate': 4.349912739965096e-05, 'epoch': 0.39}\n",
      "{'loss': 3.202, 'grad_norm': 7.846218109130859, 'learning_rate': 4.349185573007563e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3143, 'grad_norm': 7.489795207977295, 'learning_rate': 4.3484584060500294e-05, 'epoch': 0.39}\n",
      "{'loss': 3.207, 'grad_norm': 7.700545787811279, 'learning_rate': 4.3477312390924954e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3463, 'grad_norm': 7.955310344696045, 'learning_rate': 4.347004072134963e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3119, 'grad_norm': 7.05980920791626, 'learning_rate': 4.346276905177429e-05, 'epoch': 0.39}\n",
      "{'loss': 3.2588, 'grad_norm': 7.773429870605469, 'learning_rate': 4.3455497382198955e-05, 'epoch': 0.39}\n",
      " 13%|████▍                             | 9000/68760 [1:41:04<9:35:44,  1.73it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.07it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:05,  5.50s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.045488, 'eval_rouge-2': 7.91865, 'eval_rouge-l': 25.076278, 'eval_bleu-4': 0.040500654370682995, 'eval_runtime': 27.2755, 'eval_samples_per_second': 1.833, 'eval_steps_per_second': 0.147, 'epoch': 0.39}\n",
      " 13%|████▍                             | 9000/68760 [1:41:31<9:35:44,  1.73it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.87s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-9000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2953, 'grad_norm': 7.128548622131348, 'learning_rate': 4.344822571262362e-05, 'epoch': 0.39}\n",
      "{'loss': 3.1787, 'grad_norm': 9.909671783447266, 'learning_rate': 4.344095404304829e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3576, 'grad_norm': 7.924907207489014, 'learning_rate': 4.343368237347295e-05, 'epoch': 0.39}\n",
      "{'loss': 3.1768, 'grad_norm': 8.552227020263672, 'learning_rate': 4.3426410703897615e-05, 'epoch': 0.39}\n",
      "{'loss': 3.3275, 'grad_norm': 7.433839797973633, 'learning_rate': 4.341913903432228e-05, 'epoch': 0.39}\n",
      "{'loss': 3.2342, 'grad_norm': 7.129913330078125, 'learning_rate': 4.341186736474695e-05, 'epoch': 0.4}\n",
      "{'loss': 3.1867, 'grad_norm': 7.748262882232666, 'learning_rate': 4.340459569517161e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2785, 'grad_norm': 7.164796829223633, 'learning_rate': 4.339732402559628e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2137, 'grad_norm': 7.8112640380859375, 'learning_rate': 4.339005235602094e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2152, 'grad_norm': 7.937103748321533, 'learning_rate': 4.338278068644561e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3578, 'grad_norm': 7.708724021911621, 'learning_rate': 4.337550901687028e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2133, 'grad_norm': 7.39786958694458, 'learning_rate': 4.3368237347294944e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3002, 'grad_norm': 6.7032952308654785, 'learning_rate': 4.3360965677719604e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3143, 'grad_norm': 7.884064674377441, 'learning_rate': 4.335369400814428e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2822, 'grad_norm': 8.243814468383789, 'learning_rate': 4.334642233856894e-05, 'epoch': 0.4}\n",
      "{'loss': 3.1236, 'grad_norm': 7.068573474884033, 'learning_rate': 4.3339150668993605e-05, 'epoch': 0.4}\n",
      "{'loss': 3.1934, 'grad_norm': 8.475379943847656, 'learning_rate': 4.333187899941827e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2379, 'grad_norm': 7.0203776359558105, 'learning_rate': 4.332460732984294e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2572, 'grad_norm': 16.780988693237305, 'learning_rate': 4.33173356602676e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2256, 'grad_norm': 7.431773662567139, 'learning_rate': 4.3310063990692265e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3635, 'grad_norm': 6.991963863372803, 'learning_rate': 4.330279232111693e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2162, 'grad_norm': 6.737697124481201, 'learning_rate': 4.329552065154159e-05, 'epoch': 0.4}\n",
      "{'loss': 3.4301, 'grad_norm': 6.586991310119629, 'learning_rate': 4.328824898196626e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3248, 'grad_norm': 7.22230863571167, 'learning_rate': 4.3280977312390926e-05, 'epoch': 0.4}\n",
      "{'loss': 3.3227, 'grad_norm': 7.4245524406433105, 'learning_rate': 4.327370564281559e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2607, 'grad_norm': 7.99364709854126, 'learning_rate': 4.326643397324025e-05, 'epoch': 0.4}\n",
      "{'loss': 3.351, 'grad_norm': 6.748149871826172, 'learning_rate': 4.325916230366493e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2391, 'grad_norm': 6.702996253967285, 'learning_rate': 4.325189063408959e-05, 'epoch': 0.4}\n",
      "{'loss': 3.2277, 'grad_norm': 8.781631469726562, 'learning_rate': 4.3244618964514254e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2107, 'grad_norm': 6.779776573181152, 'learning_rate': 4.323734729493892e-05, 'epoch': 0.41}\n",
      "{'loss': 3.317, 'grad_norm': 8.000569343566895, 'learning_rate': 4.323007562536359e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2365, 'grad_norm': 6.96127986907959, 'learning_rate': 4.322280395578825e-05, 'epoch': 0.41}\n",
      "{'loss': 3.098, 'grad_norm': 7.094104766845703, 'learning_rate': 4.3215532286212915e-05, 'epoch': 0.41}\n",
      "{'loss': 3.1998, 'grad_norm': 7.966033458709717, 'learning_rate': 4.320826061663758e-05, 'epoch': 0.41}\n",
      "{'loss': 3.1932, 'grad_norm': 7.491912841796875, 'learning_rate': 4.320098894706225e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2748, 'grad_norm': 8.480586051940918, 'learning_rate': 4.319371727748691e-05, 'epoch': 0.41}\n",
      "{'loss': 3.1996, 'grad_norm': 8.794828414916992, 'learning_rate': 4.318644560791158e-05, 'epoch': 0.41}\n",
      "{'loss': 3.0723, 'grad_norm': 7.666959762573242, 'learning_rate': 4.317917393833624e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2934, 'grad_norm': 7.017352104187012, 'learning_rate': 4.317190226876091e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3461, 'grad_norm': 6.6610283851623535, 'learning_rate': 4.3164630599185576e-05, 'epoch': 0.41}\n",
      "{'loss': 3.1961, 'grad_norm': 7.335174560546875, 'learning_rate': 4.315735892961024e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3051, 'grad_norm': 7.486812114715576, 'learning_rate': 4.31500872600349e-05, 'epoch': 0.41}\n",
      "{'loss': 3.4424, 'grad_norm': 6.736368179321289, 'learning_rate': 4.314281559045957e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2578, 'grad_norm': 7.114572048187256, 'learning_rate': 4.313554392088424e-05, 'epoch': 0.41}\n",
      "{'loss': 3.1779, 'grad_norm': 7.151339530944824, 'learning_rate': 4.3128272251308904e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2203, 'grad_norm': 8.685583114624023, 'learning_rate': 4.3121000581733564e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3436, 'grad_norm': 8.051700592041016, 'learning_rate': 4.311372891215824e-05, 'epoch': 0.41}\n",
      "{'loss': 3.0756, 'grad_norm': 8.62196159362793, 'learning_rate': 4.31064572425829e-05, 'epoch': 0.41}\n",
      "{'loss': 3.3395, 'grad_norm': 7.984282493591309, 'learning_rate': 4.3099185573007565e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2469, 'grad_norm': 8.446615219116211, 'learning_rate': 4.309191390343223e-05, 'epoch': 0.41}\n",
      " 14%|████▋                             | 9500/68760 [1:46:44<9:45:12,  1.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.50s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.473011999999997, 'eval_rouge-2': 7.5226880000000005, 'eval_rouge-l': 24.813747999999997, 'eval_bleu-4': 0.033246830400468066, 'eval_runtime': 37.711, 'eval_samples_per_second': 1.326, 'eval_steps_per_second': 0.106, 'epoch': 0.41}\n",
      " 14%|████▋                             | 9500/68760 [1:47:22<9:45:12,  1.69it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.80s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-9500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1896, 'grad_norm': 7.321358680725098, 'learning_rate': 4.30846422338569e-05, 'epoch': 0.41}\n",
      "{'loss': 3.2039, 'grad_norm': 7.743348121643066, 'learning_rate': 4.307737056428156e-05, 'epoch': 0.42}\n",
      "{'loss': 3.1139, 'grad_norm': 7.699756145477295, 'learning_rate': 4.307009889470623e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2777, 'grad_norm': 6.697274684906006, 'learning_rate': 4.306282722513089e-05, 'epoch': 0.42}\n",
      "{'loss': 3.199, 'grad_norm': 8.50674057006836, 'learning_rate': 4.305555555555556e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2924, 'grad_norm': 7.522512435913086, 'learning_rate': 4.3048283885980226e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2455, 'grad_norm': 8.060163497924805, 'learning_rate': 4.304101221640489e-05, 'epoch': 0.42}\n",
      "{'loss': 3.1721, 'grad_norm': 7.5774431228637695, 'learning_rate': 4.303374054682955e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3148, 'grad_norm': 7.906108856201172, 'learning_rate': 4.302646887725422e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2281, 'grad_norm': 8.48637580871582, 'learning_rate': 4.301919720767889e-05, 'epoch': 0.42}\n",
      "{'loss': 3.1885, 'grad_norm': 7.312258243560791, 'learning_rate': 4.301192553810355e-05, 'epoch': 0.42}\n",
      "{'loss': 3.257, 'grad_norm': 7.297785758972168, 'learning_rate': 4.3004653868528214e-05, 'epoch': 0.42}\n",
      "{'loss': 3.3709, 'grad_norm': 7.980346202850342, 'learning_rate': 4.299738219895288e-05, 'epoch': 0.42}\n",
      "{'loss': 3.1439, 'grad_norm': 8.444560050964355, 'learning_rate': 4.299011052937755e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2496, 'grad_norm': 7.488216400146484, 'learning_rate': 4.298283885980221e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2164, 'grad_norm': 7.535046100616455, 'learning_rate': 4.297556719022688e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2576, 'grad_norm': 6.798106670379639, 'learning_rate': 4.296829552065154e-05, 'epoch': 0.42}\n",
      "{'loss': 3.1379, 'grad_norm': 7.925125598907471, 'learning_rate': 4.296102385107621e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2168, 'grad_norm': 7.244101524353027, 'learning_rate': 4.2953752181500876e-05, 'epoch': 0.42}\n",
      "{'loss': 3.0943, 'grad_norm': 7.28833532333374, 'learning_rate': 4.294648051192554e-05, 'epoch': 0.42}\n",
      "{'loss': 3.218, 'grad_norm': 8.00937557220459, 'learning_rate': 4.29392088423502e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2594, 'grad_norm': 7.823063850402832, 'learning_rate': 4.293193717277487e-05, 'epoch': 0.42}\n",
      "{'loss': 3.2162, 'grad_norm': 7.732597351074219, 'learning_rate': 4.2924665503199536e-05, 'epoch': 0.42}\n",
      "{'loss': 3.1936, 'grad_norm': 6.835764408111572, 'learning_rate': 4.29173938336242e-05, 'epoch': 0.42}\n",
      "{'loss': 3.1023, 'grad_norm': 7.462162017822266, 'learning_rate': 4.291012216404886e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3904, 'grad_norm': 6.798039436340332, 'learning_rate': 4.290285049447354e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2316, 'grad_norm': 7.512227535247803, 'learning_rate': 4.28955788248982e-05, 'epoch': 0.43}\n",
      "{'loss': 3.1447, 'grad_norm': 7.953654766082764, 'learning_rate': 4.2888307155322864e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3168, 'grad_norm': 7.641809940338135, 'learning_rate': 4.288103548574753e-05, 'epoch': 0.43}\n",
      "{'loss': 3.1521, 'grad_norm': 7.287789821624756, 'learning_rate': 4.28737638161722e-05, 'epoch': 0.43}\n",
      "{'loss': 3.251, 'grad_norm': 7.986062526702881, 'learning_rate': 4.286649214659686e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2232, 'grad_norm': 7.407412052154541, 'learning_rate': 4.2859220477021525e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3, 'grad_norm': 7.6553497314453125, 'learning_rate': 4.285194880744619e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2848, 'grad_norm': 7.735942840576172, 'learning_rate': 4.284467713787086e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2449, 'grad_norm': 7.095907211303711, 'learning_rate': 4.283740546829552e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2385, 'grad_norm': 8.064139366149902, 'learning_rate': 4.283013379872019e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3262, 'grad_norm': 8.08484172821045, 'learning_rate': 4.282286212914485e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3725, 'grad_norm': 7.084269046783447, 'learning_rate': 4.281559045956952e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2152, 'grad_norm': 8.095467567443848, 'learning_rate': 4.2808318789994186e-05, 'epoch': 0.43}\n",
      "{'loss': 3.1047, 'grad_norm': 7.800179481506348, 'learning_rate': 4.280104712041885e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2381, 'grad_norm': 7.736114025115967, 'learning_rate': 4.279377545084351e-05, 'epoch': 0.43}\n",
      "{'loss': 3.1303, 'grad_norm': 7.495537281036377, 'learning_rate': 4.278650378126818e-05, 'epoch': 0.43}\n",
      "{'loss': 3.1816, 'grad_norm': 8.415070533752441, 'learning_rate': 4.277923211169285e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2721, 'grad_norm': 7.057873725891113, 'learning_rate': 4.2771960442117514e-05, 'epoch': 0.43}\n",
      "{'loss': 3.2318, 'grad_norm': 8.317378997802734, 'learning_rate': 4.2764688772542174e-05, 'epoch': 0.43}\n",
      "{'loss': 3.1895, 'grad_norm': 6.535617828369141, 'learning_rate': 4.275741710296685e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3297, 'grad_norm': 7.642533302307129, 'learning_rate': 4.275014543339151e-05, 'epoch': 0.43}\n",
      "{'loss': 3.3012, 'grad_norm': 8.336002349853516, 'learning_rate': 4.2742873763816175e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2012, 'grad_norm': 8.387754440307617, 'learning_rate': 4.273560209424084e-05, 'epoch': 0.44}\n",
      "{'loss': 3.4098, 'grad_norm': 7.456011772155762, 'learning_rate': 4.27283304246655e-05, 'epoch': 0.44}\n",
      " 15%|████▊                            | 10000/68760 [1:52:33<9:38:40,  1.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.46s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.481606, 'eval_rouge-2': 7.573094, 'eval_rouge-l': 24.719137999999997, 'eval_bleu-4': 0.03762138368926259, 'eval_runtime': 17.8077, 'eval_samples_per_second': 2.808, 'eval_steps_per_second': 0.225, 'epoch': 0.44}\n",
      " 15%|████▊                            | 10000/68760 [1:52:50<9:38:40,  1.69it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.39s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-10000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2496, 'grad_norm': 7.618551731109619, 'learning_rate': 4.272105875509017e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1992, 'grad_norm': 8.202037811279297, 'learning_rate': 4.2713787085514836e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2545, 'grad_norm': 7.474199295043945, 'learning_rate': 4.27065154159395e-05, 'epoch': 0.44}\n",
      "{'loss': 3.3068, 'grad_norm': 8.683016777038574, 'learning_rate': 4.269924374636416e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2502, 'grad_norm': 7.888477802276611, 'learning_rate': 4.2691972076788836e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1646, 'grad_norm': 7.796139240264893, 'learning_rate': 4.2684700407213496e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2779, 'grad_norm': 7.502491474151611, 'learning_rate': 4.267742873763816e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1549, 'grad_norm': 7.106755256652832, 'learning_rate': 4.267015706806283e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2119, 'grad_norm': 9.980727195739746, 'learning_rate': 4.26628853984875e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1324, 'grad_norm': 7.915760517120361, 'learning_rate': 4.265561372891216e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2375, 'grad_norm': 7.957005500793457, 'learning_rate': 4.2648342059336824e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2723, 'grad_norm': 7.379694938659668, 'learning_rate': 4.264107038976149e-05, 'epoch': 0.44}\n",
      "{'loss': 3.4354, 'grad_norm': 8.5818510055542, 'learning_rate': 4.263379872018616e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1736, 'grad_norm': 7.588994979858398, 'learning_rate': 4.262652705061082e-05, 'epoch': 0.44}\n",
      "{'loss': 3.143, 'grad_norm': 7.658984184265137, 'learning_rate': 4.261925538103549e-05, 'epoch': 0.44}\n",
      "{'loss': 3.25, 'grad_norm': 7.838949680328369, 'learning_rate': 4.261198371146015e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1492, 'grad_norm': 8.138389587402344, 'learning_rate': 4.260471204188482e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1768, 'grad_norm': 7.945836544036865, 'learning_rate': 4.2597440372309486e-05, 'epoch': 0.44}\n",
      "{'loss': 3.2795, 'grad_norm': 7.534097671508789, 'learning_rate': 4.259016870273415e-05, 'epoch': 0.44}\n",
      "{'loss': 3.1652, 'grad_norm': 8.13369369506836, 'learning_rate': 4.258289703315881e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3039, 'grad_norm': 8.95875358581543, 'learning_rate': 4.257562536358348e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2812, 'grad_norm': 8.391114234924316, 'learning_rate': 4.2568353694008146e-05, 'epoch': 0.45}\n",
      "{'loss': 3.1256, 'grad_norm': 7.062891960144043, 'learning_rate': 4.256108202443281e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3135, 'grad_norm': 7.461826801300049, 'learning_rate': 4.2553810354857473e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2207, 'grad_norm': 8.824661254882812, 'learning_rate': 4.254653868528215e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2549, 'grad_norm': 7.814207077026367, 'learning_rate': 4.253926701570681e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2168, 'grad_norm': 6.9388322830200195, 'learning_rate': 4.2531995346131474e-05, 'epoch': 0.45}\n",
      "{'loss': 3.1738, 'grad_norm': 7.1869330406188965, 'learning_rate': 4.252472367655614e-05, 'epoch': 0.45}\n",
      "{'loss': 3.333, 'grad_norm': 8.169686317443848, 'learning_rate': 4.251745200698081e-05, 'epoch': 0.45}\n",
      "{'loss': 3.0846, 'grad_norm': 7.153517723083496, 'learning_rate': 4.251018033740547e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2389, 'grad_norm': 6.921996593475342, 'learning_rate': 4.2502908667830135e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2182, 'grad_norm': 7.83350944519043, 'learning_rate': 4.24956369982548e-05, 'epoch': 0.45}\n",
      "{'loss': 3.25, 'grad_norm': 7.479408264160156, 'learning_rate': 4.248836532867947e-05, 'epoch': 0.45}\n",
      "{'loss': 3.259, 'grad_norm': 7.511359214782715, 'learning_rate': 4.248109365910413e-05, 'epoch': 0.45}\n",
      "{'loss': 3.0703, 'grad_norm': 8.015515327453613, 'learning_rate': 4.24738219895288e-05, 'epoch': 0.45}\n",
      "{'loss': 3.1936, 'grad_norm': 7.130244255065918, 'learning_rate': 4.246655031995346e-05, 'epoch': 0.45}\n",
      "{'loss': 3.1854, 'grad_norm': 7.267121315002441, 'learning_rate': 4.245927865037813e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2551, 'grad_norm': 7.731924057006836, 'learning_rate': 4.2452006980802796e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2711, 'grad_norm': 7.607332706451416, 'learning_rate': 4.244473531122746e-05, 'epoch': 0.45}\n",
      "{'loss': 3.1934, 'grad_norm': 7.586780548095703, 'learning_rate': 4.2437463641652123e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2031, 'grad_norm': 7.25322961807251, 'learning_rate': 4.243019197207679e-05, 'epoch': 0.45}\n",
      "{'loss': 3.2084, 'grad_norm': 7.763599395751953, 'learning_rate': 4.242292030250146e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3123, 'grad_norm': 7.01353120803833, 'learning_rate': 4.241564863292612e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2469, 'grad_norm': 7.777375221252441, 'learning_rate': 4.240837696335079e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2137, 'grad_norm': 7.275515556335449, 'learning_rate': 4.240110529377545e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1859, 'grad_norm': 7.04747200012207, 'learning_rate': 4.239383362420012e-05, 'epoch': 0.46}\n",
      "{'loss': 3.191, 'grad_norm': 7.279183387756348, 'learning_rate': 4.2386561954624785e-05, 'epoch': 0.46}\n",
      "{'loss': 3.225, 'grad_norm': 7.517220497131348, 'learning_rate': 4.237929028504945e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1998, 'grad_norm': 7.374392032623291, 'learning_rate': 4.237201861547411e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1947, 'grad_norm': 7.53219747543335, 'learning_rate': 4.236474694589878e-05, 'epoch': 0.46}\n",
      " 15%|█████                            | 10500/68760 [1:58:01<9:57:06,  1.63it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.50s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.634816, 'eval_rouge-2': 7.731226, 'eval_rouge-l': 24.6094, 'eval_bleu-4': 0.03781652633135226, 'eval_runtime': 27.8326, 'eval_samples_per_second': 1.796, 'eval_steps_per_second': 0.144, 'epoch': 0.46}\n",
      " 15%|█████                            | 10500/68760 [1:58:28<9:57:06,  1.63it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.85s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-10500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3285, 'grad_norm': 8.20579719543457, 'learning_rate': 4.2357475276323446e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2053, 'grad_norm': 8.262389183044434, 'learning_rate': 4.235020360674811e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2678, 'grad_norm': 8.084524154663086, 'learning_rate': 4.234293193717277e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1309, 'grad_norm': 6.705654621124268, 'learning_rate': 4.2335660267597446e-05, 'epoch': 0.46}\n",
      "{'loss': 3.4691, 'grad_norm': 7.321531295776367, 'learning_rate': 4.2328388598022107e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2402, 'grad_norm': 7.47875452041626, 'learning_rate': 4.2321116928446773e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2016, 'grad_norm': 7.067037105560303, 'learning_rate': 4.231384525887144e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1723, 'grad_norm': 8.428786277770996, 'learning_rate': 4.230657358929611e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2359, 'grad_norm': 6.926473617553711, 'learning_rate': 4.229930191972077e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2336, 'grad_norm': 7.264453411102295, 'learning_rate': 4.2292030250145434e-05, 'epoch': 0.46}\n",
      "{'loss': 3.3275, 'grad_norm': 7.563476085662842, 'learning_rate': 4.22847585805701e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1309, 'grad_norm': 7.291137218475342, 'learning_rate': 4.227748691099477e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2449, 'grad_norm': 7.6180033683776855, 'learning_rate': 4.227021524141943e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1514, 'grad_norm': 7.489519119262695, 'learning_rate': 4.22629435718441e-05, 'epoch': 0.46}\n",
      "{'loss': 3.2674, 'grad_norm': 7.429955959320068, 'learning_rate': 4.225567190226876e-05, 'epoch': 0.46}\n",
      "{'loss': 3.1965, 'grad_norm': 7.185999870300293, 'learning_rate': 4.224840023269343e-05, 'epoch': 0.47}\n",
      "{'loss': 3.4131, 'grad_norm': 7.843628406524658, 'learning_rate': 4.2241128563118096e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2709, 'grad_norm': 8.613158226013184, 'learning_rate': 4.223385689354276e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2139, 'grad_norm': 8.783806800842285, 'learning_rate': 4.222658522396742e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2135, 'grad_norm': 7.13261079788208, 'learning_rate': 4.221931355439209e-05, 'epoch': 0.47}\n",
      "{'loss': 3.184, 'grad_norm': 7.569830417633057, 'learning_rate': 4.2212041884816757e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3373, 'grad_norm': 7.711251735687256, 'learning_rate': 4.2204770215241423e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2916, 'grad_norm': 9.17635440826416, 'learning_rate': 4.2197498545666084e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2332, 'grad_norm': 7.080746650695801, 'learning_rate': 4.219022687609076e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2135, 'grad_norm': 7.63759183883667, 'learning_rate': 4.218295520651542e-05, 'epoch': 0.47}\n",
      "{'loss': 3.1672, 'grad_norm': 7.507835388183594, 'learning_rate': 4.2175683536940084e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2346, 'grad_norm': 7.2519025802612305, 'learning_rate': 4.216841186736475e-05, 'epoch': 0.47}\n",
      "{'loss': 3.1914, 'grad_norm': 7.630246639251709, 'learning_rate': 4.216114019778942e-05, 'epoch': 0.47}\n",
      "{'loss': 3.3354, 'grad_norm': 7.212799072265625, 'learning_rate': 4.215386852821408e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2408, 'grad_norm': 7.807244300842285, 'learning_rate': 4.2146596858638745e-05, 'epoch': 0.47}\n",
      "{'loss': 3.1893, 'grad_norm': 7.7887187004089355, 'learning_rate': 4.213932518906341e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2494, 'grad_norm': 7.370454788208008, 'learning_rate': 4.213205351948807e-05, 'epoch': 0.47}\n",
      "{'loss': 3.1078, 'grad_norm': 7.327929973602295, 'learning_rate': 4.212478184991274e-05, 'epoch': 0.47}\n",
      "{'loss': 3.4572, 'grad_norm': 7.391478061676025, 'learning_rate': 4.2117510180337406e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2277, 'grad_norm': 6.847546100616455, 'learning_rate': 4.211023851076207e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2738, 'grad_norm': 7.121138095855713, 'learning_rate': 4.210296684118673e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2563, 'grad_norm': 7.472380638122559, 'learning_rate': 4.2095695171611407e-05, 'epoch': 0.47}\n",
      "{'loss': 3.2094, 'grad_norm': 7.875640392303467, 'learning_rate': 4.208842350203607e-05, 'epoch': 0.47}\n",
      "{'loss': 3.1947, 'grad_norm': 7.812166690826416, 'learning_rate': 4.2081151832460734e-05, 'epoch': 0.48}\n",
      "{'loss': 3.1615, 'grad_norm': 6.857804298400879, 'learning_rate': 4.20738801628854e-05, 'epoch': 0.48}\n",
      "{'loss': 3.1371, 'grad_norm': 8.20391845703125, 'learning_rate': 4.206660849331007e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2652, 'grad_norm': 6.8823323249816895, 'learning_rate': 4.205933682373473e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3305, 'grad_norm': 8.681367874145508, 'learning_rate': 4.20520651541594e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3783, 'grad_norm': 8.521074295043945, 'learning_rate': 4.204479348458406e-05, 'epoch': 0.48}\n",
      "{'loss': 3.1766, 'grad_norm': 7.724036693572998, 'learning_rate': 4.203752181500873e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2314, 'grad_norm': 8.284896850585938, 'learning_rate': 4.2030250145433395e-05, 'epoch': 0.48}\n",
      "{'loss': 3.218, 'grad_norm': 7.8923773765563965, 'learning_rate': 4.202297847585806e-05, 'epoch': 0.48}\n",
      "{'loss': 3.35, 'grad_norm': 7.690859794616699, 'learning_rate': 4.201570680628272e-05, 'epoch': 0.48}\n",
      "{'loss': 3.1119, 'grad_norm': 7.64578104019165, 'learning_rate': 4.200843513670739e-05, 'epoch': 0.48}\n",
      "{'loss': 3.158, 'grad_norm': 7.140754699707031, 'learning_rate': 4.2001163467132056e-05, 'epoch': 0.48}\n",
      " 16%|█████▎                           | 11000/68760 [2:03:39<9:36:51,  1.67it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.38s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.330148, 'eval_rouge-2': 7.9711620000000005, 'eval_rouge-l': 24.983936000000003, 'eval_bleu-4': 0.036970393476310925, 'eval_runtime': 27.8224, 'eval_samples_per_second': 1.797, 'eval_steps_per_second': 0.144, 'epoch': 0.48}\n",
      " 16%|█████▎                           | 11000/68760 [2:04:07<9:36:51,  1.67it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.22s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-11000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1748, 'grad_norm': 7.895678520202637, 'learning_rate': 4.199389179755672e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2984, 'grad_norm': 7.402378559112549, 'learning_rate': 4.198662012798138e-05, 'epoch': 0.48}\n",
      "{'loss': 3.1547, 'grad_norm': 7.53262996673584, 'learning_rate': 4.1979348458406057e-05, 'epoch': 0.48}\n",
      "{'loss': 3.1951, 'grad_norm': 7.265271186828613, 'learning_rate': 4.197207678883072e-05, 'epoch': 0.48}\n",
      "{'loss': 3.243, 'grad_norm': 7.760372638702393, 'learning_rate': 4.1964805119255384e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3316, 'grad_norm': 8.098919868469238, 'learning_rate': 4.195753344968005e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2006, 'grad_norm': 8.381012916564941, 'learning_rate': 4.195026178010472e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2197, 'grad_norm': 8.179131507873535, 'learning_rate': 4.194299011052938e-05, 'epoch': 0.48}\n",
      "{'loss': 3.132, 'grad_norm': 7.26724910736084, 'learning_rate': 4.1935718440954044e-05, 'epoch': 0.48}\n",
      "{'loss': 3.1766, 'grad_norm': 7.431443214416504, 'learning_rate': 4.192844677137871e-05, 'epoch': 0.48}\n",
      "{'loss': 3.3172, 'grad_norm': 7.310121536254883, 'learning_rate': 4.192117510180338e-05, 'epoch': 0.48}\n",
      "{'loss': 3.2082, 'grad_norm': 7.165156364440918, 'learning_rate': 4.191390343222804e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2896, 'grad_norm': 7.523006439208984, 'learning_rate': 4.190663176265271e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2072, 'grad_norm': 6.950035095214844, 'learning_rate': 4.189936009307737e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1725, 'grad_norm': 8.306015014648438, 'learning_rate': 4.189208842350204e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2824, 'grad_norm': 7.556572914123535, 'learning_rate': 4.1884816753926706e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2758, 'grad_norm': 9.386360168457031, 'learning_rate': 4.187754508435137e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3143, 'grad_norm': 7.603931903839111, 'learning_rate': 4.187027341477603e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3162, 'grad_norm': 7.730470657348633, 'learning_rate': 4.18630017452007e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2539, 'grad_norm': 7.583658218383789, 'learning_rate': 4.185573007562537e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2326, 'grad_norm': 8.678321838378906, 'learning_rate': 4.184845840605003e-05, 'epoch': 0.49}\n",
      "{'loss': 2.984, 'grad_norm': 7.732889175415039, 'learning_rate': 4.1841186736474694e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2102, 'grad_norm': 7.457645893096924, 'learning_rate': 4.183391506689936e-05, 'epoch': 0.49}\n",
      "{'loss': 3.308, 'grad_norm': 7.329677581787109, 'learning_rate': 4.182664339732403e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2275, 'grad_norm': 7.338423252105713, 'learning_rate': 4.181937172774869e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1029, 'grad_norm': 8.129712104797363, 'learning_rate': 4.181210005817336e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2203, 'grad_norm': 7.6878557205200195, 'learning_rate': 4.180482838859802e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1311, 'grad_norm': 7.7870402336120605, 'learning_rate': 4.179755671902269e-05, 'epoch': 0.49}\n",
      "{'loss': 3.257, 'grad_norm': 7.755646228790283, 'learning_rate': 4.1790285049447355e-05, 'epoch': 0.49}\n",
      "{'loss': 3.2139, 'grad_norm': 7.038646221160889, 'learning_rate': 4.178301337987202e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1621, 'grad_norm': 7.6486029624938965, 'learning_rate': 4.177574171029668e-05, 'epoch': 0.49}\n",
      "{'loss': 3.3143, 'grad_norm': 7.643017768859863, 'learning_rate': 4.1768470040721356e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1561, 'grad_norm': 7.7543206214904785, 'learning_rate': 4.1761198371146016e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1963, 'grad_norm': 7.90208101272583, 'learning_rate': 4.175392670157068e-05, 'epoch': 0.49}\n",
      "{'loss': 3.1461, 'grad_norm': 7.103697776794434, 'learning_rate': 4.174665503199535e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2617, 'grad_norm': 8.03635025024414, 'learning_rate': 4.173938336242002e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2164, 'grad_norm': 7.1560444831848145, 'learning_rate': 4.173211169284468e-05, 'epoch': 0.5}\n",
      "{'loss': 3.1439, 'grad_norm': 8.513749122619629, 'learning_rate': 4.1724840023269344e-05, 'epoch': 0.5}\n",
      "{'loss': 3.1656, 'grad_norm': 7.248238563537598, 'learning_rate': 4.171756835369401e-05, 'epoch': 0.5}\n",
      "{'loss': 3.0797, 'grad_norm': 8.113780975341797, 'learning_rate': 4.171029668411868e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2809, 'grad_norm': 8.229867935180664, 'learning_rate': 4.170302501454334e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2961, 'grad_norm': 6.788690090179443, 'learning_rate': 4.169575334496801e-05, 'epoch': 0.5}\n",
      "{'loss': 3.348, 'grad_norm': 7.788972854614258, 'learning_rate': 4.168848167539267e-05, 'epoch': 0.5}\n",
      "{'loss': 3.1861, 'grad_norm': 7.681365966796875, 'learning_rate': 4.168121000581734e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2398, 'grad_norm': 7.497366428375244, 'learning_rate': 4.1673938336242005e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2086, 'grad_norm': 7.712936878204346, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.5}\n",
      "{'loss': 3.1482, 'grad_norm': 7.787313461303711, 'learning_rate': 4.165939499709133e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2404, 'grad_norm': 7.996029853820801, 'learning_rate': 4.1652123327516e-05, 'epoch': 0.5}\n",
      "{'loss': 3.1463, 'grad_norm': 8.505399703979492, 'learning_rate': 4.1644851657940666e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2611, 'grad_norm': 7.990884780883789, 'learning_rate': 4.163757998836533e-05, 'epoch': 0.5}\n",
      " 17%|█████▌                           | 11500/68760 [2:09:15<9:37:41,  1.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.55s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.74s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.750282, 'eval_rouge-2': 8.071756, 'eval_rouge-l': 24.8924, 'eval_bleu-4': 0.0394168325995904, 'eval_runtime': 26.6365, 'eval_samples_per_second': 1.877, 'eval_steps_per_second': 0.15, 'epoch': 0.5}\n",
      " 17%|█████▌                           | 11500/68760 [2:09:42<9:37:41,  1.65it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  4.50s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-11500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1967, 'grad_norm': 8.34544563293457, 'learning_rate': 4.163030831878999e-05, 'epoch': 0.5}\n",
      "{'loss': 3.1996, 'grad_norm': 7.8845319747924805, 'learning_rate': 4.162303664921467e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2734, 'grad_norm': 7.121938228607178, 'learning_rate': 4.161576497963933e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2043, 'grad_norm': 7.650084495544434, 'learning_rate': 4.1608493310063994e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2404, 'grad_norm': 7.781332015991211, 'learning_rate': 4.160122164048866e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2348, 'grad_norm': 7.440217971801758, 'learning_rate': 4.159394997091333e-05, 'epoch': 0.5}\n",
      "{'loss': 3.1463, 'grad_norm': 7.767183780670166, 'learning_rate': 4.158667830133799e-05, 'epoch': 0.5}\n",
      "{'loss': 3.2932, 'grad_norm': 7.807531356811523, 'learning_rate': 4.1579406631762654e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3238, 'grad_norm': 7.345456123352051, 'learning_rate': 4.157213496218732e-05, 'epoch': 0.51}\n",
      "{'loss': 3.1449, 'grad_norm': 7.02761173248291, 'learning_rate': 4.156486329261198e-05, 'epoch': 0.51}\n",
      "{'loss': 3.1996, 'grad_norm': 7.484103202819824, 'learning_rate': 4.155759162303665e-05, 'epoch': 0.51}\n",
      "{'loss': 3.1639, 'grad_norm': 7.196832656860352, 'learning_rate': 4.1550319953461315e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2318, 'grad_norm': 7.756939888000488, 'learning_rate': 4.154304828388598e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2602, 'grad_norm': 7.919569969177246, 'learning_rate': 4.153577661431064e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2863, 'grad_norm': 7.745327949523926, 'learning_rate': 4.1528504944735316e-05, 'epoch': 0.51}\n",
      "{'loss': 3.1867, 'grad_norm': 7.70573091506958, 'learning_rate': 4.1521233275159976e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2287, 'grad_norm': 8.376349449157715, 'learning_rate': 4.151396160558464e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2242, 'grad_norm': 7.568531036376953, 'learning_rate': 4.150668993600931e-05, 'epoch': 0.51}\n",
      "{'loss': 3.1793, 'grad_norm': 7.6789116859436035, 'learning_rate': 4.149941826643398e-05, 'epoch': 0.51}\n",
      "{'loss': 3.1945, 'grad_norm': 8.344544410705566, 'learning_rate': 4.149214659685864e-05, 'epoch': 0.51}\n",
      "{'loss': 3.0691, 'grad_norm': 7.2102131843566895, 'learning_rate': 4.1484874927283304e-05, 'epoch': 0.51}\n",
      "{'loss': 3.0781, 'grad_norm': 8.291535377502441, 'learning_rate': 4.147760325770797e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2955, 'grad_norm': 8.033008575439453, 'learning_rate': 4.147033158813264e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2631, 'grad_norm': 7.596556186676025, 'learning_rate': 4.14630599185573e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2367, 'grad_norm': 6.731502056121826, 'learning_rate': 4.145578824898197e-05, 'epoch': 0.51}\n",
      "{'loss': 3.1113, 'grad_norm': 7.7654709815979, 'learning_rate': 4.144851657940663e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2861, 'grad_norm': 7.559879302978516, 'learning_rate': 4.14412449098313e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3746, 'grad_norm': 8.507129669189453, 'learning_rate': 4.1433973240255965e-05, 'epoch': 0.51}\n",
      "{'loss': 3.3105, 'grad_norm': 7.577733516693115, 'learning_rate': 4.142670157068063e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2137, 'grad_norm': 7.99843692779541, 'learning_rate': 4.141942990110529e-05, 'epoch': 0.51}\n",
      "{'loss': 3.2049, 'grad_norm': 7.77971887588501, 'learning_rate': 4.1412158231529966e-05, 'epoch': 0.52}\n",
      "{'loss': 3.1809, 'grad_norm': 7.824216365814209, 'learning_rate': 4.1404886561954626e-05, 'epoch': 0.52}\n",
      "{'loss': 3.4377, 'grad_norm': 7.5078864097595215, 'learning_rate': 4.139761489237929e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2471, 'grad_norm': 7.944098472595215, 'learning_rate': 4.139034322280396e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3564, 'grad_norm': 7.376644611358643, 'learning_rate': 4.138307155322863e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2031, 'grad_norm': 7.94570779800415, 'learning_rate': 4.137579988365329e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2264, 'grad_norm': 7.455770015716553, 'learning_rate': 4.1368528214077954e-05, 'epoch': 0.52}\n",
      "{'loss': 3.1611, 'grad_norm': 7.925167560577393, 'learning_rate': 4.136125654450262e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2035, 'grad_norm': 7.773187160491943, 'learning_rate': 4.135398487492729e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2633, 'grad_norm': 8.417436599731445, 'learning_rate': 4.134671320535195e-05, 'epoch': 0.52}\n",
      "{'loss': 3.218, 'grad_norm': 6.998443126678467, 'learning_rate': 4.133944153577662e-05, 'epoch': 0.52}\n",
      "{'loss': 3.1572, 'grad_norm': 7.592219352722168, 'learning_rate': 4.133216986620128e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2098, 'grad_norm': 7.688827037811279, 'learning_rate': 4.132489819662595e-05, 'epoch': 0.52}\n",
      "{'loss': 3.266, 'grad_norm': 8.222906112670898, 'learning_rate': 4.1317626527050615e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2826, 'grad_norm': 8.003904342651367, 'learning_rate': 4.131035485747528e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2551, 'grad_norm': 7.627768039703369, 'learning_rate': 4.130308318789994e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2229, 'grad_norm': 7.401848793029785, 'learning_rate': 4.129581151832461e-05, 'epoch': 0.52}\n",
      "{'loss': 3.1791, 'grad_norm': 7.639404773712158, 'learning_rate': 4.1288539848749276e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2875, 'grad_norm': 6.993587017059326, 'learning_rate': 4.128126817917394e-05, 'epoch': 0.52}\n",
      "{'loss': 3.3691, 'grad_norm': 7.8276047706604, 'learning_rate': 4.12739965095986e-05, 'epoch': 0.52}\n",
      " 17%|█████▊                           | 12000/68760 [2:14:48<9:26:54,  1.67it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.52s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.84s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.815688, 'eval_rouge-2': 7.59346, 'eval_rouge-l': 23.198182000000003, 'eval_bleu-4': 0.03501572974309229, 'eval_runtime': 36.4819, 'eval_samples_per_second': 1.371, 'eval_steps_per_second': 0.11, 'epoch': 0.52}\n",
      " 17%|█████▊                           | 12000/68760 [2:15:25<9:26:54,  1.67it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:24<00:00,  7.08s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-12000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3344, 'grad_norm': 8.46741771697998, 'learning_rate': 4.126672484002327e-05, 'epoch': 0.52}\n",
      "{'loss': 3.2377, 'grad_norm': 7.403171539306641, 'learning_rate': 4.125945317044794e-05, 'epoch': 0.52}\n",
      "{'loss': 3.0547, 'grad_norm': 7.484209060668945, 'learning_rate': 4.12521815008726e-05, 'epoch': 0.52}\n",
      "{'loss': 3.1961, 'grad_norm': 6.6860175132751465, 'learning_rate': 4.124490983129727e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2555, 'grad_norm': 8.00098991394043, 'learning_rate': 4.123763816172193e-05, 'epoch': 0.53}\n",
      "{'loss': 3.265, 'grad_norm': 7.884917259216309, 'learning_rate': 4.12303664921466e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2051, 'grad_norm': 8.406806945800781, 'learning_rate': 4.1223094822571265e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2217, 'grad_norm': 7.871180057525635, 'learning_rate': 4.121582315299593e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1389, 'grad_norm': 9.349536895751953, 'learning_rate': 4.120855148342059e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1781, 'grad_norm': 7.656238079071045, 'learning_rate': 4.120127981384526e-05, 'epoch': 0.53}\n",
      "{'loss': 3.0895, 'grad_norm': 7.558004379272461, 'learning_rate': 4.1194008144269925e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2082, 'grad_norm': 7.877237319946289, 'learning_rate': 4.118673647469459e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1883, 'grad_norm': 6.9767303466796875, 'learning_rate': 4.117946480511925e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1602, 'grad_norm': 7.283055305480957, 'learning_rate': 4.1172193135543926e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2629, 'grad_norm': 7.821200847625732, 'learning_rate': 4.1164921465968586e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1484, 'grad_norm': 8.630508422851562, 'learning_rate': 4.115764979639325e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1836, 'grad_norm': 7.630926132202148, 'learning_rate': 4.115037812681792e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1977, 'grad_norm': 7.1973419189453125, 'learning_rate': 4.114310645724259e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2797, 'grad_norm': 8.338383674621582, 'learning_rate': 4.113583478766725e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2211, 'grad_norm': 7.923412322998047, 'learning_rate': 4.112856311809192e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1953, 'grad_norm': 8.77213191986084, 'learning_rate': 4.112129144851658e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2887, 'grad_norm': 7.3711771965026855, 'learning_rate': 4.111401977894125e-05, 'epoch': 0.53}\n",
      "{'loss': 3.0516, 'grad_norm': 8.219521522521973, 'learning_rate': 4.1106748109365915e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2281, 'grad_norm': 7.7331624031066895, 'learning_rate': 4.109947643979058e-05, 'epoch': 0.53}\n",
      "{'loss': 3.3572, 'grad_norm': 8.198884010314941, 'learning_rate': 4.109220477021524e-05, 'epoch': 0.53}\n",
      "{'loss': 3.1156, 'grad_norm': 7.6572699546813965, 'learning_rate': 4.108493310063991e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2867, 'grad_norm': 6.981943130493164, 'learning_rate': 4.1077661431064575e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2377, 'grad_norm': 8.440789222717285, 'learning_rate': 4.107038976148924e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1951, 'grad_norm': 7.535615921020508, 'learning_rate': 4.10631180919139e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1348, 'grad_norm': 7.328155517578125, 'learning_rate': 4.1055846422338576e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1779, 'grad_norm': 7.713378429412842, 'learning_rate': 4.1048574752763236e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2193, 'grad_norm': 7.4665398597717285, 'learning_rate': 4.10413030831879e-05, 'epoch': 0.54}\n",
      "{'loss': 3.4357, 'grad_norm': 7.803890228271484, 'learning_rate': 4.103403141361257e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2723, 'grad_norm': 8.90870189666748, 'learning_rate': 4.102675974403724e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2424, 'grad_norm': 7.300349712371826, 'learning_rate': 4.10194880744619e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3047, 'grad_norm': 7.053110122680664, 'learning_rate': 4.1012216404886564e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3848, 'grad_norm': 7.549436092376709, 'learning_rate': 4.100494473531123e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1367, 'grad_norm': 7.0765380859375, 'learning_rate': 4.09976730657359e-05, 'epoch': 0.54}\n",
      "{'loss': 3.168, 'grad_norm': 8.134109497070312, 'learning_rate': 4.099040139616056e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1994, 'grad_norm': 7.248431205749512, 'learning_rate': 4.0983129726585225e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1838, 'grad_norm': 8.451437950134277, 'learning_rate': 4.097585805700989e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2461, 'grad_norm': 7.208248138427734, 'learning_rate': 4.096858638743455e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1775, 'grad_norm': 7.530059814453125, 'learning_rate': 4.0961314717859225e-05, 'epoch': 0.54}\n",
      "{'loss': 3.0951, 'grad_norm': 7.355241775512695, 'learning_rate': 4.0954043048283885e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2176, 'grad_norm': 7.520098686218262, 'learning_rate': 4.094677137870855e-05, 'epoch': 0.54}\n",
      "{'loss': 3.251, 'grad_norm': 7.835165977478027, 'learning_rate': 4.093949970913322e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1477, 'grad_norm': 8.28936767578125, 'learning_rate': 4.0932228039557886e-05, 'epoch': 0.54}\n",
      "{'loss': 3.1215, 'grad_norm': 9.084975242614746, 'learning_rate': 4.0924956369982546e-05, 'epoch': 0.54}\n",
      "{'loss': 3.3533, 'grad_norm': 7.024189472198486, 'learning_rate': 4.091768470040721e-05, 'epoch': 0.54}\n",
      "{'loss': 3.2486, 'grad_norm': 7.176736354827881, 'learning_rate': 4.091041303083188e-05, 'epoch': 0.55}\n",
      " 18%|█████▊                          | 12500/68760 [2:20:31<10:09:39,  1.54it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.07it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.16s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.192892, 'eval_rouge-2': 7.926572000000001, 'eval_rouge-l': 26.175979999999996, 'eval_bleu-4': 0.03974804488302258, 'eval_runtime': 17.0396, 'eval_samples_per_second': 2.934, 'eval_steps_per_second': 0.235, 'epoch': 0.55}\n",
      " 18%|█████▊                          | 12500/68760 [2:20:48<10:09:39,  1.54it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.21s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-12500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2969, 'grad_norm': 7.578760147094727, 'learning_rate': 4.090314136125655e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2002, 'grad_norm': 7.182266712188721, 'learning_rate': 4.089586969168121e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2498, 'grad_norm': 8.705859184265137, 'learning_rate': 4.088859802210588e-05, 'epoch': 0.55}\n",
      "{'loss': 3.315, 'grad_norm': 7.6403656005859375, 'learning_rate': 4.088132635253054e-05, 'epoch': 0.55}\n",
      "{'loss': 3.165, 'grad_norm': 7.980368614196777, 'learning_rate': 4.087405468295521e-05, 'epoch': 0.55}\n",
      "{'loss': 3.0697, 'grad_norm': 8.117249488830566, 'learning_rate': 4.0866783013379875e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2531, 'grad_norm': 7.7087297439575195, 'learning_rate': 4.085951134380454e-05, 'epoch': 0.55}\n",
      "{'loss': 3.249, 'grad_norm': 7.829921245574951, 'learning_rate': 4.08522396742292e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2287, 'grad_norm': 7.768955230712891, 'learning_rate': 4.084496800465387e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2016, 'grad_norm': 7.97939395904541, 'learning_rate': 4.0837696335078535e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3203, 'grad_norm': 8.034029006958008, 'learning_rate': 4.08304246655032e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2248, 'grad_norm': 7.4527907371521, 'learning_rate': 4.082315299592786e-05, 'epoch': 0.55}\n",
      "{'loss': 3.1816, 'grad_norm': 7.601128578186035, 'learning_rate': 4.0815881326352536e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2201, 'grad_norm': 7.4996442794799805, 'learning_rate': 4.0808609656777196e-05, 'epoch': 0.55}\n",
      "{'loss': 3.1842, 'grad_norm': 6.862509250640869, 'learning_rate': 4.080133798720186e-05, 'epoch': 0.55}\n",
      "{'loss': 3.1953, 'grad_norm': 8.692357063293457, 'learning_rate': 4.079406631762653e-05, 'epoch': 0.55}\n",
      "{'loss': 3.1803, 'grad_norm': 7.300843715667725, 'learning_rate': 4.07867946480512e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2348, 'grad_norm': 7.852153301239014, 'learning_rate': 4.077952297847586e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2314, 'grad_norm': 7.42076301574707, 'learning_rate': 4.077225130890053e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2613, 'grad_norm': 7.932766437530518, 'learning_rate': 4.076497963932519e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3402, 'grad_norm': 7.606743335723877, 'learning_rate': 4.075770796974986e-05, 'epoch': 0.55}\n",
      "{'loss': 3.3209, 'grad_norm': 7.697450160980225, 'learning_rate': 4.0750436300174525e-05, 'epoch': 0.55}\n",
      "{'loss': 3.276, 'grad_norm': 7.91611909866333, 'learning_rate': 4.074316463059919e-05, 'epoch': 0.56}\n",
      "{'loss': 3.0877, 'grad_norm': 8.747671127319336, 'learning_rate': 4.073589296102385e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2045, 'grad_norm': 7.926672458648682, 'learning_rate': 4.072862129144852e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2801, 'grad_norm': 6.880202293395996, 'learning_rate': 4.0721349621873185e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2191, 'grad_norm': 8.721514701843262, 'learning_rate': 4.071407795229785e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1049, 'grad_norm': 7.453094005584717, 'learning_rate': 4.070680628272251e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1775, 'grad_norm': 7.999392986297607, 'learning_rate': 4.0699534613147186e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2266, 'grad_norm': 7.57749605178833, 'learning_rate': 4.0692262943571846e-05, 'epoch': 0.56}\n",
      "{'loss': 3.0545, 'grad_norm': 6.932430267333984, 'learning_rate': 4.0684991273996506e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1506, 'grad_norm': 7.4698004722595215, 'learning_rate': 4.067771960442118e-05, 'epoch': 0.56}\n",
      "{'loss': 3.133, 'grad_norm': 8.401163101196289, 'learning_rate': 4.067044793484584e-05, 'epoch': 0.56}\n",
      "{'loss': 3.283, 'grad_norm': 7.874455451965332, 'learning_rate': 4.066317626527051e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1848, 'grad_norm': 8.104046821594238, 'learning_rate': 4.0655904595695174e-05, 'epoch': 0.56}\n",
      "{'loss': 3.2109, 'grad_norm': 7.710710525512695, 'learning_rate': 4.064863292611984e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1535, 'grad_norm': 7.390378475189209, 'learning_rate': 4.06413612565445e-05, 'epoch': 0.56}\n",
      "{'loss': 3.234, 'grad_norm': 7.414527893066406, 'learning_rate': 4.063408958696917e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1924, 'grad_norm': 7.320301532745361, 'learning_rate': 4.0626817917393835e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1064, 'grad_norm': 6.992701530456543, 'learning_rate': 4.06195462478185e-05, 'epoch': 0.56}\n",
      "{'loss': 3.0396, 'grad_norm': 7.369624137878418, 'learning_rate': 4.061227457824316e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1498, 'grad_norm': 8.085625648498535, 'learning_rate': 4.0605002908667835e-05, 'epoch': 0.56}\n",
      "{'loss': 3.168, 'grad_norm': 7.038574695587158, 'learning_rate': 4.0597731239092496e-05, 'epoch': 0.56}\n",
      "{'loss': 3.1777, 'grad_norm': 7.902235507965088, 'learning_rate': 4.059045956951716e-05, 'epoch': 0.56}\n",
      "{'loss': 2.9516, 'grad_norm': 7.573413848876953, 'learning_rate': 4.058318789994183e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2297, 'grad_norm': 7.445470333099365, 'learning_rate': 4.0575916230366496e-05, 'epoch': 0.57}\n",
      "{'loss': 3.0936, 'grad_norm': 7.28718376159668, 'learning_rate': 4.0568644560791156e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2389, 'grad_norm': 7.7111735343933105, 'learning_rate': 4.056137289121582e-05, 'epoch': 0.57}\n",
      "{'loss': 3.1346, 'grad_norm': 8.188786506652832, 'learning_rate': 4.055410122164049e-05, 'epoch': 0.57}\n",
      "{'loss': 3.1016, 'grad_norm': 7.524961948394775, 'learning_rate': 4.054682955206516e-05, 'epoch': 0.57}\n",
      " 19%|██████▏                          | 13000/68760 [2:25:59<8:25:46,  1.84it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.093058, 'eval_rouge-2': 7.383958, 'eval_rouge-l': 24.225722000000005, 'eval_bleu-4': 0.034598868653827525, 'eval_runtime': 37.6297, 'eval_samples_per_second': 1.329, 'eval_steps_per_second': 0.106, 'epoch': 0.57}\n",
      " 19%|██████▏                          | 13000/68760 [2:26:36<8:25:46,  1.84it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.78s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-13000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2473, 'grad_norm': 8.041695594787598, 'learning_rate': 4.053955788248982e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2291, 'grad_norm': 6.8724846839904785, 'learning_rate': 4.053228621291449e-05, 'epoch': 0.57}\n",
      "{'loss': 3.1756, 'grad_norm': 8.12236499786377, 'learning_rate': 4.052501454333915e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2359, 'grad_norm': 7.566225051879883, 'learning_rate': 4.051774287376382e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2578, 'grad_norm': 7.357451438903809, 'learning_rate': 4.0510471204188485e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2156, 'grad_norm': 7.662652015686035, 'learning_rate': 4.050319953461315e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2104, 'grad_norm': 7.484418869018555, 'learning_rate': 4.049592786503781e-05, 'epoch': 0.57}\n",
      "{'loss': 3.3273, 'grad_norm': 9.727614402770996, 'learning_rate': 4.0488656195462485e-05, 'epoch': 0.57}\n",
      "{'loss': 3.1568, 'grad_norm': 7.754488468170166, 'learning_rate': 4.0481384525887146e-05, 'epoch': 0.57}\n",
      "{'loss': 3.1229, 'grad_norm': 7.5159735679626465, 'learning_rate': 4.047411285631181e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2148, 'grad_norm': 7.434347152709961, 'learning_rate': 4.046684118673648e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2199, 'grad_norm': 7.273381233215332, 'learning_rate': 4.0459569517161146e-05, 'epoch': 0.57}\n",
      "{'loss': 3.099, 'grad_norm': 7.8419365882873535, 'learning_rate': 4.0452297847585806e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2395, 'grad_norm': 7.591102123260498, 'learning_rate': 4.044502617801047e-05, 'epoch': 0.57}\n",
      "{'loss': 3.1748, 'grad_norm': 8.5763521194458, 'learning_rate': 4.043775450843514e-05, 'epoch': 0.57}\n",
      "{'loss': 3.1521, 'grad_norm': 8.580607414245605, 'learning_rate': 4.043048283885981e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2367, 'grad_norm': 8.099160194396973, 'learning_rate': 4.042321116928447e-05, 'epoch': 0.57}\n",
      "{'loss': 3.2133, 'grad_norm': 7.582543849945068, 'learning_rate': 4.041593949970914e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1832, 'grad_norm': 8.616524696350098, 'learning_rate': 4.04086678301338e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2979, 'grad_norm': 7.6247944831848145, 'learning_rate': 4.040139616055846e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1752, 'grad_norm': 7.279486656188965, 'learning_rate': 4.0394124490983135e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1773, 'grad_norm': 7.4089765548706055, 'learning_rate': 4.0386852821407795e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1611, 'grad_norm': 8.02471923828125, 'learning_rate': 4.037958115183246e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1635, 'grad_norm': 7.741641521453857, 'learning_rate': 4.037230948225713e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2623, 'grad_norm': 8.108863830566406, 'learning_rate': 4.0365037812681796e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1309, 'grad_norm': 8.59316349029541, 'learning_rate': 4.0357766143106456e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1637, 'grad_norm': 8.965564727783203, 'learning_rate': 4.035049447353112e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2102, 'grad_norm': 7.669646739959717, 'learning_rate': 4.034322280395579e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3193, 'grad_norm': 7.246290683746338, 'learning_rate': 4.0335951134380456e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1123, 'grad_norm': 7.570312023162842, 'learning_rate': 4.0328679464805117e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2348, 'grad_norm': 8.43057632446289, 'learning_rate': 4.032140779522979e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2502, 'grad_norm': 8.352787971496582, 'learning_rate': 4.031413612565445e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1668, 'grad_norm': 7.668426036834717, 'learning_rate': 4.030686445607912e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2762, 'grad_norm': 7.781623363494873, 'learning_rate': 4.0299592786503784e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3131, 'grad_norm': 7.983165264129639, 'learning_rate': 4.029232111692845e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3049, 'grad_norm': 7.969620227813721, 'learning_rate': 4.028504944735311e-05, 'epoch': 0.58}\n",
      "{'loss': 3.3506, 'grad_norm': 8.2532958984375, 'learning_rate': 4.027777777777778e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1359, 'grad_norm': 7.711508274078369, 'learning_rate': 4.0270506108202445e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2732, 'grad_norm': 7.035286903381348, 'learning_rate': 4.026323443862711e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1754, 'grad_norm': 7.747228622436523, 'learning_rate': 4.025596276905177e-05, 'epoch': 0.58}\n",
      "{'loss': 3.2928, 'grad_norm': 8.152522087097168, 'learning_rate': 4.0248691099476446e-05, 'epoch': 0.59}\n",
      "{'loss': 3.073, 'grad_norm': 7.5128302574157715, 'learning_rate': 4.0241419429901106e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2646, 'grad_norm': 7.259652137756348, 'learning_rate': 4.023414776032577e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2801, 'grad_norm': 9.847668647766113, 'learning_rate': 4.022687609075044e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1215, 'grad_norm': 7.917150974273682, 'learning_rate': 4.0219604421175106e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1951, 'grad_norm': 7.984715461730957, 'learning_rate': 4.0212332751599767e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2785, 'grad_norm': 7.891687870025635, 'learning_rate': 4.0205061082024433e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1703, 'grad_norm': 7.290482044219971, 'learning_rate': 4.01977894124491e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1387, 'grad_norm': 7.803276062011719, 'learning_rate': 4.019051774287377e-05, 'epoch': 0.59}\n",
      "{'loss': 3.225, 'grad_norm': 7.358601093292236, 'learning_rate': 4.018324607329843e-05, 'epoch': 0.59}\n",
      " 20%|██████▍                          | 13500/68760 [2:31:46<9:22:45,  1.64it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.770032, 'eval_rouge-2': 8.52684, 'eval_rouge-l': 25.218673999999996, 'eval_bleu-4': 0.04066765416504342, 'eval_runtime': 37.9494, 'eval_samples_per_second': 1.318, 'eval_steps_per_second': 0.105, 'epoch': 0.59}\n",
      " 20%|██████▍                          | 13500/68760 [2:32:23<9:22:45,  1.64it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.89s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-13500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2951, 'grad_norm': 8.419544219970703, 'learning_rate': 4.01759744037231e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2568, 'grad_norm': 8.281381607055664, 'learning_rate': 4.016870273414776e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1422, 'grad_norm': 7.525489807128906, 'learning_rate': 4.016143106457243e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1846, 'grad_norm': 7.25645112991333, 'learning_rate': 4.0154159394997095e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2836, 'grad_norm': 7.994227886199951, 'learning_rate': 4.014688772542176e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2674, 'grad_norm': 7.413639068603516, 'learning_rate': 4.013961605584642e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2736, 'grad_norm': 7.804602146148682, 'learning_rate': 4.0132344386271096e-05, 'epoch': 0.59}\n",
      "{'loss': 3.3572, 'grad_norm': 6.755675792694092, 'learning_rate': 4.0125072716695756e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2186, 'grad_norm': 7.732108116149902, 'learning_rate': 4.011780104712042e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2584, 'grad_norm': 8.176541328430176, 'learning_rate': 4.011052937754509e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2684, 'grad_norm': 7.634589195251465, 'learning_rate': 4.010325770796975e-05, 'epoch': 0.59}\n",
      "{'loss': 3.0947, 'grad_norm': 8.665489196777344, 'learning_rate': 4.0095986038394417e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1541, 'grad_norm': 7.894349098205566, 'learning_rate': 4.0088714368819083e-05, 'epoch': 0.59}\n",
      "{'loss': 3.2764, 'grad_norm': 7.3632493019104, 'learning_rate': 4.008144269924375e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2209, 'grad_norm': 7.398014545440674, 'learning_rate': 4.007417102966841e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2102, 'grad_norm': 7.291388511657715, 'learning_rate': 4.006689936009308e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2684, 'grad_norm': 8.298026084899902, 'learning_rate': 4.0059627690517744e-05, 'epoch': 0.6}\n",
      "{'loss': 3.3395, 'grad_norm': 7.840210437774658, 'learning_rate': 4.005235602094241e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1672, 'grad_norm': 7.202576160430908, 'learning_rate': 4.004508435136707e-05, 'epoch': 0.6}\n",
      "{'loss': 3.274, 'grad_norm': 7.410611629486084, 'learning_rate': 4.0037812681791745e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2648, 'grad_norm': 7.146062850952148, 'learning_rate': 4.0030541012216405e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1326, 'grad_norm': 6.660699367523193, 'learning_rate': 4.002326934264107e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1027, 'grad_norm': 7.2488203048706055, 'learning_rate': 4.001599767306574e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2146, 'grad_norm': 8.087028503417969, 'learning_rate': 4.0008726003490406e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1557, 'grad_norm': 7.809024810791016, 'learning_rate': 4.0001454333915066e-05, 'epoch': 0.6}\n",
      "{'loss': 3.202, 'grad_norm': 8.243325233459473, 'learning_rate': 3.999418266433973e-05, 'epoch': 0.6}\n",
      "{'loss': 2.9674, 'grad_norm': 7.778382301330566, 'learning_rate': 3.99869109947644e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2156, 'grad_norm': 7.446516513824463, 'learning_rate': 3.9979639325189067e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1807, 'grad_norm': 8.013067245483398, 'learning_rate': 3.997236765561373e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1025, 'grad_norm': 8.24184799194336, 'learning_rate': 3.99650959860384e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2924, 'grad_norm': 9.34044361114502, 'learning_rate': 3.995782431646306e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1678, 'grad_norm': 7.6403727531433105, 'learning_rate': 3.995055264688773e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2111, 'grad_norm': 7.685421943664551, 'learning_rate': 3.9943280977312394e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1635, 'grad_norm': 7.524327278137207, 'learning_rate': 3.993600930773706e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1902, 'grad_norm': 7.509565353393555, 'learning_rate': 3.992873763816172e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2607, 'grad_norm': 8.077951431274414, 'learning_rate': 3.992146596858639e-05, 'epoch': 0.6}\n",
      "{'loss': 3.1195, 'grad_norm': 7.370335578918457, 'learning_rate': 3.9914194299011055e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2822, 'grad_norm': 8.177242279052734, 'learning_rate': 3.990692262943572e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3375, 'grad_norm': 7.753490447998047, 'learning_rate': 3.989965095986038e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2213, 'grad_norm': 7.687072277069092, 'learning_rate': 3.9892379290285056e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2762, 'grad_norm': 7.925093650817871, 'learning_rate': 3.9885107620709716e-05, 'epoch': 0.61}\n",
      "{'loss': 3.166, 'grad_norm': 7.730546951293945, 'learning_rate': 3.987783595113438e-05, 'epoch': 0.61}\n",
      "{'loss': 3.1588, 'grad_norm': 8.923234939575195, 'learning_rate': 3.987056428155905e-05, 'epoch': 0.61}\n",
      "{'loss': 3.1193, 'grad_norm': 7.619490146636963, 'learning_rate': 3.9863292611983717e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2139, 'grad_norm': 8.066890716552734, 'learning_rate': 3.985602094240838e-05, 'epoch': 0.61}\n",
      "{'loss': 3.243, 'grad_norm': 7.533508777618408, 'learning_rate': 3.984874927283305e-05, 'epoch': 0.61}\n",
      "{'loss': 3.326, 'grad_norm': 7.8159613609313965, 'learning_rate': 3.984147760325771e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3195, 'grad_norm': 8.947622299194336, 'learning_rate': 3.983420593368238e-05, 'epoch': 0.61}\n",
      "{'loss': 3.216, 'grad_norm': 8.13452434539795, 'learning_rate': 3.9826934264107044e-05, 'epoch': 0.61}\n",
      "{'loss': 3.3994, 'grad_norm': 7.600617408752441, 'learning_rate': 3.9819662594531704e-05, 'epoch': 0.61}\n",
      " 20%|██████▋                          | 14000/68760 [2:37:30<8:54:28,  1.71it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.37s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.940342, 'eval_rouge-2': 8.208474, 'eval_rouge-l': 24.762956, 'eval_bleu-4': 0.03568461339067024, 'eval_runtime': 27.7641, 'eval_samples_per_second': 1.801, 'eval_steps_per_second': 0.144, 'epoch': 0.61}\n",
      " 20%|██████▋                          | 14000/68760 [2:37:58<8:54:28,  1.71it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.19s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-14000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2525, 'grad_norm': 8.575270652770996, 'learning_rate': 3.981239092495637e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2064, 'grad_norm': 8.79738998413086, 'learning_rate': 3.980511925538104e-05, 'epoch': 0.61}\n",
      "{'loss': 3.1535, 'grad_norm': 7.956704616546631, 'learning_rate': 3.9797847585805705e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2418, 'grad_norm': 7.213415622711182, 'learning_rate': 3.9790575916230365e-05, 'epoch': 0.61}\n",
      "{'loss': 3.0902, 'grad_norm': 7.325032711029053, 'learning_rate': 3.978330424665503e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2381, 'grad_norm': 8.852131843566895, 'learning_rate': 3.97760325770797e-05, 'epoch': 0.61}\n",
      "{'loss': 3.2482, 'grad_norm': 7.301793575286865, 'learning_rate': 3.9768760907504366e-05, 'epoch': 0.61}\n",
      "{'loss': 3.0693, 'grad_norm': 7.2745795249938965, 'learning_rate': 3.9761489237929026e-05, 'epoch': 0.61}\n",
      "{'loss': 3.1063, 'grad_norm': 7.164819717407227, 'learning_rate': 3.97542175683537e-05, 'epoch': 0.61}\n",
      "{'loss': 3.208, 'grad_norm': 7.97818660736084, 'learning_rate': 3.974694589877836e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2223, 'grad_norm': 8.006753921508789, 'learning_rate': 3.973967422920303e-05, 'epoch': 0.62}\n",
      "{'loss': 3.0664, 'grad_norm': 7.905484676361084, 'learning_rate': 3.9732402559627694e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2293, 'grad_norm': 7.324699878692627, 'learning_rate': 3.972513089005236e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2947, 'grad_norm': 7.789957523345947, 'learning_rate': 3.971785922047702e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1123, 'grad_norm': 6.864917755126953, 'learning_rate': 3.971058755090169e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2146, 'grad_norm': 7.217103481292725, 'learning_rate': 3.9703315881326354e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1736, 'grad_norm': 8.15804672241211, 'learning_rate': 3.969604421175102e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2613, 'grad_norm': 7.378493309020996, 'learning_rate': 3.968877254217568e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1395, 'grad_norm': 7.553635597229004, 'learning_rate': 3.9681500872600355e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2672, 'grad_norm': 7.621645450592041, 'learning_rate': 3.9674229203025015e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2328, 'grad_norm': 7.871247291564941, 'learning_rate': 3.966695753344968e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2365, 'grad_norm': 8.37960147857666, 'learning_rate': 3.965968586387435e-05, 'epoch': 0.62}\n",
      "{'loss': 3.318, 'grad_norm': 8.048015594482422, 'learning_rate': 3.9652414194299016e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1768, 'grad_norm': 8.168550491333008, 'learning_rate': 3.9645142524723676e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2572, 'grad_norm': 7.252255439758301, 'learning_rate': 3.963787085514834e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2076, 'grad_norm': 7.659902572631836, 'learning_rate': 3.963059918557301e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3098, 'grad_norm': 8.73670768737793, 'learning_rate': 3.962332751599768e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2266, 'grad_norm': 7.475680351257324, 'learning_rate': 3.961605584642234e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2488, 'grad_norm': 7.391939163208008, 'learning_rate': 3.960878417684701e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2762, 'grad_norm': 7.375321865081787, 'learning_rate': 3.960151250727167e-05, 'epoch': 0.62}\n",
      "{'loss': 3.2213, 'grad_norm': 7.590917110443115, 'learning_rate': 3.959424083769634e-05, 'epoch': 0.62}\n",
      "{'loss': 3.1271, 'grad_norm': 7.610018730163574, 'learning_rate': 3.9586969168121004e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3287, 'grad_norm': 7.366739273071289, 'learning_rate': 3.957969749854567e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2029, 'grad_norm': 8.22976016998291, 'learning_rate': 3.957242582897033e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1127, 'grad_norm': 7.463948726654053, 'learning_rate': 3.9565154159395e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1615, 'grad_norm': 9.513212203979492, 'learning_rate': 3.9557882489819665e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1037, 'grad_norm': 8.098750114440918, 'learning_rate': 3.955061082024433e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1633, 'grad_norm': 7.284276962280273, 'learning_rate': 3.954333915066899e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2443, 'grad_norm': 8.006148338317871, 'learning_rate': 3.9536067481093666e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2088, 'grad_norm': 7.710770130157471, 'learning_rate': 3.9528795811518326e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2568, 'grad_norm': 8.133862495422363, 'learning_rate': 3.9521524141942986e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3008, 'grad_norm': 7.856619834899902, 'learning_rate': 3.951425247236766e-05, 'epoch': 0.63}\n",
      "{'loss': 3.257, 'grad_norm': 7.770253658294678, 'learning_rate': 3.950698080279232e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2896, 'grad_norm': 7.4172539710998535, 'learning_rate': 3.949970913321699e-05, 'epoch': 0.63}\n",
      "{'loss': 3.4107, 'grad_norm': 7.960063457489014, 'learning_rate': 3.9492437463641654e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3471, 'grad_norm': 7.26173210144043, 'learning_rate': 3.948516579406632e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2514, 'grad_norm': 7.070158004760742, 'learning_rate': 3.947789412449098e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1885, 'grad_norm': 7.990306854248047, 'learning_rate': 3.9470622454915654e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2789, 'grad_norm': 7.422043800354004, 'learning_rate': 3.9463350785340314e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1336, 'grad_norm': 7.275355339050293, 'learning_rate': 3.945607911576498e-05, 'epoch': 0.63}\n",
      " 21%|██████▉                          | 14500/68760 [2:43:08<9:16:17,  1.63it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.26s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.676304, 'eval_rouge-2': 7.179136, 'eval_rouge-l': 24.778598, 'eval_bleu-4': 0.02941898450398857, 'eval_runtime': 27.4199, 'eval_samples_per_second': 1.823, 'eval_steps_per_second': 0.146, 'epoch': 0.63}\n",
      " 21%|██████▉                          | 14500/68760 [2:43:35<9:16:17,  1.63it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.07s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-14500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2193, 'grad_norm': 8.87010669708252, 'learning_rate': 3.944880744618965e-05, 'epoch': 0.63}\n",
      "{'loss': 3.3914, 'grad_norm': 7.5801849365234375, 'learning_rate': 3.9441535776614315e-05, 'epoch': 0.63}\n",
      "{'loss': 3.152, 'grad_norm': 7.333428382873535, 'learning_rate': 3.9434264107038975e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2385, 'grad_norm': 8.253605842590332, 'learning_rate': 3.942699243746364e-05, 'epoch': 0.63}\n",
      "{'loss': 3.2617, 'grad_norm': 8.328642845153809, 'learning_rate': 3.941972076788831e-05, 'epoch': 0.63}\n",
      "{'loss': 3.1869, 'grad_norm': 7.839198112487793, 'learning_rate': 3.9412449098312976e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2518, 'grad_norm': 7.737492084503174, 'learning_rate': 3.9405177428737636e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2162, 'grad_norm': 7.749752044677734, 'learning_rate': 3.939790575916231e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2062, 'grad_norm': 7.811419486999512, 'learning_rate': 3.939063408958697e-05, 'epoch': 0.64}\n",
      "{'loss': 3.1973, 'grad_norm': 7.709822654724121, 'learning_rate': 3.938336242001164e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3207, 'grad_norm': 7.152307510375977, 'learning_rate': 3.9376090750436304e-05, 'epoch': 0.64}\n",
      "{'loss': 3.1598, 'grad_norm': 7.163972854614258, 'learning_rate': 3.936881908086097e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2738, 'grad_norm': 7.682249069213867, 'learning_rate': 3.936154741128563e-05, 'epoch': 0.64}\n",
      "{'loss': 3.1943, 'grad_norm': 8.187731742858887, 'learning_rate': 3.93542757417103e-05, 'epoch': 0.64}\n",
      "{'loss': 3.14, 'grad_norm': 7.72268009185791, 'learning_rate': 3.9347004072134964e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2156, 'grad_norm': 8.578497886657715, 'learning_rate': 3.933973240255963e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2059, 'grad_norm': 7.747208595275879, 'learning_rate': 3.933246073298429e-05, 'epoch': 0.64}\n",
      "{'loss': 3.1805, 'grad_norm': 7.9249372482299805, 'learning_rate': 3.9325189063408965e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2598, 'grad_norm': 7.882843971252441, 'learning_rate': 3.9317917393833625e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2002, 'grad_norm': 7.112048625946045, 'learning_rate': 3.931064572425829e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2887, 'grad_norm': 8.689447402954102, 'learning_rate': 3.930337405468296e-05, 'epoch': 0.64}\n",
      "{'loss': 3.0949, 'grad_norm': 9.26287841796875, 'learning_rate': 3.9296102385107626e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2621, 'grad_norm': 7.890876293182373, 'learning_rate': 3.9288830715532286e-05, 'epoch': 0.64}\n",
      "{'loss': 3.1982, 'grad_norm': 7.945784568786621, 'learning_rate': 3.928155904595695e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2541, 'grad_norm': 8.068754196166992, 'learning_rate': 3.927428737638162e-05, 'epoch': 0.64}\n",
      "{'loss': 3.3055, 'grad_norm': 7.034875392913818, 'learning_rate': 3.926701570680629e-05, 'epoch': 0.64}\n",
      "{'loss': 3.258, 'grad_norm': 7.4520649909973145, 'learning_rate': 3.925974403723095e-05, 'epoch': 0.64}\n",
      "{'loss': 3.0906, 'grad_norm': 8.277494430541992, 'learning_rate': 3.925247236765562e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2037, 'grad_norm': 8.585870742797852, 'learning_rate': 3.924520069808028e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1852, 'grad_norm': 7.7995476722717285, 'learning_rate': 3.923792902850494e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1637, 'grad_norm': 7.904664039611816, 'learning_rate': 3.9230657358929614e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1043, 'grad_norm': 7.62562894821167, 'learning_rate': 3.9223385689354275e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2109, 'grad_norm': 7.630485534667969, 'learning_rate': 3.921611401977894e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1629, 'grad_norm': 8.441975593566895, 'learning_rate': 3.920884235020361e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1893, 'grad_norm': 8.133808135986328, 'learning_rate': 3.9201570680628275e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2928, 'grad_norm': 8.026966094970703, 'learning_rate': 3.9194299011052935e-05, 'epoch': 0.65}\n",
      "{'loss': 3.183, 'grad_norm': 7.214130878448486, 'learning_rate': 3.918702734147761e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2703, 'grad_norm': 7.927828311920166, 'learning_rate': 3.917975567190227e-05, 'epoch': 0.65}\n",
      "{'loss': 3.0979, 'grad_norm': 7.300676345825195, 'learning_rate': 3.9172484002326936e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1412, 'grad_norm': 8.40229606628418, 'learning_rate': 3.91652123327516e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2311, 'grad_norm': 8.360015869140625, 'learning_rate': 3.915794066317627e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2445, 'grad_norm': 7.874113082885742, 'learning_rate': 3.915066899360093e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1764, 'grad_norm': 7.9377851486206055, 'learning_rate': 3.91433973240256e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2975, 'grad_norm': 7.570348739624023, 'learning_rate': 3.9136125654450264e-05, 'epoch': 0.65}\n",
      "{'loss': 3.0629, 'grad_norm': 8.17567253112793, 'learning_rate': 3.912885398487493e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2375, 'grad_norm': 7.852899074554443, 'learning_rate': 3.912158231529959e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1355, 'grad_norm': 7.67465353012085, 'learning_rate': 3.9114310645724264e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2504, 'grad_norm': 7.810197830200195, 'learning_rate': 3.9107038976148925e-05, 'epoch': 0.65}\n",
      "{'loss': 3.3154, 'grad_norm': 7.427430629730225, 'learning_rate': 3.909976730657359e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1299, 'grad_norm': 10.571555137634277, 'learning_rate': 3.909249563699826e-05, 'epoch': 0.65}\n",
      " 22%|███████▏                         | 15000/68760 [2:48:44<9:06:53,  1.64it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.49s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.62s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.042374, 'eval_rouge-2': 8.367108, 'eval_rouge-l': 25.628223999999996, 'eval_bleu-4': 0.038869888327025, 'eval_runtime': 18.4769, 'eval_samples_per_second': 2.706, 'eval_steps_per_second': 0.216, 'epoch': 0.65}\n",
      " 22%|███████▏                         | 15000/68760 [2:49:02<9:06:53,  1.64it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.51s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-15000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2375, 'grad_norm': 7.437739849090576, 'learning_rate': 3.9085223967422925e-05, 'epoch': 0.65}\n",
      "{'loss': 3.1109, 'grad_norm': 7.833884239196777, 'learning_rate': 3.9077952297847585e-05, 'epoch': 0.66}\n",
      "{'loss': 3.1838, 'grad_norm': 8.362568855285645, 'learning_rate': 3.907068062827225e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2217, 'grad_norm': 8.559876441955566, 'learning_rate': 3.906340895869692e-05, 'epoch': 0.66}\n",
      "{'loss': 3.1844, 'grad_norm': 7.802859783172607, 'learning_rate': 3.9056137289121586e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2695, 'grad_norm': 7.057811737060547, 'learning_rate': 3.9048865619546246e-05, 'epoch': 0.66}\n",
      "{'loss': 3.283, 'grad_norm': 8.398641586303711, 'learning_rate': 3.904159394997092e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2148, 'grad_norm': 8.331385612487793, 'learning_rate': 3.903432228039558e-05, 'epoch': 0.66}\n",
      "{'loss': 3.1246, 'grad_norm': 9.422054290771484, 'learning_rate': 3.902705061082025e-05, 'epoch': 0.66}\n",
      "{'loss': 3.3492, 'grad_norm': 7.563380718231201, 'learning_rate': 3.9019778941244914e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2207, 'grad_norm': 7.952386856079102, 'learning_rate': 3.901250727166958e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2828, 'grad_norm': 7.764816761016846, 'learning_rate': 3.900523560209424e-05, 'epoch': 0.66}\n",
      "{'loss': 3.0551, 'grad_norm': 7.560866832733154, 'learning_rate': 3.899796393251891e-05, 'epoch': 0.66}\n",
      "{'loss': 3.1859, 'grad_norm': 8.705483436584473, 'learning_rate': 3.8990692262943575e-05, 'epoch': 0.66}\n",
      "{'loss': 3.1828, 'grad_norm': 7.471484184265137, 'learning_rate': 3.898342059336824e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2314, 'grad_norm': 8.112687110900879, 'learning_rate': 3.89761489237929e-05, 'epoch': 0.66}\n",
      "{'loss': 3.135, 'grad_norm': 8.033927917480469, 'learning_rate': 3.8968877254217575e-05, 'epoch': 0.66}\n",
      "{'loss': 3.1572, 'grad_norm': 7.265153408050537, 'learning_rate': 3.8961605584642235e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2174, 'grad_norm': 8.985620498657227, 'learning_rate': 3.89543339150669e-05, 'epoch': 0.66}\n",
      "{'loss': 3.1389, 'grad_norm': 8.033138275146484, 'learning_rate': 3.894706224549157e-05, 'epoch': 0.66}\n",
      "{'loss': 3.182, 'grad_norm': 7.719212055206299, 'learning_rate': 3.893979057591623e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2066, 'grad_norm': 7.750281810760498, 'learning_rate': 3.8932518906340896e-05, 'epoch': 0.66}\n",
      "{'loss': 3.175, 'grad_norm': 7.9879326820373535, 'learning_rate': 3.892524723676556e-05, 'epoch': 0.66}\n",
      "{'loss': 3.2863, 'grad_norm': 7.625528812408447, 'learning_rate': 3.891797556719023e-05, 'epoch': 0.66}\n",
      "{'loss': 3.1561, 'grad_norm': 7.30778694152832, 'learning_rate': 3.891070389761489e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2973, 'grad_norm': 7.481345176696777, 'learning_rate': 3.890343222803956e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1178, 'grad_norm': 8.50058364868164, 'learning_rate': 3.8896160558464224e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1598, 'grad_norm': 7.0404839515686035, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}\n",
      "{'loss': 3.0777, 'grad_norm': 7.898126602172852, 'learning_rate': 3.888161721931355e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2189, 'grad_norm': 8.583812713623047, 'learning_rate': 3.8874345549738225e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1885, 'grad_norm': 8.655389785766602, 'learning_rate': 3.8867073880162885e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1238, 'grad_norm': 8.084320068359375, 'learning_rate': 3.885980221058755e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1828, 'grad_norm': 8.339756965637207, 'learning_rate': 3.885253054101222e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1311, 'grad_norm': 7.2421393394470215, 'learning_rate': 3.8845258871436885e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1971, 'grad_norm': 7.819986820220947, 'learning_rate': 3.8837987201861545e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2941, 'grad_norm': 8.136922836303711, 'learning_rate': 3.883071553228622e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2613, 'grad_norm': 8.451147079467773, 'learning_rate': 3.882344386271088e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1789, 'grad_norm': 8.171916961669922, 'learning_rate': 3.8816172193135546e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1588, 'grad_norm': 8.555794715881348, 'learning_rate': 3.880890052356021e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2324, 'grad_norm': 8.89417839050293, 'learning_rate': 3.880162885398488e-05, 'epoch': 0.67}\n",
      "{'loss': 3.0893, 'grad_norm': 8.085563659667969, 'learning_rate': 3.879435718440954e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1059, 'grad_norm': 7.76287841796875, 'learning_rate': 3.878708551483421e-05, 'epoch': 0.67}\n",
      "{'loss': 3.3668, 'grad_norm': 7.7031941413879395, 'learning_rate': 3.8779813845258874e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2023, 'grad_norm': 8.345295906066895, 'learning_rate': 3.877254217568354e-05, 'epoch': 0.67}\n",
      "{'loss': 3.0984, 'grad_norm': 7.1787519454956055, 'learning_rate': 3.87652705061082e-05, 'epoch': 0.67}\n",
      "{'loss': 3.1545, 'grad_norm': 7.58291482925415, 'learning_rate': 3.8757998836532875e-05, 'epoch': 0.67}\n",
      "{'loss': 3.357, 'grad_norm': 7.919417381286621, 'learning_rate': 3.8750727166957535e-05, 'epoch': 0.67}\n",
      "{'loss': 2.9627, 'grad_norm': 7.347199440002441, 'learning_rate': 3.87434554973822e-05, 'epoch': 0.68}\n",
      "{'loss': 3.3225, 'grad_norm': 8.248857498168945, 'learning_rate': 3.873618382780687e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1262, 'grad_norm': 7.616751670837402, 'learning_rate': 3.8728912158231535e-05, 'epoch': 0.68}\n",
      " 23%|███████▏                        | 15500/68760 [2:54:16<10:02:31,  1.47it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.02s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.26s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.67597, 'eval_rouge-2': 8.232088, 'eval_rouge-l': 26.844986, 'eval_bleu-4': 0.040479239305204834, 'eval_runtime': 16.9715, 'eval_samples_per_second': 2.946, 'eval_steps_per_second': 0.236, 'epoch': 0.68}\n",
      " 23%|███████▏                        | 15500/68760 [2:54:33<10:02:31,  1.47it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.14s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-15500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2086, 'grad_norm': 7.7474894523620605, 'learning_rate': 3.8721640488656195e-05, 'epoch': 0.68}\n",
      "{'loss': 3.0738, 'grad_norm': 7.279265880584717, 'learning_rate': 3.871436881908086e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1854, 'grad_norm': 7.76061487197876, 'learning_rate': 3.870709714950553e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1166, 'grad_norm': 8.096869468688965, 'learning_rate': 3.8699825479930196e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1951, 'grad_norm': 7.696727752685547, 'learning_rate': 3.8692553810354856e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2465, 'grad_norm': 8.077466011047363, 'learning_rate': 3.868528214077953e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2402, 'grad_norm': 8.842304229736328, 'learning_rate': 3.867801047120419e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1273, 'grad_norm': 7.939764499664307, 'learning_rate': 3.867073880162886e-05, 'epoch': 0.68}\n",
      "{'loss': 3.0621, 'grad_norm': 8.705913543701172, 'learning_rate': 3.8663467132053524e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2133, 'grad_norm': 7.340353965759277, 'learning_rate': 3.8656195462478184e-05, 'epoch': 0.68}\n",
      "{'loss': 3.0697, 'grad_norm': 7.101896286010742, 'learning_rate': 3.864892379290285e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2479, 'grad_norm': 7.737669944763184, 'learning_rate': 3.864165212332752e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2754, 'grad_norm': 8.387439727783203, 'learning_rate': 3.8634380453752185e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2051, 'grad_norm': 7.718190670013428, 'learning_rate': 3.8627108784176845e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1182, 'grad_norm': 8.346670150756836, 'learning_rate': 3.861983711460151e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2221, 'grad_norm': 6.959982395172119, 'learning_rate': 3.861256544502618e-05, 'epoch': 0.68}\n",
      "{'loss': 3.0709, 'grad_norm': 7.601166725158691, 'learning_rate': 3.8605293775450845e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1652, 'grad_norm': 7.828001499176025, 'learning_rate': 3.8598022105875506e-05, 'epoch': 0.68}\n",
      "{'loss': 3.1391, 'grad_norm': 7.32948112487793, 'learning_rate': 3.859075043630018e-05, 'epoch': 0.68}\n",
      "{'loss': 3.224, 'grad_norm': 8.388571739196777, 'learning_rate': 3.858347876672484e-05, 'epoch': 0.68}\n",
      "{'loss': 3.2703, 'grad_norm': 8.424269676208496, 'learning_rate': 3.8576207097149506e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2506, 'grad_norm': 8.598565101623535, 'learning_rate': 3.856893542757417e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2059, 'grad_norm': 8.442887306213379, 'learning_rate': 3.856166375799884e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2105, 'grad_norm': 7.836377143859863, 'learning_rate': 3.85543920884235e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3605, 'grad_norm': 7.369689464569092, 'learning_rate': 3.8547120418848174e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1246, 'grad_norm': 7.886969089508057, 'learning_rate': 3.8539848749272834e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2635, 'grad_norm': 7.741373538970947, 'learning_rate': 3.85325770796975e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1328, 'grad_norm': 8.096668243408203, 'learning_rate': 3.852530541012217e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1029, 'grad_norm': 7.261204719543457, 'learning_rate': 3.8518033740546835e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1377, 'grad_norm': 8.04552936553955, 'learning_rate': 3.8510762070971495e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2117, 'grad_norm': 8.014803886413574, 'learning_rate': 3.850349040139616e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3117, 'grad_norm': 7.419826030731201, 'learning_rate': 3.849621873182083e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1809, 'grad_norm': 7.548140525817871, 'learning_rate': 3.8488947062245495e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2615, 'grad_norm': 7.0924787521362305, 'learning_rate': 3.8481675392670156e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1678, 'grad_norm': 7.860629558563232, 'learning_rate': 3.847440372309483e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1693, 'grad_norm': 8.478116035461426, 'learning_rate': 3.846713205351949e-05, 'epoch': 0.69}\n",
      "{'loss': 3.1879, 'grad_norm': 8.719026565551758, 'learning_rate': 3.8459860383944156e-05, 'epoch': 0.69}\n",
      "{'loss': 3.0746, 'grad_norm': 7.605981826782227, 'learning_rate': 3.845258871436882e-05, 'epoch': 0.69}\n",
      "{'loss': 3.3191, 'grad_norm': 8.158651351928711, 'learning_rate': 3.844531704479349e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2031, 'grad_norm': 9.178013801574707, 'learning_rate': 3.843804537521815e-05, 'epoch': 0.69}\n",
      "{'loss': 3.199, 'grad_norm': 8.52994441986084, 'learning_rate': 3.843077370564282e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2264, 'grad_norm': 7.799450874328613, 'learning_rate': 3.8423502036067484e-05, 'epoch': 0.69}\n",
      "{'loss': 3.2129, 'grad_norm': 7.3829851150512695, 'learning_rate': 3.841623036649215e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1223, 'grad_norm': 7.21738862991333, 'learning_rate': 3.840895869691681e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1672, 'grad_norm': 11.268712043762207, 'learning_rate': 3.8401687027341485e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2434, 'grad_norm': 7.457207202911377, 'learning_rate': 3.8394415357766145e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2811, 'grad_norm': 7.979429721832275, 'learning_rate': 3.838714368819081e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1352, 'grad_norm': 7.973261833190918, 'learning_rate': 3.837987201861548e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2607, 'grad_norm': 8.307138442993164, 'learning_rate': 3.8372600349040145e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1873, 'grad_norm': 7.5743632316589355, 'learning_rate': 3.8365328679464806e-05, 'epoch': 0.7}\n",
      " 23%|███████▋                         | 16000/68760 [2:59:44<8:19:09,  1.76it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.36s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.77s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.416484, 'eval_rouge-2': 7.582746000000001, 'eval_rouge-l': 26.137952, 'eval_bleu-4': 0.03693283455638054, 'eval_runtime': 18.1159, 'eval_samples_per_second': 2.76, 'eval_steps_per_second': 0.221, 'epoch': 0.7}\n",
      " 23%|███████▋                         | 16000/68760 [3:00:02<8:19:09,  1.76it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  4.07s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-16000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2006, 'grad_norm': 6.890818119049072, 'learning_rate': 3.835805700988947e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2867, 'grad_norm': 8.5554838180542, 'learning_rate': 3.835078534031414e-05, 'epoch': 0.7}\n",
      "{'loss': 3.117, 'grad_norm': 7.9143218994140625, 'learning_rate': 3.83435136707388e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1553, 'grad_norm': 7.987584590911865, 'learning_rate': 3.8336242001163466e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2082, 'grad_norm': 6.999429225921631, 'learning_rate': 3.832897033158813e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2621, 'grad_norm': 9.00091552734375, 'learning_rate': 3.83216986620128e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1918, 'grad_norm': 7.453863143920898, 'learning_rate': 3.831442699243746e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2861, 'grad_norm': 7.5134735107421875, 'learning_rate': 3.8307155322862134e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1725, 'grad_norm': 7.5946364402771, 'learning_rate': 3.8299883653286794e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1137, 'grad_norm': 7.092611789703369, 'learning_rate': 3.829261198371146e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1043, 'grad_norm': 7.399266242980957, 'learning_rate': 3.828534031413613e-05, 'epoch': 0.7}\n",
      "{'loss': 3.1844, 'grad_norm': 7.835608005523682, 'learning_rate': 3.8278068644560795e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2443, 'grad_norm': 8.597434043884277, 'learning_rate': 3.8270796974985455e-05, 'epoch': 0.7}\n",
      "{'loss': 3.0055, 'grad_norm': 9.188178062438965, 'learning_rate': 3.826352530541012e-05, 'epoch': 0.7}\n",
      "{'loss': 3.2814, 'grad_norm': 7.440126419067383, 'learning_rate': 3.825625363583479e-05, 'epoch': 0.7}\n",
      "{'loss': 3.3012, 'grad_norm': 7.4003586769104, 'learning_rate': 3.8248981966259456e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2316, 'grad_norm': 7.897799968719482, 'learning_rate': 3.8241710296684116e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3057, 'grad_norm': 7.659396171569824, 'learning_rate': 3.823443862710879e-05, 'epoch': 0.71}\n",
      "{'loss': 3.215, 'grad_norm': 7.463318347930908, 'learning_rate': 3.822716695753345e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1793, 'grad_norm': 7.5254316329956055, 'learning_rate': 3.8219895287958116e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1771, 'grad_norm': 8.448454856872559, 'learning_rate': 3.821262361838278e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2389, 'grad_norm': 7.19519567489624, 'learning_rate': 3.820535194880745e-05, 'epoch': 0.71}\n",
      "{'loss': 3.4176, 'grad_norm': 9.456727027893066, 'learning_rate': 3.819808027923211e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2713, 'grad_norm': 7.864983081817627, 'learning_rate': 3.8190808609656784e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1994, 'grad_norm': 8.660552978515625, 'learning_rate': 3.8183536940081444e-05, 'epoch': 0.71}\n",
      "{'loss': 3.0949, 'grad_norm': 7.984115123748779, 'learning_rate': 3.817626527050611e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2672, 'grad_norm': 7.8321943283081055, 'learning_rate': 3.816899360093078e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1395, 'grad_norm': 8.140775680541992, 'learning_rate': 3.8161721931355445e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1248, 'grad_norm': 8.233062744140625, 'learning_rate': 3.8154450261780105e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2613, 'grad_norm': 8.582301139831543, 'learning_rate': 3.814717859220477e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1051, 'grad_norm': 8.228364944458008, 'learning_rate': 3.813990692262944e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2084, 'grad_norm': 7.671544551849365, 'learning_rate': 3.8132635253054106e-05, 'epoch': 0.71}\n",
      "{'loss': 3.3039, 'grad_norm': 7.287178039550781, 'learning_rate': 3.8125363583478766e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1211, 'grad_norm': 8.187590599060059, 'learning_rate': 3.811809191390344e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2127, 'grad_norm': 8.248649597167969, 'learning_rate': 3.81108202443281e-05, 'epoch': 0.71}\n",
      "{'loss': 3.315, 'grad_norm': 7.644204139709473, 'learning_rate': 3.8103548574752766e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1146, 'grad_norm': 7.6338934898376465, 'learning_rate': 3.809627690517743e-05, 'epoch': 0.71}\n",
      "{'loss': 3.2494, 'grad_norm': 7.844022750854492, 'learning_rate': 3.80890052356021e-05, 'epoch': 0.71}\n",
      "{'loss': 3.0434, 'grad_norm': 7.48177433013916, 'learning_rate': 3.808173356602676e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1723, 'grad_norm': 7.7594828605651855, 'learning_rate': 3.807446189645143e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2092, 'grad_norm': 8.038703918457031, 'learning_rate': 3.8067190226876094e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3404, 'grad_norm': 9.100799560546875, 'learning_rate': 3.8059918557300754e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2605, 'grad_norm': 8.148813247680664, 'learning_rate': 3.805264688772542e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1121, 'grad_norm': 7.422021865844727, 'learning_rate': 3.804537521815009e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1686, 'grad_norm': 7.532440662384033, 'learning_rate': 3.8038103548574755e-05, 'epoch': 0.72}\n",
      "{'loss': 3.0961, 'grad_norm': 8.517332077026367, 'learning_rate': 3.8030831878999415e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1393, 'grad_norm': 7.408286094665527, 'learning_rate': 3.802356020942409e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1193, 'grad_norm': 8.291288375854492, 'learning_rate': 3.801628853984875e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1604, 'grad_norm': 7.909812927246094, 'learning_rate': 3.8009016870273416e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1602, 'grad_norm': 7.37923002243042, 'learning_rate': 3.800174520069808e-05, 'epoch': 0.72}\n",
      " 24%|███████▉                         | 16500/68760 [3:05:12<8:47:55,  1.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.27s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.47s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.805338, 'eval_rouge-2': 8.290792, 'eval_rouge-l': 26.234166000000002, 'eval_bleu-4': 0.038159276362410004, 'eval_runtime': 17.6971, 'eval_samples_per_second': 2.825, 'eval_steps_per_second': 0.226, 'epoch': 0.72}\n",
      " 24%|███████▉                         | 16500/68760 [3:05:30<8:47:55,  1.65it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.31s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-16500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2969, 'grad_norm': 8.24245548248291, 'learning_rate': 3.799447353112275e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2, 'grad_norm': 7.173346519470215, 'learning_rate': 3.798720186154741e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1297, 'grad_norm': 8.580896377563477, 'learning_rate': 3.7979930191972076e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2408, 'grad_norm': 7.793720245361328, 'learning_rate': 3.797265852239674e-05, 'epoch': 0.72}\n",
      "{'loss': 3.0521, 'grad_norm': 8.21738338470459, 'learning_rate': 3.796538685282141e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1627, 'grad_norm': 7.3382673263549805, 'learning_rate': 3.795811518324607e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1629, 'grad_norm': 8.080378532409668, 'learning_rate': 3.7950843513670744e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2191, 'grad_norm': 7.576502799987793, 'learning_rate': 3.7943571844095404e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1289, 'grad_norm': 7.821706771850586, 'learning_rate': 3.793630017452007e-05, 'epoch': 0.72}\n",
      "{'loss': 3.3184, 'grad_norm': 7.52758264541626, 'learning_rate': 3.792902850494474e-05, 'epoch': 0.72}\n",
      "{'loss': 3.2203, 'grad_norm': 9.148877143859863, 'learning_rate': 3.7921756835369405e-05, 'epoch': 0.72}\n",
      "{'loss': 3.1477, 'grad_norm': 8.079216957092285, 'learning_rate': 3.7914485165794065e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2299, 'grad_norm': 7.777533054351807, 'learning_rate': 3.790721349621874e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2621, 'grad_norm': 7.005526542663574, 'learning_rate': 3.78999418266434e-05, 'epoch': 0.73}\n",
      "{'loss': 3.1785, 'grad_norm': 7.780543327331543, 'learning_rate': 3.7892670157068066e-05, 'epoch': 0.73}\n",
      "{'loss': 3.1873, 'grad_norm': 8.316387176513672, 'learning_rate': 3.788539848749273e-05, 'epoch': 0.73}\n",
      "{'loss': 3.1258, 'grad_norm': 8.422233581542969, 'learning_rate': 3.78781268179174e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2148, 'grad_norm': 7.437161922454834, 'learning_rate': 3.787085514834206e-05, 'epoch': 0.73}\n",
      "{'loss': 3.09, 'grad_norm': 7.942595481872559, 'learning_rate': 3.7863583478766726e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2818, 'grad_norm': 8.300901412963867, 'learning_rate': 3.785631180919139e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3824, 'grad_norm': 8.43984317779541, 'learning_rate': 3.784904013961606e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2146, 'grad_norm': 7.818460941314697, 'learning_rate': 3.784176847004072e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2611, 'grad_norm': 7.964815616607666, 'learning_rate': 3.7834496800465394e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2229, 'grad_norm': 7.9668097496032715, 'learning_rate': 3.7827225130890054e-05, 'epoch': 0.73}\n",
      "{'loss': 3.1744, 'grad_norm': 7.903846263885498, 'learning_rate': 3.781995346131472e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3469, 'grad_norm': 8.137239456176758, 'learning_rate': 3.781268179173939e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2289, 'grad_norm': 8.075382232666016, 'learning_rate': 3.7805410122164055e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2998, 'grad_norm': 8.560047149658203, 'learning_rate': 3.7798138452588715e-05, 'epoch': 0.73}\n",
      "{'loss': 3.3443, 'grad_norm': 7.936609268188477, 'learning_rate': 3.779086678301338e-05, 'epoch': 0.73}\n",
      "{'loss': 3.1303, 'grad_norm': 9.427699089050293, 'learning_rate': 3.778359511343805e-05, 'epoch': 0.73}\n",
      "{'loss': 3.1539, 'grad_norm': 9.851654052734375, 'learning_rate': 3.777632344386271e-05, 'epoch': 0.73}\n",
      "{'loss': 3.292, 'grad_norm': 9.106440544128418, 'learning_rate': 3.7769051774287376e-05, 'epoch': 0.73}\n",
      "{'loss': 3.1488, 'grad_norm': 8.588408470153809, 'learning_rate': 3.776178010471204e-05, 'epoch': 0.73}\n",
      "{'loss': 3.2271, 'grad_norm': 7.783158779144287, 'learning_rate': 3.775450843513671e-05, 'epoch': 0.73}\n",
      "{'loss': 3.1812, 'grad_norm': 8.015825271606445, 'learning_rate': 3.774723676556137e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1289, 'grad_norm': 7.747255802154541, 'learning_rate': 3.773996509598604e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1957, 'grad_norm': 7.90533971786499, 'learning_rate': 3.7732693426410703e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3107, 'grad_norm': 7.3593854904174805, 'learning_rate': 3.772542175683537e-05, 'epoch': 0.74}\n",
      "{'loss': 3.0371, 'grad_norm': 8.219429016113281, 'learning_rate': 3.771815008726004e-05, 'epoch': 0.74}\n",
      "{'loss': 3.3014, 'grad_norm': 7.96975564956665, 'learning_rate': 3.7710878417684704e-05, 'epoch': 0.74}\n",
      "{'loss': 3.0693, 'grad_norm': 7.753227710723877, 'learning_rate': 3.7703606748109364e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2229, 'grad_norm': 7.650589942932129, 'learning_rate': 3.769633507853403e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2391, 'grad_norm': 8.346650123596191, 'learning_rate': 3.76890634089587e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2803, 'grad_norm': 7.783866882324219, 'learning_rate': 3.7681791739383365e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1367, 'grad_norm': 8.106371879577637, 'learning_rate': 3.7674520069808025e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1756, 'grad_norm': 8.228544235229492, 'learning_rate': 3.76672484002327e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1402, 'grad_norm': 7.6517653465271, 'learning_rate': 3.765997673065736e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1664, 'grad_norm': 8.12145709991455, 'learning_rate': 3.7652705061082026e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1949, 'grad_norm': 7.575345993041992, 'learning_rate': 3.764543339150669e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2039, 'grad_norm': 8.261082649230957, 'learning_rate': 3.763816172193136e-05, 'epoch': 0.74}\n",
      " 25%|████████▏                        | 17000/68760 [3:10:41<9:20:27,  1.54it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.274264, 'eval_rouge-2': 8.201572, 'eval_rouge-l': 24.728644000000003, 'eval_bleu-4': 0.035308090841190666, 'eval_runtime': 29.2907, 'eval_samples_per_second': 1.707, 'eval_steps_per_second': 0.137, 'epoch': 0.74}\n",
      " 25%|████████▏                        | 17000/68760 [3:11:10<9:20:27,  1.54it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.68s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-17000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2232, 'grad_norm': 8.574373245239258, 'learning_rate': 3.763089005235602e-05, 'epoch': 0.74}\n",
      "{'loss': 3.058, 'grad_norm': 7.4820756912231445, 'learning_rate': 3.7623618382780687e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2268, 'grad_norm': 10.16461181640625, 'learning_rate': 3.7616346713205353e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1609, 'grad_norm': 8.968384742736816, 'learning_rate': 3.760907504363002e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1068, 'grad_norm': 8.438436508178711, 'learning_rate': 3.760180337405468e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1291, 'grad_norm': 8.139375686645508, 'learning_rate': 3.7594531704479354e-05, 'epoch': 0.74}\n",
      "{'loss': 3.0822, 'grad_norm': 7.612888336181641, 'learning_rate': 3.7587260034904014e-05, 'epoch': 0.74}\n",
      "{'loss': 3.2539, 'grad_norm': 11.78723430633545, 'learning_rate': 3.757998836532868e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2451, 'grad_norm': 8.260185241699219, 'learning_rate': 3.757271669575335e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1666, 'grad_norm': 7.726574420928955, 'learning_rate': 3.7565445026178015e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1635, 'grad_norm': 7.614596843719482, 'learning_rate': 3.7558173356602675e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1529, 'grad_norm': 8.935394287109375, 'learning_rate': 3.755090168702735e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1928, 'grad_norm': 6.788893699645996, 'learning_rate': 3.754363001745201e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1691, 'grad_norm': 7.489498138427734, 'learning_rate': 3.7536358347876676e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2963, 'grad_norm': 7.578457355499268, 'learning_rate': 3.752908667830134e-05, 'epoch': 0.75}\n",
      "{'loss': 3.0906, 'grad_norm': 9.18264102935791, 'learning_rate': 3.752181500872601e-05, 'epoch': 0.75}\n",
      "{'loss': 3.293, 'grad_norm': 7.95835542678833, 'learning_rate': 3.751454333915067e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2648, 'grad_norm': 7.800616264343262, 'learning_rate': 3.7507271669575337e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1971, 'grad_norm': 8.280191421508789, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1311, 'grad_norm': 8.441291809082031, 'learning_rate': 3.7492728330424664e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2381, 'grad_norm': 8.004523277282715, 'learning_rate': 3.748545666084933e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1139, 'grad_norm': 8.33804702758789, 'learning_rate': 3.7478184991274e-05, 'epoch': 0.75}\n",
      "{'loss': 3.0959, 'grad_norm': 8.815367698669434, 'learning_rate': 3.7470913321698664e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2125, 'grad_norm': 8.237060546875, 'learning_rate': 3.7463641652123324e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1928, 'grad_norm': 8.078837394714355, 'learning_rate': 3.7456369982548e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1898, 'grad_norm': 7.169955730438232, 'learning_rate': 3.744909831297266e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3631, 'grad_norm': 8.023877143859863, 'learning_rate': 3.7441826643397325e-05, 'epoch': 0.75}\n",
      "{'loss': 3.3496, 'grad_norm': 7.696108818054199, 'learning_rate': 3.743455497382199e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1582, 'grad_norm': 10.02125358581543, 'learning_rate': 3.742728330424666e-05, 'epoch': 0.75}\n",
      "{'loss': 3.2992, 'grad_norm': 7.942392349243164, 'learning_rate': 3.742001163467132e-05, 'epoch': 0.75}\n",
      "{'loss': 3.1477, 'grad_norm': 7.279101371765137, 'learning_rate': 3.7412739965095986e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1611, 'grad_norm': 8.503581047058105, 'learning_rate': 3.740546829552065e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1396, 'grad_norm': 8.441229820251465, 'learning_rate': 3.739819662594532e-05, 'epoch': 0.76}\n",
      "{'loss': 3.0805, 'grad_norm': 7.3796515464782715, 'learning_rate': 3.739092495636998e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1619, 'grad_norm': 7.303352355957031, 'learning_rate': 3.7383653286794653e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1961, 'grad_norm': 8.074376106262207, 'learning_rate': 3.7376381617219314e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1609, 'grad_norm': 8.051060676574707, 'learning_rate': 3.736910994764398e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2582, 'grad_norm': 8.393138885498047, 'learning_rate': 3.736183827806865e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1775, 'grad_norm': 7.912818431854248, 'learning_rate': 3.7354566608493314e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1391, 'grad_norm': 8.087870597839355, 'learning_rate': 3.7347294938917974e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1537, 'grad_norm': 8.946839332580566, 'learning_rate': 3.734002326934264e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2939, 'grad_norm': 8.623282432556152, 'learning_rate': 3.733275159976731e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2617, 'grad_norm': 7.52569055557251, 'learning_rate': 3.7325479930191975e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1041, 'grad_norm': 8.142179489135742, 'learning_rate': 3.7318208260616635e-05, 'epoch': 0.76}\n",
      "{'loss': 3.068, 'grad_norm': 7.819359302520752, 'learning_rate': 3.731093659104131e-05, 'epoch': 0.76}\n",
      "{'loss': 3.3346, 'grad_norm': 7.548928260803223, 'learning_rate': 3.730366492146597e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2578, 'grad_norm': 7.749958038330078, 'learning_rate': 3.7296393251890636e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1959, 'grad_norm': 7.148674488067627, 'learning_rate': 3.72891215823153e-05, 'epoch': 0.76}\n",
      "{'loss': 3.1789, 'grad_norm': 8.355072975158691, 'learning_rate': 3.728184991273997e-05, 'epoch': 0.76}\n",
      "{'loss': 3.0906, 'grad_norm': 7.129764556884766, 'learning_rate': 3.727457824316463e-05, 'epoch': 0.76}\n",
      " 25%|████████▍                        | 17500/68760 [3:16:25<8:36:22,  1.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.22s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.980364, 'eval_rouge-2': 8.58477, 'eval_rouge-l': 26.333288000000003, 'eval_bleu-4': 0.041550921868747384, 'eval_runtime': 16.6916, 'eval_samples_per_second': 2.996, 'eval_steps_per_second': 0.24, 'epoch': 0.76}\n",
      " 25%|████████▍                        | 17500/68760 [3:16:41<8:36:22,  1.65it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.03s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-17500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.207, 'grad_norm': 7.957918643951416, 'learning_rate': 3.7267306573589303e-05, 'epoch': 0.76}\n",
      "{'loss': 3.235, 'grad_norm': 8.444395065307617, 'learning_rate': 3.7260034904013964e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2342, 'grad_norm': 7.579690933227539, 'learning_rate': 3.725276323443863e-05, 'epoch': 0.76}\n",
      "{'loss': 3.2213, 'grad_norm': 7.492605686187744, 'learning_rate': 3.72454915648633e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1414, 'grad_norm': 8.185013771057129, 'learning_rate': 3.7238219895287964e-05, 'epoch': 0.77}\n",
      "{'loss': 3.201, 'grad_norm': 8.36108112335205, 'learning_rate': 3.7230948225712624e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1551, 'grad_norm': 7.445399761199951, 'learning_rate': 3.722367655613729e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2199, 'grad_norm': 8.399401664733887, 'learning_rate': 3.721640488656196e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1811, 'grad_norm': 7.6122145652771, 'learning_rate': 3.720913321698662e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2791, 'grad_norm': 9.34129810333252, 'learning_rate': 3.7201861547411285e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1107, 'grad_norm': 7.990421772003174, 'learning_rate': 3.719458987783595e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1961, 'grad_norm': 8.332259178161621, 'learning_rate': 3.718731820826062e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2252, 'grad_norm': 7.956151485443115, 'learning_rate': 3.718004653868528e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1348, 'grad_norm': 7.6673583984375, 'learning_rate': 3.717277486910995e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1305, 'grad_norm': 7.685539722442627, 'learning_rate': 3.716550319953461e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2564, 'grad_norm': 8.769251823425293, 'learning_rate': 3.715823152995928e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2861, 'grad_norm': 7.317356586456299, 'learning_rate': 3.715095986038395e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2445, 'grad_norm': 8.53887939453125, 'learning_rate': 3.7143688190808614e-05, 'epoch': 0.77}\n",
      "{'loss': 3.3064, 'grad_norm': 7.369511127471924, 'learning_rate': 3.7136416521233274e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1059, 'grad_norm': 7.740023136138916, 'learning_rate': 3.712914485165794e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1322, 'grad_norm': 7.3183465003967285, 'learning_rate': 3.712187318208261e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1822, 'grad_norm': 8.049505233764648, 'learning_rate': 3.7114601512507274e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1396, 'grad_norm': 7.821105003356934, 'learning_rate': 3.7107329842931935e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1703, 'grad_norm': 8.16277027130127, 'learning_rate': 3.710005817335661e-05, 'epoch': 0.77}\n",
      "{'loss': 3.2164, 'grad_norm': 6.9783148765563965, 'learning_rate': 3.709278650378127e-05, 'epoch': 0.77}\n",
      "{'loss': 3.166, 'grad_norm': 7.7059712409973145, 'learning_rate': 3.7085514834205935e-05, 'epoch': 0.77}\n",
      "{'loss': 3.1416, 'grad_norm': 9.07115650177002, 'learning_rate': 3.70782431646306e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1545, 'grad_norm': 7.64009428024292, 'learning_rate': 3.707097149505527e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2381, 'grad_norm': 7.5475921630859375, 'learning_rate': 3.706369982547993e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1121, 'grad_norm': 7.950916290283203, 'learning_rate': 3.7056428155904596e-05, 'epoch': 0.78}\n",
      "{'loss': 3.235, 'grad_norm': 7.911520957946777, 'learning_rate': 3.704915648632926e-05, 'epoch': 0.78}\n",
      "{'loss': 3.257, 'grad_norm': 8.539864540100098, 'learning_rate': 3.704188481675393e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2, 'grad_norm': 7.38919734954834, 'learning_rate': 3.703461314717859e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3168, 'grad_norm': 7.239993572235107, 'learning_rate': 3.7027341477603264e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2613, 'grad_norm': 8.302252769470215, 'learning_rate': 3.7020069808027924e-05, 'epoch': 0.78}\n",
      "{'loss': 3.0477, 'grad_norm': 8.768856048583984, 'learning_rate': 3.701279813845259e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2291, 'grad_norm': 7.8883490562438965, 'learning_rate': 3.700552646887726e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2736, 'grad_norm': 7.357547760009766, 'learning_rate': 3.6998254799301924e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1262, 'grad_norm': 8.11050033569336, 'learning_rate': 3.6990983129726585e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2738, 'grad_norm': 7.91113805770874, 'learning_rate': 3.698371146015125e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1748, 'grad_norm': 8.694140434265137, 'learning_rate': 3.697643979057592e-05, 'epoch': 0.78}\n",
      "{'loss': 3.0973, 'grad_norm': 6.989067077636719, 'learning_rate': 3.6969168121000585e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2865, 'grad_norm': 8.2330961227417, 'learning_rate': 3.6961896451425245e-05, 'epoch': 0.78}\n",
      "{'loss': 3.2174, 'grad_norm': 7.655693054199219, 'learning_rate': 3.695462478184992e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3604, 'grad_norm': 8.420570373535156, 'learning_rate': 3.694735311227458e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1695, 'grad_norm': 7.600851535797119, 'learning_rate': 3.6940081442699246e-05, 'epoch': 0.78}\n",
      "{'loss': 3.3393, 'grad_norm': 7.967371463775635, 'learning_rate': 3.693280977312391e-05, 'epoch': 0.78}\n",
      "{'loss': 3.0615, 'grad_norm': 7.085078716278076, 'learning_rate': 3.692553810354858e-05, 'epoch': 0.78}\n",
      "{'loss': 3.1701, 'grad_norm': 7.364314079284668, 'learning_rate': 3.691826643397324e-05, 'epoch': 0.78}\n",
      "{'loss': 3.0654, 'grad_norm': 8.60440444946289, 'learning_rate': 3.691099476439791e-05, 'epoch': 0.79}\n",
      " 26%|████████▋                        | 18000/68760 [3:21:50<9:17:43,  1.52it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.48s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.60s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.310188000000004, 'eval_rouge-2': 8.633842, 'eval_rouge-l': 26.01856, 'eval_bleu-4': 0.043443407701337665, 'eval_runtime': 18.429, 'eval_samples_per_second': 2.713, 'eval_steps_per_second': 0.217, 'epoch': 0.79}\n",
      " 26%|████████▋                        | 18000/68760 [3:22:08<9:17:43,  1.52it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.49s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-18000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2088, 'grad_norm': 8.22126579284668, 'learning_rate': 3.6903723094822574e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1301, 'grad_norm': 8.170112609863281, 'learning_rate': 3.6896451425247234e-05, 'epoch': 0.79}\n",
      "{'loss': 3.0766, 'grad_norm': 8.483725547790527, 'learning_rate': 3.688917975567191e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2539, 'grad_norm': 8.198351860046387, 'learning_rate': 3.688190808609657e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1086, 'grad_norm': 7.838245391845703, 'learning_rate': 3.6874636416521235e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2086, 'grad_norm': 8.027447700500488, 'learning_rate': 3.68673647469459e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2799, 'grad_norm': 7.661595344543457, 'learning_rate': 3.686009307737057e-05, 'epoch': 0.79}\n",
      "{'loss': 3.0686, 'grad_norm': 7.848235130310059, 'learning_rate': 3.685282140779523e-05, 'epoch': 0.79}\n",
      "{'loss': 3.0674, 'grad_norm': 7.954672813415527, 'learning_rate': 3.6845549738219895e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1678, 'grad_norm': 7.633327007293701, 'learning_rate': 3.683827806864456e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2117, 'grad_norm': 7.9346604347229, 'learning_rate': 3.683100639906923e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2092, 'grad_norm': 7.202475547790527, 'learning_rate': 3.682373472949389e-05, 'epoch': 0.79}\n",
      "{'loss': 3.041, 'grad_norm': 9.724050521850586, 'learning_rate': 3.681646305991856e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1535, 'grad_norm': 8.339437484741211, 'learning_rate': 3.680919139034322e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1377, 'grad_norm': 7.633415699005127, 'learning_rate': 3.680191972076789e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2748, 'grad_norm': 7.998966217041016, 'learning_rate': 3.679464805119256e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2123, 'grad_norm': 8.602802276611328, 'learning_rate': 3.6787376381617224e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1811, 'grad_norm': 8.49227237701416, 'learning_rate': 3.6780104712041884e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2373, 'grad_norm': 7.6269612312316895, 'learning_rate': 3.677283304246655e-05, 'epoch': 0.79}\n",
      "{'loss': 3.317, 'grad_norm': 7.89265251159668, 'learning_rate': 3.676556137289122e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1959, 'grad_norm': 9.16141128540039, 'learning_rate': 3.6758289703315885e-05, 'epoch': 0.79}\n",
      "{'loss': 3.1072, 'grad_norm': 8.544244766235352, 'learning_rate': 3.6751018033740545e-05, 'epoch': 0.79}\n",
      "{'loss': 3.2098, 'grad_norm': 7.913019180297852, 'learning_rate': 3.674374636416522e-05, 'epoch': 0.8}\n",
      "{'loss': 3.3057, 'grad_norm': 7.35593843460083, 'learning_rate': 3.673647469458988e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1352, 'grad_norm': 8.770835876464844, 'learning_rate': 3.6729203025014545e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1086, 'grad_norm': 7.339284420013428, 'learning_rate': 3.672193135543921e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2307, 'grad_norm': 9.299989700317383, 'learning_rate': 3.671465968586388e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1387, 'grad_norm': 7.220175266265869, 'learning_rate': 3.670738801628854e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1873, 'grad_norm': 8.506340980529785, 'learning_rate': 3.6700116346713206e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1014, 'grad_norm': 7.881710529327393, 'learning_rate': 3.669284467713787e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1945, 'grad_norm': 8.281984329223633, 'learning_rate': 3.668557300756254e-05, 'epoch': 0.8}\n",
      "{'loss': 3.0932, 'grad_norm': 8.969222068786621, 'learning_rate': 3.66783013379872e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1287, 'grad_norm': 8.924251556396484, 'learning_rate': 3.6671029668411874e-05, 'epoch': 0.8}\n",
      "{'loss': 3.0963, 'grad_norm': 7.727870464324951, 'learning_rate': 3.6663757998836534e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2139, 'grad_norm': 7.769956111907959, 'learning_rate': 3.66564863292612e-05, 'epoch': 0.8}\n",
      "{'loss': 3.0922, 'grad_norm': 7.490914821624756, 'learning_rate': 3.664921465968587e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1566, 'grad_norm': 7.708798408508301, 'learning_rate': 3.6641942990110535e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2434, 'grad_norm': 7.130505561828613, 'learning_rate': 3.6634671320535195e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1834, 'grad_norm': 7.794682025909424, 'learning_rate': 3.662739965095986e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2465, 'grad_norm': 7.666947364807129, 'learning_rate': 3.662012798138453e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2799, 'grad_norm': 7.974581718444824, 'learning_rate': 3.661285631180919e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2281, 'grad_norm': 7.067878246307373, 'learning_rate': 3.660558464223386e-05, 'epoch': 0.8}\n",
      "{'loss': 3.0691, 'grad_norm': 7.873664379119873, 'learning_rate': 3.659831297265852e-05, 'epoch': 0.8}\n",
      "{'loss': 3.174, 'grad_norm': 7.263832092285156, 'learning_rate': 3.659104130308319e-05, 'epoch': 0.8}\n",
      "{'loss': 3.198, 'grad_norm': 7.551748752593994, 'learning_rate': 3.6583769633507856e-05, 'epoch': 0.8}\n",
      "{'loss': 3.2072, 'grad_norm': 7.666117191314697, 'learning_rate': 3.657649796393252e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2852, 'grad_norm': 7.914773464202881, 'learning_rate': 3.656922629435718e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2152, 'grad_norm': 7.489670276641846, 'learning_rate': 3.656195462478185e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2268, 'grad_norm': 7.601496696472168, 'learning_rate': 3.655468295520652e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2803, 'grad_norm': 7.501827239990234, 'learning_rate': 3.6547411285631184e-05, 'epoch': 0.81}\n",
      " 27%|████████▉                        | 18500/68760 [3:27:15<8:06:01,  1.72it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.911210000000004, 'eval_rouge-2': 7.5127440000000005, 'eval_rouge-l': 24.050118, 'eval_bleu-4': 0.038263040942663015, 'eval_runtime': 37.7341, 'eval_samples_per_second': 1.325, 'eval_steps_per_second': 0.106, 'epoch': 0.81}\n",
      " 27%|████████▉                        | 18500/68760 [3:27:53<8:06:01,  1.72it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.81s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-18500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0496, 'grad_norm': 7.584646701812744, 'learning_rate': 3.6540139616055844e-05, 'epoch': 0.81}\n",
      "{'loss': 3.3104, 'grad_norm': 7.994850158691406, 'learning_rate': 3.653286794648052e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2627, 'grad_norm': 7.852884769439697, 'learning_rate': 3.652559627690518e-05, 'epoch': 0.81}\n",
      "{'loss': 3.1611, 'grad_norm': 9.312664985656738, 'learning_rate': 3.6518324607329845e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2398, 'grad_norm': 8.435432434082031, 'learning_rate': 3.651105293775451e-05, 'epoch': 0.81}\n",
      "{'loss': 3.1053, 'grad_norm': 7.550731182098389, 'learning_rate': 3.650378126817918e-05, 'epoch': 0.81}\n",
      "{'loss': 3.159, 'grad_norm': 9.466828346252441, 'learning_rate': 3.649650959860384e-05, 'epoch': 0.81}\n",
      "{'loss': 3.149, 'grad_norm': 8.137874603271484, 'learning_rate': 3.6489237929028505e-05, 'epoch': 0.81}\n",
      "{'loss': 3.1986, 'grad_norm': 7.293887138366699, 'learning_rate': 3.648196625945317e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2467, 'grad_norm': 8.015310287475586, 'learning_rate': 3.647469458987784e-05, 'epoch': 0.81}\n",
      "{'loss': 3.173, 'grad_norm': 7.527369976043701, 'learning_rate': 3.64674229203025e-05, 'epoch': 0.81}\n",
      "{'loss': 3.1008, 'grad_norm': 8.719428062438965, 'learning_rate': 3.646015125072717e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2389, 'grad_norm': 8.084489822387695, 'learning_rate': 3.645287958115183e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2033, 'grad_norm': 9.818685531616211, 'learning_rate': 3.64456079115765e-05, 'epoch': 0.81}\n",
      "{'loss': 3.0863, 'grad_norm': 7.813736915588379, 'learning_rate': 3.643833624200117e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2443, 'grad_norm': 7.886631965637207, 'learning_rate': 3.6431064572425834e-05, 'epoch': 0.81}\n",
      "{'loss': 3.2363, 'grad_norm': 7.957200527191162, 'learning_rate': 3.6423792902850494e-05, 'epoch': 0.81}\n",
      "{'loss': 3.184, 'grad_norm': 7.340740203857422, 'learning_rate': 3.641652123327516e-05, 'epoch': 0.82}\n",
      "{'loss': 3.1002, 'grad_norm': 8.163596153259277, 'learning_rate': 3.640924956369983e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2666, 'grad_norm': 7.5510454177856445, 'learning_rate': 3.6401977894124495e-05, 'epoch': 0.82}\n",
      "{'loss': 3.1664, 'grad_norm': 7.982402801513672, 'learning_rate': 3.6394706224549155e-05, 'epoch': 0.82}\n",
      "{'loss': 3.0787, 'grad_norm': 8.020520210266113, 'learning_rate': 3.638743455497383e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2834, 'grad_norm': 8.058084487915039, 'learning_rate': 3.638016288539849e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2275, 'grad_norm': 8.87125015258789, 'learning_rate': 3.6372891215823155e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2273, 'grad_norm': 8.483062744140625, 'learning_rate': 3.636561954624782e-05, 'epoch': 0.82}\n",
      "{'loss': 3.198, 'grad_norm': 7.5490946769714355, 'learning_rate': 3.635834787667249e-05, 'epoch': 0.82}\n",
      "{'loss': 3.1252, 'grad_norm': 7.617620944976807, 'learning_rate': 3.635107620709715e-05, 'epoch': 0.82}\n",
      "{'loss': 3.0293, 'grad_norm': 7.509067058563232, 'learning_rate': 3.6343804537521816e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2146, 'grad_norm': 7.301353931427002, 'learning_rate': 3.633653286794648e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2975, 'grad_norm': 7.964759349822998, 'learning_rate': 3.632926119837114e-05, 'epoch': 0.82}\n",
      "{'loss': 3.1367, 'grad_norm': 7.818360805511475, 'learning_rate': 3.632198952879581e-05, 'epoch': 0.82}\n",
      "{'loss': 3.165, 'grad_norm': 7.958576202392578, 'learning_rate': 3.631471785922048e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2059, 'grad_norm': 7.8660149574279785, 'learning_rate': 3.6307446189645144e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2375, 'grad_norm': 8.501354217529297, 'learning_rate': 3.6300174520069804e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2062, 'grad_norm': 7.6905317306518555, 'learning_rate': 3.629290285049448e-05, 'epoch': 0.82}\n",
      "{'loss': 3.1717, 'grad_norm': 7.800267696380615, 'learning_rate': 3.628563118091914e-05, 'epoch': 0.82}\n",
      "{'loss': 3.1223, 'grad_norm': 9.220399856567383, 'learning_rate': 3.6278359511343805e-05, 'epoch': 0.82}\n",
      "{'loss': 3.1314, 'grad_norm': 7.328200817108154, 'learning_rate': 3.627108784176847e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2125, 'grad_norm': 7.440313339233398, 'learning_rate': 3.626381617219314e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2812, 'grad_norm': 7.629741668701172, 'learning_rate': 3.62565445026178e-05, 'epoch': 0.82}\n",
      "{'loss': 3.2033, 'grad_norm': 8.41981315612793, 'learning_rate': 3.624927283304247e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2215, 'grad_norm': 7.367910385131836, 'learning_rate': 3.624200116346713e-05, 'epoch': 0.83}\n",
      "{'loss': 3.0908, 'grad_norm': 7.540830612182617, 'learning_rate': 3.62347294938918e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2098, 'grad_norm': 7.343442440032959, 'learning_rate': 3.6227457824316466e-05, 'epoch': 0.83}\n",
      "{'loss': 3.273, 'grad_norm': 7.833449363708496, 'learning_rate': 3.622018615474113e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2275, 'grad_norm': 7.711394309997559, 'learning_rate': 3.621291448516579e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2641, 'grad_norm': 9.090855598449707, 'learning_rate': 3.620564281559046e-05, 'epoch': 0.83}\n",
      "{'loss': 3.3715, 'grad_norm': 8.086509704589844, 'learning_rate': 3.619837114601513e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1566, 'grad_norm': 7.959792137145996, 'learning_rate': 3.6191099476439794e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1293, 'grad_norm': 9.089523315429688, 'learning_rate': 3.6183827806864454e-05, 'epoch': 0.83}\n",
      " 28%|█████████                        | 19000/68760 [3:33:01<8:41:59,  1.59it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.21s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.071948, 'eval_rouge-2': 7.289032, 'eval_rouge-l': 24.990958, 'eval_bleu-4': 0.035703195286306014, 'eval_runtime': 16.68, 'eval_samples_per_second': 2.998, 'eval_steps_per_second': 0.24, 'epoch': 0.83}\n",
      " 28%|█████████                        | 19000/68760 [3:33:17<8:41:59,  1.59it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.05s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-19000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1697, 'grad_norm': 8.282221794128418, 'learning_rate': 3.617655613728913e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1584, 'grad_norm': 8.166977882385254, 'learning_rate': 3.616928446771379e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2332, 'grad_norm': 7.479157447814941, 'learning_rate': 3.6162012798138455e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2201, 'grad_norm': 8.405609130859375, 'learning_rate': 3.615474112856312e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1328, 'grad_norm': 8.87483024597168, 'learning_rate': 3.614746945898779e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1336, 'grad_norm': 7.6911139488220215, 'learning_rate': 3.614019778941245e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1445, 'grad_norm': 7.624318599700928, 'learning_rate': 3.6132926119837116e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1572, 'grad_norm': 7.613248348236084, 'learning_rate': 3.612565445026178e-05, 'epoch': 0.83}\n",
      "{'loss': 2.9895, 'grad_norm': 8.346822738647461, 'learning_rate': 3.611838278068645e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2332, 'grad_norm': 9.000980377197266, 'learning_rate': 3.611111111111111e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2295, 'grad_norm': 7.710882663726807, 'learning_rate': 3.610383944153578e-05, 'epoch': 0.83}\n",
      "{'loss': 3.2785, 'grad_norm': 8.028982162475586, 'learning_rate': 3.609656777196044e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1777, 'grad_norm': 7.889006614685059, 'learning_rate': 3.608929610238511e-05, 'epoch': 0.83}\n",
      "{'loss': 3.1891, 'grad_norm': 8.302604675292969, 'learning_rate': 3.608202443280978e-05, 'epoch': 0.84}\n",
      "{'loss': 3.0666, 'grad_norm': 7.3063459396362305, 'learning_rate': 3.6074752763234444e-05, 'epoch': 0.84}\n",
      "{'loss': 3.0871, 'grad_norm': 8.361674308776855, 'learning_rate': 3.6067481093659104e-05, 'epoch': 0.84}\n",
      "{'loss': 3.1113, 'grad_norm': 7.667718410491943, 'learning_rate': 3.606020942408377e-05, 'epoch': 0.84}\n",
      "{'loss': 3.135, 'grad_norm': 8.105694770812988, 'learning_rate': 3.605293775450844e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2191, 'grad_norm': 8.876570701599121, 'learning_rate': 3.60456660849331e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2029, 'grad_norm': 8.259936332702637, 'learning_rate': 3.6038394415357765e-05, 'epoch': 0.84}\n",
      "{'loss': 3.1594, 'grad_norm': 8.380528450012207, 'learning_rate': 3.603112274578243e-05, 'epoch': 0.84}\n",
      "{'loss': 3.209, 'grad_norm': 7.314243316650391, 'learning_rate': 3.60238510762071e-05, 'epoch': 0.84}\n",
      "{'loss': 3.1574, 'grad_norm': 8.059270858764648, 'learning_rate': 3.601657940663176e-05, 'epoch': 0.84}\n",
      "{'loss': 3.1359, 'grad_norm': 8.71035385131836, 'learning_rate': 3.600930773705643e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2162, 'grad_norm': 8.34604549407959, 'learning_rate': 3.600203606748109e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2047, 'grad_norm': 9.457942008972168, 'learning_rate': 3.599476439790576e-05, 'epoch': 0.84}\n",
      "{'loss': 3.0369, 'grad_norm': 7.5670599937438965, 'learning_rate': 3.5987492728330426e-05, 'epoch': 0.84}\n",
      "{'loss': 3.077, 'grad_norm': 8.093460083007812, 'learning_rate': 3.598022105875509e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2209, 'grad_norm': 8.242731094360352, 'learning_rate': 3.597294938917975e-05, 'epoch': 0.84}\n",
      "{'loss': 3.3135, 'grad_norm': 8.540806770324707, 'learning_rate': 3.596567771960443e-05, 'epoch': 0.84}\n",
      "{'loss': 3.1469, 'grad_norm': 8.058858871459961, 'learning_rate': 3.595840605002909e-05, 'epoch': 0.84}\n",
      "{'loss': 3.0891, 'grad_norm': 8.42743968963623, 'learning_rate': 3.5951134380453754e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2293, 'grad_norm': 8.006609916687012, 'learning_rate': 3.594386271087842e-05, 'epoch': 0.84}\n",
      "{'loss': 3.0979, 'grad_norm': 7.8683671951293945, 'learning_rate': 3.593659104130309e-05, 'epoch': 0.84}\n",
      "{'loss': 3.0809, 'grad_norm': 7.941493034362793, 'learning_rate': 3.592931937172775e-05, 'epoch': 0.84}\n",
      "{'loss': 3.1184, 'grad_norm': 7.429275035858154, 'learning_rate': 3.5922047702152415e-05, 'epoch': 0.84}\n",
      "{'loss': 3.2941, 'grad_norm': 8.3751859664917, 'learning_rate': 3.591477603257708e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2273, 'grad_norm': 8.075620651245117, 'learning_rate': 3.590750436300175e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2166, 'grad_norm': 8.168731689453125, 'learning_rate': 3.590023269342641e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2523, 'grad_norm': 8.254298210144043, 'learning_rate': 3.589296102385108e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2826, 'grad_norm': 7.0140485763549805, 'learning_rate': 3.588568935427574e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2967, 'grad_norm': 7.573962211608887, 'learning_rate': 3.587841768470041e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1213, 'grad_norm': 8.03565502166748, 'learning_rate': 3.5871146015125076e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2344, 'grad_norm': 7.78928279876709, 'learning_rate': 3.586387434554974e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1598, 'grad_norm': 7.756230354309082, 'learning_rate': 3.58566026759744e-05, 'epoch': 0.85}\n",
      "{'loss': 3.0039, 'grad_norm': 7.554778575897217, 'learning_rate': 3.584933100639907e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1422, 'grad_norm': 7.7850422859191895, 'learning_rate': 3.584205933682374e-05, 'epoch': 0.85}\n",
      "{'loss': 3.3496, 'grad_norm': 8.894688606262207, 'learning_rate': 3.5834787667248404e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1896, 'grad_norm': 7.847911834716797, 'learning_rate': 3.5827515997673064e-05, 'epoch': 0.85}\n",
      "{'loss': 3.101, 'grad_norm': 7.90159797668457, 'learning_rate': 3.582024432809774e-05, 'epoch': 0.85}\n",
      " 28%|█████████▎                       | 19500/68760 [3:38:26<8:39:39,  1.58it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.07s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.39s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.308922, 'eval_rouge-2': 8.553482, 'eval_rouge-l': 26.37082, 'eval_bleu-4': 0.04317026268713307, 'eval_runtime': 7.37, 'eval_samples_per_second': 6.784, 'eval_steps_per_second': 0.543, 'epoch': 0.85}\n",
      " 28%|█████████▎                       | 19500/68760 [3:38:34<8:39:39,  1.58it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.39s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-19500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1633, 'grad_norm': 8.554267883300781, 'learning_rate': 3.58129726585224e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1477, 'grad_norm': 7.92738676071167, 'learning_rate': 3.5805700988947065e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1434, 'grad_norm': 9.571613311767578, 'learning_rate': 3.579842931937173e-05, 'epoch': 0.85}\n",
      "{'loss': 3.2535, 'grad_norm': 8.486750602722168, 'learning_rate': 3.57911576497964e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1758, 'grad_norm': 7.69506311416626, 'learning_rate': 3.578388598022106e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1826, 'grad_norm': 8.252668380737305, 'learning_rate': 3.5776614310645726e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1768, 'grad_norm': 8.153793334960938, 'learning_rate': 3.576934264107039e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1615, 'grad_norm': 7.849747657775879, 'learning_rate': 3.576207097149506e-05, 'epoch': 0.85}\n",
      "{'loss': 3.0984, 'grad_norm': 8.380945205688477, 'learning_rate': 3.575479930191972e-05, 'epoch': 0.85}\n",
      "{'loss': 3.249, 'grad_norm': 10.544405937194824, 'learning_rate': 3.5747527632344386e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1732, 'grad_norm': 7.602262020111084, 'learning_rate': 3.574025596276905e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1121, 'grad_norm': 7.706119060516357, 'learning_rate': 3.5732984293193713e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2555, 'grad_norm': 8.111997604370117, 'learning_rate': 3.572571262361839e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1182, 'grad_norm': 7.802649974822998, 'learning_rate': 3.571844095404305e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2084, 'grad_norm': 8.27004337310791, 'learning_rate': 3.5711169284467714e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1885, 'grad_norm': 7.888299942016602, 'learning_rate': 3.570389761489238e-05, 'epoch': 0.86}\n",
      "{'loss': 3.0055, 'grad_norm': 7.926074981689453, 'learning_rate': 3.569662594531705e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1496, 'grad_norm': 8.383296966552734, 'learning_rate': 3.568935427574171e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1895, 'grad_norm': 8.268495559692383, 'learning_rate': 3.5682082606166375e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2256, 'grad_norm': 7.420984268188477, 'learning_rate': 3.567481093659104e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2311, 'grad_norm': 8.044135093688965, 'learning_rate': 3.566753926701571e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1102, 'grad_norm': 7.069319248199463, 'learning_rate': 3.566026759744037e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2246, 'grad_norm': 9.039186477661133, 'learning_rate': 3.565299592786504e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1186, 'grad_norm': 9.021960258483887, 'learning_rate': 3.56457242582897e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1129, 'grad_norm': 7.350409984588623, 'learning_rate': 3.563845258871437e-05, 'epoch': 0.86}\n",
      "{'loss': 3.0938, 'grad_norm': 8.626399993896484, 'learning_rate': 3.5631180919139036e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2426, 'grad_norm': 8.347309112548828, 'learning_rate': 3.56239092495637e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2125, 'grad_norm': 8.309112548828125, 'learning_rate': 3.5616637579988363e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2439, 'grad_norm': 7.5899529457092285, 'learning_rate': 3.560936591041304e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1703, 'grad_norm': 8.146354675292969, 'learning_rate': 3.56020942408377e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1602, 'grad_norm': 8.327937126159668, 'learning_rate': 3.5594822571262364e-05, 'epoch': 0.86}\n",
      "{'loss': 3.2957, 'grad_norm': 7.026061058044434, 'learning_rate': 3.558755090168703e-05, 'epoch': 0.86}\n",
      "{'loss': 3.1256, 'grad_norm': 8.911483764648438, 'learning_rate': 3.55802792321117e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1535, 'grad_norm': 7.714608192443848, 'learning_rate': 3.557300756253636e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1492, 'grad_norm': 9.392428398132324, 'learning_rate': 3.5565735892961025e-05, 'epoch': 0.87}\n",
      "{'loss': 3.0645, 'grad_norm': 7.908726692199707, 'learning_rate': 3.555846422338569e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2496, 'grad_norm': 7.4540205001831055, 'learning_rate': 3.555119255381036e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2891, 'grad_norm': 8.252166748046875, 'learning_rate': 3.554392088423502e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2547, 'grad_norm': 7.168155670166016, 'learning_rate': 3.553664921465969e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1742, 'grad_norm': 8.612414360046387, 'learning_rate': 3.552937754508435e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1963, 'grad_norm': 8.947019577026367, 'learning_rate': 3.552210587550902e-05, 'epoch': 0.87}\n",
      "{'loss': 3.0945, 'grad_norm': 8.451444625854492, 'learning_rate': 3.5514834205933686e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1754, 'grad_norm': 9.727977752685547, 'learning_rate': 3.550756253635835e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1805, 'grad_norm': 8.550494194030762, 'learning_rate': 3.5500290866783013e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1645, 'grad_norm': 8.23597240447998, 'learning_rate': 3.549301919720768e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1889, 'grad_norm': 7.589023113250732, 'learning_rate': 3.548574752763235e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1584, 'grad_norm': 8.102681159973145, 'learning_rate': 3.5478475858057014e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1336, 'grad_norm': 7.81483268737793, 'learning_rate': 3.5471204188481674e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2258, 'grad_norm': 9.425023078918457, 'learning_rate': 3.546393251890634e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2873, 'grad_norm': 8.335698127746582, 'learning_rate': 3.545666084933101e-05, 'epoch': 0.87}\n",
      " 29%|█████████▌                       | 20000/68760 [3:43:45<8:05:00,  1.68it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.50s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.95062, 'eval_rouge-2': 7.3734920000000015, 'eval_rouge-l': 24.659710000000004, 'eval_bleu-4': 0.0363093799943305, 'eval_runtime': 45.5943, 'eval_samples_per_second': 1.097, 'eval_steps_per_second': 0.088, 'epoch': 0.87}\n",
      " 29%|█████████▌                       | 20000/68760 [3:44:31<8:05:00,  1.68it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:33<00:00,  8.74s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-20000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1418, 'grad_norm': 7.848031520843506, 'learning_rate': 3.544938917975567e-05, 'epoch': 0.87}\n",
      "{'loss': 3.2139, 'grad_norm': 7.779385566711426, 'learning_rate': 3.544211751018034e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1725, 'grad_norm': 7.880082130432129, 'learning_rate': 3.5434845840605e-05, 'epoch': 0.87}\n",
      "{'loss': 3.0975, 'grad_norm': 7.65709114074707, 'learning_rate': 3.542757417102967e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1949, 'grad_norm': 8.780661582946777, 'learning_rate': 3.5420302501454336e-05, 'epoch': 0.87}\n",
      "{'loss': 3.1143, 'grad_norm': 7.8983635902404785, 'learning_rate': 3.5413030831879e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2291, 'grad_norm': 8.797142028808594, 'learning_rate': 3.540575916230366e-05, 'epoch': 0.88}\n",
      "{'loss': 3.0934, 'grad_norm': 7.215309143066406, 'learning_rate': 3.539848749272833e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1422, 'grad_norm': 8.136617660522461, 'learning_rate': 3.5391215823152997e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2162, 'grad_norm': 8.713282585144043, 'learning_rate': 3.5383944153577663e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2135, 'grad_norm': 8.33165454864502, 'learning_rate': 3.5376672484002324e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1184, 'grad_norm': 7.805602073669434, 'learning_rate': 3.5369400814427e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1658, 'grad_norm': 7.417775630950928, 'learning_rate': 3.536212914485166e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1318, 'grad_norm': 7.176745891571045, 'learning_rate': 3.5354857475276324e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1684, 'grad_norm': 7.807040214538574, 'learning_rate': 3.534758580570099e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1143, 'grad_norm': 8.949751853942871, 'learning_rate': 3.534031413612566e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1938, 'grad_norm': 8.153655052185059, 'learning_rate': 3.533304246655032e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1764, 'grad_norm': 7.618350505828857, 'learning_rate': 3.532577079697499e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1904, 'grad_norm': 7.355208873748779, 'learning_rate': 3.531849912739965e-05, 'epoch': 0.88}\n",
      "{'loss': 3.043, 'grad_norm': 7.82636022567749, 'learning_rate': 3.531122745782432e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1527, 'grad_norm': 8.129244804382324, 'learning_rate': 3.5303955788248986e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2174, 'grad_norm': 8.398555755615234, 'learning_rate': 3.529668411867365e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1281, 'grad_norm': 8.268796920776367, 'learning_rate': 3.528941244909831e-05, 'epoch': 0.88}\n",
      "{'loss': 3.2555, 'grad_norm': 8.445995330810547, 'learning_rate': 3.528214077952298e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1252, 'grad_norm': 8.739958763122559, 'learning_rate': 3.5274869109947647e-05, 'epoch': 0.88}\n",
      "{'loss': 3.3059, 'grad_norm': 8.77741527557373, 'learning_rate': 3.5267597440372313e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1988, 'grad_norm': 7.490074157714844, 'learning_rate': 3.5260325770796974e-05, 'epoch': 0.88}\n",
      "{'loss': 3.1072, 'grad_norm': 8.157502174377441, 'learning_rate': 3.525305410122165e-05, 'epoch': 0.88}\n",
      "{'loss': 3.215, 'grad_norm': 6.91847562789917, 'learning_rate': 3.524578243164631e-05, 'epoch': 0.89}\n",
      "{'loss': 3.1719, 'grad_norm': 9.057304382324219, 'learning_rate': 3.5238510762070974e-05, 'epoch': 0.89}\n",
      "{'loss': 3.134, 'grad_norm': 8.107269287109375, 'learning_rate': 3.523123909249564e-05, 'epoch': 0.89}\n",
      "{'loss': 3.3219, 'grad_norm': 7.971311569213867, 'learning_rate': 3.522396742292031e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2855, 'grad_norm': 8.102521896362305, 'learning_rate': 3.521669575334497e-05, 'epoch': 0.89}\n",
      "{'loss': 3.0205, 'grad_norm': 7.847582817077637, 'learning_rate': 3.5209424083769635e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2863, 'grad_norm': 7.563335418701172, 'learning_rate': 3.52021524141943e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2646, 'grad_norm': 8.595367431640625, 'learning_rate': 3.519488074461897e-05, 'epoch': 0.89}\n",
      "{'loss': 3.0893, 'grad_norm': 9.360143661499023, 'learning_rate': 3.518760907504363e-05, 'epoch': 0.89}\n",
      "{'loss': 3.1236, 'grad_norm': 8.157240867614746, 'learning_rate': 3.51803374054683e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2195, 'grad_norm': 8.253447532653809, 'learning_rate': 3.517306573589296e-05, 'epoch': 0.89}\n",
      "{'loss': 3.1852, 'grad_norm': 8.819234848022461, 'learning_rate': 3.516579406631762e-05, 'epoch': 0.89}\n",
      "{'loss': 3.0727, 'grad_norm': 8.558152198791504, 'learning_rate': 3.5158522396742297e-05, 'epoch': 0.89}\n",
      "{'loss': 3.1664, 'grad_norm': 7.627379417419434, 'learning_rate': 3.515125072716696e-05, 'epoch': 0.89}\n",
      "{'loss': 3.1682, 'grad_norm': 8.698495864868164, 'learning_rate': 3.5143979057591624e-05, 'epoch': 0.89}\n",
      "{'loss': 3.232, 'grad_norm': 7.07728910446167, 'learning_rate': 3.513670738801629e-05, 'epoch': 0.89}\n",
      "{'loss': 3.0771, 'grad_norm': 7.380282878875732, 'learning_rate': 3.512943571844096e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2158, 'grad_norm': 7.785438537597656, 'learning_rate': 3.512216404886562e-05, 'epoch': 0.89}\n",
      "{'loss': 3.0994, 'grad_norm': 7.696807861328125, 'learning_rate': 3.5114892379290284e-05, 'epoch': 0.89}\n",
      "{'loss': 3.1801, 'grad_norm': 7.705305099487305, 'learning_rate': 3.510762070971495e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2299, 'grad_norm': 8.681039810180664, 'learning_rate': 3.510034904013962e-05, 'epoch': 0.89}\n",
      "{'loss': 3.208, 'grad_norm': 8.209063529968262, 'learning_rate': 3.509307737056428e-05, 'epoch': 0.89}\n",
      " 30%|█████████▊                       | 20500/68760 [3:49:42<7:43:43,  1.73it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.19s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.500834, 'eval_rouge-2': 8.220258, 'eval_rouge-l': 25.851322, 'eval_bleu-4': 0.03935087787777184, 'eval_runtime': 26.9719, 'eval_samples_per_second': 1.854, 'eval_steps_per_second': 0.148, 'epoch': 0.89}\n",
      " 30%|█████████▊                       | 20500/68760 [3:50:09<7:43:43,  1.73it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  2.96s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-20500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1801, 'grad_norm': 9.151729583740234, 'learning_rate': 3.508580570098895e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2027, 'grad_norm': 7.6109700202941895, 'learning_rate': 3.507853403141361e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1357, 'grad_norm': 8.611576080322266, 'learning_rate': 3.507126236183828e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1213, 'grad_norm': 7.755867958068848, 'learning_rate': 3.5063990692262946e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1748, 'grad_norm': 7.699131011962891, 'learning_rate': 3.505671902268761e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2516, 'grad_norm': 7.930370807647705, 'learning_rate': 3.504944735311227e-05, 'epoch': 0.9}\n",
      "{'loss': 3.0504, 'grad_norm': 8.410786628723145, 'learning_rate': 3.504217568353694e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2639, 'grad_norm': 7.45721960067749, 'learning_rate': 3.503490401396161e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2504, 'grad_norm': 7.736198425292969, 'learning_rate': 3.5027632344386274e-05, 'epoch': 0.9}\n",
      "{'loss': 3.184, 'grad_norm': 7.939810276031494, 'learning_rate': 3.5020360674810934e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1953, 'grad_norm': 7.582747459411621, 'learning_rate': 3.501308900523561e-05, 'epoch': 0.9}\n",
      "{'loss': 3.0879, 'grad_norm': 8.034330368041992, 'learning_rate': 3.500581733566027e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2314, 'grad_norm': 7.877023696899414, 'learning_rate': 3.4998545666084934e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1232, 'grad_norm': 7.574649333953857, 'learning_rate': 3.49912739965096e-05, 'epoch': 0.9}\n",
      "{'loss': 3.235, 'grad_norm': 8.60181713104248, 'learning_rate': 3.498400232693427e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2, 'grad_norm': 6.953157901763916, 'learning_rate': 3.497673065735893e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2727, 'grad_norm': 7.572429180145264, 'learning_rate': 3.49694589877836e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1307, 'grad_norm': 8.218144416809082, 'learning_rate': 3.496218731820826e-05, 'epoch': 0.9}\n",
      "{'loss': 3.235, 'grad_norm': 8.832873344421387, 'learning_rate': 3.495491564863293e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2238, 'grad_norm': 7.92448091506958, 'learning_rate': 3.4947643979057596e-05, 'epoch': 0.9}\n",
      "{'loss': 3.0873, 'grad_norm': 8.200698852539062, 'learning_rate': 3.494037230948226e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2404, 'grad_norm': 8.324231147766113, 'learning_rate': 3.493310063990692e-05, 'epoch': 0.9}\n",
      "{'loss': 3.2207, 'grad_norm': 8.435928344726562, 'learning_rate': 3.492582897033159e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1736, 'grad_norm': 8.522907257080078, 'learning_rate': 3.491855730075626e-05, 'epoch': 0.9}\n",
      "{'loss': 3.1764, 'grad_norm': 9.592264175415039, 'learning_rate': 3.4911285631180924e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2537, 'grad_norm': 8.329866409301758, 'learning_rate': 3.4904013961605584e-05, 'epoch': 0.91}\n",
      "{'loss': 3.018, 'grad_norm': 9.683013916015625, 'learning_rate': 3.489674229203026e-05, 'epoch': 0.91}\n",
      "{'loss': 3.166, 'grad_norm': 7.880192756652832, 'learning_rate': 3.488947062245492e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1598, 'grad_norm': 10.870777130126953, 'learning_rate': 3.488219895287958e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1283, 'grad_norm': 7.800489902496338, 'learning_rate': 3.487492728330425e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1568, 'grad_norm': 8.12778091430664, 'learning_rate': 3.486765561372891e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1551, 'grad_norm': 8.248330116271973, 'learning_rate': 3.486038394415358e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2596, 'grad_norm': 8.775602340698242, 'learning_rate': 3.4853112274578245e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1699, 'grad_norm': 8.362260818481445, 'learning_rate': 3.484584060500291e-05, 'epoch': 0.91}\n",
      "{'loss': 3.0695, 'grad_norm': 7.6557183265686035, 'learning_rate': 3.483856893542757e-05, 'epoch': 0.91}\n",
      "{'loss': 3.0898, 'grad_norm': 8.163955688476562, 'learning_rate': 3.483129726585224e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1842, 'grad_norm': 7.459826469421387, 'learning_rate': 3.4824025596276906e-05, 'epoch': 0.91}\n",
      "{'loss': 3.134, 'grad_norm': 7.829040050506592, 'learning_rate': 3.481675392670157e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1824, 'grad_norm': 7.201362609863281, 'learning_rate': 3.480948225712623e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2422, 'grad_norm': 8.036663055419922, 'learning_rate': 3.480221058755091e-05, 'epoch': 0.91}\n",
      "{'loss': 3.199, 'grad_norm': 8.104511260986328, 'learning_rate': 3.479493891797557e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2625, 'grad_norm': 7.793060779571533, 'learning_rate': 3.4787667248400234e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2988, 'grad_norm': 7.7498321533203125, 'learning_rate': 3.47803955788249e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2219, 'grad_norm': 7.6363677978515625, 'learning_rate': 3.477312390924957e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2625, 'grad_norm': 8.106925964355469, 'learning_rate': 3.476585223967423e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1059, 'grad_norm': 9.192526817321777, 'learning_rate': 3.4758580570098894e-05, 'epoch': 0.91}\n",
      "{'loss': 3.2359, 'grad_norm': 7.589532852172852, 'learning_rate': 3.475130890052356e-05, 'epoch': 0.91}\n",
      "{'loss': 3.1109, 'grad_norm': 8.844095230102539, 'learning_rate': 3.474403723094823e-05, 'epoch': 0.92}\n",
      "{'loss': 3.123, 'grad_norm': 9.075190544128418, 'learning_rate': 3.473676556137289e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2094, 'grad_norm': 10.123273849487305, 'learning_rate': 3.472949389179756e-05, 'epoch': 0.92}\n",
      " 31%|██████████                       | 21000/68760 [3:55:16<7:50:22,  1.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.45s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.52s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.668811999999996, 'eval_rouge-2': 8.139528, 'eval_rouge-l': 26.016562, 'eval_bleu-4': 0.039352252919757066, 'eval_runtime': 26.0446, 'eval_samples_per_second': 1.92, 'eval_steps_per_second': 0.154, 'epoch': 0.92}\n",
      " 31%|██████████                       | 21000/68760 [3:55:42<7:50:22,  1.69it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:13<00:00,  4.36s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-21000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2932, 'grad_norm': 8.65312671661377, 'learning_rate': 3.472222222222222e-05, 'epoch': 0.92}\n",
      "{'loss': 3.05, 'grad_norm': 7.911133766174316, 'learning_rate': 3.471495055264689e-05, 'epoch': 0.92}\n",
      "{'loss': 3.1998, 'grad_norm': 8.669595718383789, 'learning_rate': 3.4707678883071556e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2324, 'grad_norm': 8.747299194335938, 'learning_rate': 3.470040721349622e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2021, 'grad_norm': 8.25151538848877, 'learning_rate': 3.469313554392088e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2381, 'grad_norm': 8.086381912231445, 'learning_rate': 3.468586387434556e-05, 'epoch': 0.92}\n",
      "{'loss': 3.21, 'grad_norm': 8.6745023727417, 'learning_rate': 3.467859220477022e-05, 'epoch': 0.92}\n",
      "{'loss': 3.1607, 'grad_norm': 7.935971736907959, 'learning_rate': 3.4671320535194884e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2232, 'grad_norm': 7.989405632019043, 'learning_rate': 3.466404886561955e-05, 'epoch': 0.92}\n",
      "{'loss': 3.1346, 'grad_norm': 8.60010814666748, 'learning_rate': 3.465677719604422e-05, 'epoch': 0.92}\n",
      "{'loss': 3.1809, 'grad_norm': 7.805055618286133, 'learning_rate': 3.464950552646888e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2172, 'grad_norm': 7.579054355621338, 'learning_rate': 3.4642233856893544e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2275, 'grad_norm': 8.368556022644043, 'learning_rate': 3.463496218731821e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2541, 'grad_norm': 8.62263011932373, 'learning_rate': 3.462769051774288e-05, 'epoch': 0.92}\n",
      "{'loss': 3.0949, 'grad_norm': 8.243186950683594, 'learning_rate': 3.462041884816754e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2213, 'grad_norm': 7.571781635284424, 'learning_rate': 3.461314717859221e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2684, 'grad_norm': 8.106621742248535, 'learning_rate': 3.460587550901687e-05, 'epoch': 0.92}\n",
      "{'loss': 3.1994, 'grad_norm': 8.734134674072266, 'learning_rate': 3.459860383944154e-05, 'epoch': 0.92}\n",
      "{'loss': 3.1928, 'grad_norm': 7.914866924285889, 'learning_rate': 3.4591332169866206e-05, 'epoch': 0.92}\n",
      "{'loss': 3.2908, 'grad_norm': 9.05734920501709, 'learning_rate': 3.4584060500290866e-05, 'epoch': 0.92}\n",
      "{'loss': 3.1096, 'grad_norm': 7.987945079803467, 'learning_rate': 3.457678883071553e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2625, 'grad_norm': 8.252766609191895, 'learning_rate': 3.45695171611402e-05, 'epoch': 0.93}\n",
      "{'loss': 3.0068, 'grad_norm': 7.929773807525635, 'learning_rate': 3.456224549156487e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2271, 'grad_norm': 8.368165016174316, 'learning_rate': 3.455497382198953e-05, 'epoch': 0.93}\n",
      "{'loss': 3.108, 'grad_norm': 7.659419059753418, 'learning_rate': 3.4547702152414194e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2217, 'grad_norm': 7.295581340789795, 'learning_rate': 3.454043048283886e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2166, 'grad_norm': 9.054766654968262, 'learning_rate': 3.453315881326353e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2326, 'grad_norm': 9.743927001953125, 'learning_rate': 3.452588714368819e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1436, 'grad_norm': 8.574334144592285, 'learning_rate': 3.451861547411286e-05, 'epoch': 0.93}\n",
      "{'loss': 3.209, 'grad_norm': 7.93471622467041, 'learning_rate': 3.451134380453752e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2314, 'grad_norm': 8.212178230285645, 'learning_rate': 3.450407213496219e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2207, 'grad_norm': 7.922145843505859, 'learning_rate': 3.4496800465386855e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1646, 'grad_norm': 8.141033172607422, 'learning_rate': 3.448952879581152e-05, 'epoch': 0.93}\n",
      "{'loss': 3.0625, 'grad_norm': 7.536037921905518, 'learning_rate': 3.448225712623618e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1961, 'grad_norm': 7.912125110626221, 'learning_rate': 3.447498545666085e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1787, 'grad_norm': 8.123677253723145, 'learning_rate': 3.4467713787085516e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1428, 'grad_norm': 8.084373474121094, 'learning_rate': 3.446044211751018e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1518, 'grad_norm': 9.323054313659668, 'learning_rate': 3.445317044793484e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1238, 'grad_norm': 9.50831413269043, 'learning_rate': 3.444589877835952e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2387, 'grad_norm': 9.25540542602539, 'learning_rate': 3.443862710878418e-05, 'epoch': 0.93}\n",
      "{'loss': 3.185, 'grad_norm': 8.012911796569824, 'learning_rate': 3.4431355439208844e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1209, 'grad_norm': 9.209076881408691, 'learning_rate': 3.442408376963351e-05, 'epoch': 0.93}\n",
      "{'loss': 3.1322, 'grad_norm': 7.761633396148682, 'learning_rate': 3.441681210005818e-05, 'epoch': 0.93}\n",
      "{'loss': 3.2191, 'grad_norm': 8.022964477539062, 'learning_rate': 3.440954043048284e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2316, 'grad_norm': 9.318553924560547, 'learning_rate': 3.4402268760907505e-05, 'epoch': 0.94}\n",
      "{'loss': 2.9807, 'grad_norm': 7.467376708984375, 'learning_rate': 3.439499709133217e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1631, 'grad_norm': 8.97742748260498, 'learning_rate': 3.438772542175684e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2012, 'grad_norm': 8.16872787475586, 'learning_rate': 3.43804537521815e-05, 'epoch': 0.94}\n",
      "{'loss': 3.0426, 'grad_norm': 7.697535514831543, 'learning_rate': 3.437318208260617e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1883, 'grad_norm': 7.173849105834961, 'learning_rate': 3.436591041303083e-05, 'epoch': 0.94}\n",
      " 31%|██████████▎                      | 21500/68760 [4:00:57<7:46:11,  1.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.11s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.24s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.097536, 'eval_rouge-2': 8.374087999999999, 'eval_rouge-l': 26.512077999999995, 'eval_bleu-4': 0.03997587634632212, 'eval_runtime': 6.8865, 'eval_samples_per_second': 7.261, 'eval_steps_per_second': 0.581, 'epoch': 0.94}\n",
      " 31%|██████████▎                      | 21500/68760 [4:01:04<7:46:11,  1.69it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.31s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-21500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1006, 'grad_norm': 8.075399398803711, 'learning_rate': 3.43586387434555e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2912, 'grad_norm': 9.168302536010742, 'learning_rate': 3.4351367073880166e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1643, 'grad_norm': 8.152470588684082, 'learning_rate': 3.434409540430483e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1707, 'grad_norm': 8.029114723205566, 'learning_rate': 3.433682373472949e-05, 'epoch': 0.94}\n",
      "{'loss': 3.192, 'grad_norm': 8.259613990783691, 'learning_rate': 3.432955206515417e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2557, 'grad_norm': 8.274344444274902, 'learning_rate': 3.432228039557883e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1246, 'grad_norm': 7.353099346160889, 'learning_rate': 3.4315008726003494e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1299, 'grad_norm': 8.143872261047363, 'learning_rate': 3.430773705642816e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1674, 'grad_norm': 8.70960521697998, 'learning_rate': 3.430046538685282e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2545, 'grad_norm': 8.255666732788086, 'learning_rate': 3.429319371727749e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1758, 'grad_norm': 9.156024932861328, 'learning_rate': 3.4285922047702155e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2037, 'grad_norm': 7.804362773895264, 'learning_rate': 3.427865037812682e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1186, 'grad_norm': 7.794000148773193, 'learning_rate': 3.427137870855148e-05, 'epoch': 0.94}\n",
      "{'loss': 3.2365, 'grad_norm': 8.149904251098633, 'learning_rate': 3.426410703897615e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1307, 'grad_norm': 8.163631439208984, 'learning_rate': 3.4256835369400815e-05, 'epoch': 0.94}\n",
      "{'loss': 3.0541, 'grad_norm': 8.565652847290039, 'learning_rate': 3.424956369982548e-05, 'epoch': 0.95}\n",
      "{'loss': 3.21, 'grad_norm': 8.009276390075684, 'learning_rate': 3.424229203025014e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1414, 'grad_norm': 8.302834510803223, 'learning_rate': 3.4235020360674816e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1336, 'grad_norm': 8.60329818725586, 'learning_rate': 3.4227748691099476e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2053, 'grad_norm': 7.963031768798828, 'learning_rate': 3.422047702152414e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1197, 'grad_norm': 8.72155475616455, 'learning_rate': 3.421320535194881e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1893, 'grad_norm': 8.22937297821045, 'learning_rate': 3.420593368237348e-05, 'epoch': 0.95}\n",
      "{'loss': 3.192, 'grad_norm': 7.838771343231201, 'learning_rate': 3.419866201279814e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1693, 'grad_norm': 8.329265594482422, 'learning_rate': 3.4191390343222804e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1057, 'grad_norm': 8.38943099975586, 'learning_rate': 3.418411867364747e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1752, 'grad_norm': 8.712008476257324, 'learning_rate': 3.417684700407214e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1729, 'grad_norm': 8.199822425842285, 'learning_rate': 3.41695753344968e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1455, 'grad_norm': 8.027030944824219, 'learning_rate': 3.416230366492147e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1912, 'grad_norm': 9.409387588500977, 'learning_rate': 3.415503199534613e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1781, 'grad_norm': 8.499025344848633, 'learning_rate': 3.41477603257708e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2871, 'grad_norm': 8.34208869934082, 'learning_rate': 3.4140488656195465e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1424, 'grad_norm': 9.0363130569458, 'learning_rate': 3.413321698662013e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1242, 'grad_norm': 8.107314109802246, 'learning_rate': 3.412594531704479e-05, 'epoch': 0.95}\n",
      "{'loss': 3.0344, 'grad_norm': 8.115802764892578, 'learning_rate': 3.411867364746946e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1318, 'grad_norm': 7.757062911987305, 'learning_rate': 3.4111401977894126e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1215, 'grad_norm': 8.297857284545898, 'learning_rate': 3.410413030831879e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1338, 'grad_norm': 8.866601943969727, 'learning_rate': 3.409685863874345e-05, 'epoch': 0.95}\n",
      "{'loss': 3.1777, 'grad_norm': 8.45588207244873, 'learning_rate': 3.408958696916813e-05, 'epoch': 0.95}\n",
      "{'loss': 2.9996, 'grad_norm': 8.644110679626465, 'learning_rate': 3.408231529959279e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2029, 'grad_norm': 8.527170181274414, 'learning_rate': 3.4075043630017454e-05, 'epoch': 0.96}\n",
      "{'loss': 3.1906, 'grad_norm': 8.376497268676758, 'learning_rate': 3.406777196044212e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2955, 'grad_norm': 8.516270637512207, 'learning_rate': 3.406050029086679e-05, 'epoch': 0.96}\n",
      "{'loss': 3.24, 'grad_norm': 8.043964385986328, 'learning_rate': 3.405322862129145e-05, 'epoch': 0.96}\n",
      "{'loss': 3.0971, 'grad_norm': 8.813180923461914, 'learning_rate': 3.4045956951716115e-05, 'epoch': 0.96}\n",
      "{'loss': 3.1527, 'grad_norm': 8.417962074279785, 'learning_rate': 3.403868528214078e-05, 'epoch': 0.96}\n",
      "{'loss': 3.1871, 'grad_norm': 7.690629959106445, 'learning_rate': 3.403141361256545e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2814, 'grad_norm': 8.540044784545898, 'learning_rate': 3.4024141942990115e-05, 'epoch': 0.96}\n",
      "{'loss': 3.0785, 'grad_norm': 8.087660789489746, 'learning_rate': 3.401687027341478e-05, 'epoch': 0.96}\n",
      "{'loss': 3.224, 'grad_norm': 7.4806365966796875, 'learning_rate': 3.400959860383944e-05, 'epoch': 0.96}\n",
      "{'loss': 3.1482, 'grad_norm': 8.535040855407715, 'learning_rate': 3.400232693426411e-05, 'epoch': 0.96}\n",
      " 32%|██████████▌                      | 22000/68760 [4:06:12<7:31:37,  1.73it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.31s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.193194, 'eval_rouge-2': 7.55746, 'eval_rouge-l': 25.744866, 'eval_bleu-4': 0.03715842845504252, 'eval_runtime': 17.3788, 'eval_samples_per_second': 2.877, 'eval_steps_per_second': 0.23, 'epoch': 0.96}\n",
      " 32%|██████████▌                      | 22000/68760 [4:06:29<7:31:37,  1.73it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.23s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-22000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0287, 'grad_norm': 7.9678053855896, 'learning_rate': 3.3995055264688776e-05, 'epoch': 0.96}\n",
      "{'loss': 3.1906, 'grad_norm': 8.492347717285156, 'learning_rate': 3.3987783595113436e-05, 'epoch': 0.96}\n",
      "{'loss': 3.399, 'grad_norm': 7.154509544372559, 'learning_rate': 3.39805119255381e-05, 'epoch': 0.96}\n",
      "{'loss': 3.083, 'grad_norm': 8.890124320983887, 'learning_rate': 3.397324025596277e-05, 'epoch': 0.96}\n",
      "{'loss': 3.3158, 'grad_norm': 8.480722427368164, 'learning_rate': 3.396596858638744e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2086, 'grad_norm': 8.072121620178223, 'learning_rate': 3.39586969168121e-05, 'epoch': 0.96}\n",
      "{'loss': 3.0688, 'grad_norm': 8.021001815795898, 'learning_rate': 3.395142524723677e-05, 'epoch': 0.96}\n",
      "{'loss': 3.1316, 'grad_norm': 10.23935317993164, 'learning_rate': 3.394415357766143e-05, 'epoch': 0.96}\n",
      "{'loss': 3.2037, 'grad_norm': 9.022881507873535, 'learning_rate': 3.39368819080861e-05, 'epoch': 0.96}\n",
      "{'loss': 3.1602, 'grad_norm': 7.465064525604248, 'learning_rate': 3.3929610238510765e-05, 'epoch': 0.96}\n",
      "{'loss': 3.0846, 'grad_norm': 8.253094673156738, 'learning_rate': 3.392233856893543e-05, 'epoch': 0.96}\n",
      "{'loss': 3.292, 'grad_norm': 7.539690017700195, 'learning_rate': 3.391506689936009e-05, 'epoch': 0.97}\n",
      "{'loss': 3.0891, 'grad_norm': 8.883265495300293, 'learning_rate': 3.390779522978476e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1773, 'grad_norm': 7.97177791595459, 'learning_rate': 3.3900523560209426e-05, 'epoch': 0.97}\n",
      "{'loss': 3.0113, 'grad_norm': 7.4119391441345215, 'learning_rate': 3.389325189063409e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1115, 'grad_norm': 7.642562389373779, 'learning_rate': 3.388598022105875e-05, 'epoch': 0.97}\n",
      "{'loss': 3.3174, 'grad_norm': 8.623750686645508, 'learning_rate': 3.3878708551483426e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1627, 'grad_norm': 8.278361320495605, 'learning_rate': 3.3871436881908086e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2525, 'grad_norm': 7.407865524291992, 'learning_rate': 3.386416521233275e-05, 'epoch': 0.97}\n",
      "{'loss': 3.0803, 'grad_norm': 8.378334999084473, 'learning_rate': 3.385689354275742e-05, 'epoch': 0.97}\n",
      "{'loss': 3.059, 'grad_norm': 10.342803001403809, 'learning_rate': 3.384962187318209e-05, 'epoch': 0.97}\n",
      "{'loss': 3.3221, 'grad_norm': 9.042823791503906, 'learning_rate': 3.384235020360675e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1051, 'grad_norm': 9.147101402282715, 'learning_rate': 3.3835078534031414e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1195, 'grad_norm': 7.871696472167969, 'learning_rate': 3.382780686445608e-05, 'epoch': 0.97}\n",
      "{'loss': 3.0664, 'grad_norm': 8.825121879577637, 'learning_rate': 3.382053519488075e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1658, 'grad_norm': 7.650626182556152, 'learning_rate': 3.381326352530541e-05, 'epoch': 0.97}\n",
      "{'loss': 3.0982, 'grad_norm': 7.625146865844727, 'learning_rate': 3.380599185573008e-05, 'epoch': 0.97}\n",
      "{'loss': 2.991, 'grad_norm': 9.097765922546387, 'learning_rate': 3.379872018615474e-05, 'epoch': 0.97}\n",
      "{'loss': 3.2324, 'grad_norm': 8.771408081054688, 'learning_rate': 3.379144851657941e-05, 'epoch': 0.97}\n",
      "{'loss': 3.232, 'grad_norm': 7.671408653259277, 'learning_rate': 3.3784176847004076e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1311, 'grad_norm': 8.251749038696289, 'learning_rate': 3.377690517742874e-05, 'epoch': 0.97}\n",
      "{'loss': 3.223, 'grad_norm': 7.9246625900268555, 'learning_rate': 3.37696335078534e-05, 'epoch': 0.97}\n",
      "{'loss': 3.1324, 'grad_norm': 9.036917686462402, 'learning_rate': 3.376236183827807e-05, 'epoch': 0.97}\n",
      "{'loss': 3.0611, 'grad_norm': 8.279048919677734, 'learning_rate': 3.3755090168702736e-05, 'epoch': 0.97}\n",
      "{'loss': 3.192, 'grad_norm': 8.071805000305176, 'learning_rate': 3.37478184991274e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1002, 'grad_norm': 7.628997325897217, 'learning_rate': 3.374054682955206e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1938, 'grad_norm': 9.678210258483887, 'learning_rate': 3.373327515997674e-05, 'epoch': 0.98}\n",
      "{'loss': 3.0523, 'grad_norm': 8.417475700378418, 'learning_rate': 3.37260034904014e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2701, 'grad_norm': 8.360200881958008, 'learning_rate': 3.371873182082606e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1313, 'grad_norm': 7.7081146240234375, 'learning_rate': 3.371146015125073e-05, 'epoch': 0.98}\n",
      "{'loss': 3.052, 'grad_norm': 8.961637496948242, 'learning_rate': 3.370418848167539e-05, 'epoch': 0.98}\n",
      "{'loss': 3.115, 'grad_norm': 8.103293418884277, 'learning_rate': 3.369691681210006e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2139, 'grad_norm': 9.567258834838867, 'learning_rate': 3.3689645142524725e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1041, 'grad_norm': 8.486278533935547, 'learning_rate': 3.368237347294939e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1844, 'grad_norm': 8.804815292358398, 'learning_rate': 3.367510180337405e-05, 'epoch': 0.98}\n",
      "{'loss': 3.083, 'grad_norm': 7.621060848236084, 'learning_rate': 3.3667830133798726e-05, 'epoch': 0.98}\n",
      "{'loss': 3.117, 'grad_norm': 8.332134246826172, 'learning_rate': 3.3660558464223386e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1129, 'grad_norm': 8.06214427947998, 'learning_rate': 3.365328679464805e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1613, 'grad_norm': 8.571454048156738, 'learning_rate': 3.364601512507272e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1949, 'grad_norm': 7.544650554656982, 'learning_rate': 3.3638743455497386e-05, 'epoch': 0.98}\n",
      " 33%|██████████▊                      | 22500/68760 [4:11:42<7:46:12,  1.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.21s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.645348, 'eval_rouge-2': 8.148422, 'eval_rouge-l': 25.731652000000004, 'eval_bleu-4': 0.041843030580432476, 'eval_runtime': 28.0147, 'eval_samples_per_second': 1.785, 'eval_steps_per_second': 0.143, 'epoch': 0.98}\n",
      " 33%|██████████▊                      | 22500/68760 [4:12:10<7:46:12,  1.65it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.33s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-22500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1285, 'grad_norm': 8.287904739379883, 'learning_rate': 3.3631471785922046e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2113, 'grad_norm': 8.593152046203613, 'learning_rate': 3.362420011634671e-05, 'epoch': 0.98}\n",
      "{'loss': 3.3049, 'grad_norm': 8.113163948059082, 'learning_rate': 3.361692844677138e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1756, 'grad_norm': 8.142037391662598, 'learning_rate': 3.360965677719605e-05, 'epoch': 0.98}\n",
      "{'loss': 3.2639, 'grad_norm': 9.935845375061035, 'learning_rate': 3.360238510762071e-05, 'epoch': 0.98}\n",
      "{'loss': 3.0566, 'grad_norm': 9.213884353637695, 'learning_rate': 3.359511343804538e-05, 'epoch': 0.98}\n",
      "{'loss': 3.1611, 'grad_norm': 7.856864929199219, 'learning_rate': 3.358784176847004e-05, 'epoch': 0.98}\n",
      "{'loss': 3.0129, 'grad_norm': 7.999789237976074, 'learning_rate': 3.358057009889471e-05, 'epoch': 0.99}\n",
      "{'loss': 3.1709, 'grad_norm': 8.911519050598145, 'learning_rate': 3.3573298429319375e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2563, 'grad_norm': 7.465448379516602, 'learning_rate': 3.356602675974404e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2324, 'grad_norm': 8.87445068359375, 'learning_rate': 3.35587550901687e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2219, 'grad_norm': 8.41269302368164, 'learning_rate': 3.355148342059337e-05, 'epoch': 0.99}\n",
      "{'loss': 3.1469, 'grad_norm': 8.058506965637207, 'learning_rate': 3.3544211751018036e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2105, 'grad_norm': 8.595646858215332, 'learning_rate': 3.35369400814427e-05, 'epoch': 0.99}\n",
      "{'loss': 3.0375, 'grad_norm': 7.393467426300049, 'learning_rate': 3.352966841186736e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2162, 'grad_norm': 8.916833877563477, 'learning_rate': 3.3522396742292036e-05, 'epoch': 0.99}\n",
      "{'loss': 2.9937, 'grad_norm': 8.565555572509766, 'learning_rate': 3.3515125072716696e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2008, 'grad_norm': 8.014126777648926, 'learning_rate': 3.350785340314136e-05, 'epoch': 0.99}\n",
      "{'loss': 3.1221, 'grad_norm': 7.789655685424805, 'learning_rate': 3.350058173356603e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2033, 'grad_norm': 9.316112518310547, 'learning_rate': 3.34933100639907e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2541, 'grad_norm': 7.982951641082764, 'learning_rate': 3.348603839441536e-05, 'epoch': 0.99}\n",
      "{'loss': 3.0553, 'grad_norm': 8.220993041992188, 'learning_rate': 3.3478766724840024e-05, 'epoch': 0.99}\n",
      "{'loss': 3.1904, 'grad_norm': 8.455738067626953, 'learning_rate': 3.347149505526469e-05, 'epoch': 0.99}\n",
      "{'loss': 3.1848, 'grad_norm': 7.734719753265381, 'learning_rate': 3.346422338568936e-05, 'epoch': 0.99}\n",
      "{'loss': 3.1848, 'grad_norm': 8.132760047912598, 'learning_rate': 3.345695171611402e-05, 'epoch': 0.99}\n",
      "{'loss': 3.0795, 'grad_norm': 8.42527961730957, 'learning_rate': 3.344968004653869e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2742, 'grad_norm': 8.677690505981445, 'learning_rate': 3.344240837696335e-05, 'epoch': 0.99}\n",
      "{'loss': 3.1682, 'grad_norm': 8.087791442871094, 'learning_rate': 3.343513670738802e-05, 'epoch': 0.99}\n",
      "{'loss': 3.2906, 'grad_norm': 8.50763988494873, 'learning_rate': 3.3427865037812686e-05, 'epoch': 0.99}\n",
      "{'loss': 3.202, 'grad_norm': 8.470118522644043, 'learning_rate': 3.3420593368237346e-05, 'epoch': 0.99}\n",
      "{'loss': 3.1453, 'grad_norm': 7.587673187255859, 'learning_rate': 3.341332169866201e-05, 'epoch': 1.0}\n",
      "{'loss': 3.3311, 'grad_norm': 9.46357250213623, 'learning_rate': 3.340605002908668e-05, 'epoch': 1.0}\n",
      "{'loss': 3.158, 'grad_norm': 8.539006233215332, 'learning_rate': 3.3398778359511346e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2107, 'grad_norm': 9.253707885742188, 'learning_rate': 3.3391506689936007e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1719, 'grad_norm': 8.99354362487793, 'learning_rate': 3.338423502036068e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0189, 'grad_norm': 7.893254280090332, 'learning_rate': 3.337696335078534e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1193, 'grad_norm': 8.675494194030762, 'learning_rate': 3.336969168121001e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1455, 'grad_norm': 9.266083717346191, 'learning_rate': 3.3362420011634674e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0715, 'grad_norm': 7.802829265594482, 'learning_rate': 3.335514834205934e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2486, 'grad_norm': 7.7932257652282715, 'learning_rate': 3.3347876672484e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1549, 'grad_norm': 9.194401741027832, 'learning_rate': 3.334060500290867e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1566, 'grad_norm': 8.963071823120117, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0896, 'grad_norm': 7.813235759735107, 'learning_rate': 3.3326061663758e-05, 'epoch': 1.0}\n",
      "{'loss': 3.041, 'grad_norm': 7.8132853507995605, 'learning_rate': 3.331878999418266e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1504, 'grad_norm': 8.563708305358887, 'learning_rate': 3.3311518324607336e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1594, 'grad_norm': 8.106700897216797, 'learning_rate': 3.3304246655031996e-05, 'epoch': 1.0}\n",
      "{'loss': 3.141, 'grad_norm': 7.748703956604004, 'learning_rate': 3.329697498545666e-05, 'epoch': 1.0}\n",
      "{'loss': 3.0547, 'grad_norm': 8.322007179260254, 'learning_rate': 3.328970331588133e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1016, 'grad_norm': 8.059708595275879, 'learning_rate': 3.3282431646305996e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1187, 'grad_norm': 7.805742263793945, 'learning_rate': 3.3275159976730657e-05, 'epoch': 1.0}\n",
      " 33%|███████████                      | 23000/68760 [4:17:18<8:27:21,  1.50it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.760578, 'eval_rouge-2': 7.613804, 'eval_rouge-l': 22.958216000000007, 'eval_bleu-4': 0.030531492834988582, 'eval_runtime': 37.582, 'eval_samples_per_second': 1.33, 'eval_steps_per_second': 0.106, 'epoch': 1.0}\n",
      " 33%|███████████                      | 23000/68760 [4:17:56<8:27:21,  1.50it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.74s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-23000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3236, 'grad_norm': 7.831630229949951, 'learning_rate': 3.3267888307155323e-05, 'epoch': 1.0}\n",
      "{'loss': 3.135, 'grad_norm': 9.286681175231934, 'learning_rate': 3.326061663757999e-05, 'epoch': 1.0}\n",
      "{'loss': 3.1164, 'grad_norm': 8.476383209228516, 'learning_rate': 3.325334496800466e-05, 'epoch': 1.0}\n",
      "{'loss': 3.2201, 'grad_norm': 8.12051773071289, 'learning_rate': 3.324607329842932e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2021, 'grad_norm': 8.043586730957031, 'learning_rate': 3.323880162885399e-05, 'epoch': 1.01}\n",
      "{'loss': 3.0002, 'grad_norm': 8.904596328735352, 'learning_rate': 3.323152995927865e-05, 'epoch': 1.01}\n",
      "{'loss': 3.0996, 'grad_norm': 8.318132400512695, 'learning_rate': 3.322425828970332e-05, 'epoch': 1.01}\n",
      "{'loss': 3.1744, 'grad_norm': 9.368656158447266, 'learning_rate': 3.3216986620127985e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2049, 'grad_norm': 8.349419593811035, 'learning_rate': 3.320971495055265e-05, 'epoch': 1.01}\n",
      "{'loss': 3.1746, 'grad_norm': 10.328813552856445, 'learning_rate': 3.320244328097731e-05, 'epoch': 1.01}\n",
      "{'loss': 3.0457, 'grad_norm': 8.176734924316406, 'learning_rate': 3.319517161140198e-05, 'epoch': 1.01}\n",
      "{'loss': 3.0971, 'grad_norm': 8.751615524291992, 'learning_rate': 3.3187899941826646e-05, 'epoch': 1.01}\n",
      "{'loss': 3.0768, 'grad_norm': 8.800455093383789, 'learning_rate': 3.318062827225131e-05, 'epoch': 1.01}\n",
      "{'loss': 3.1102, 'grad_norm': 9.000253677368164, 'learning_rate': 3.317335660267597e-05, 'epoch': 1.01}\n",
      "{'loss': 3.102, 'grad_norm': 7.90488862991333, 'learning_rate': 3.3166084933100646e-05, 'epoch': 1.01}\n",
      "{'loss': 3.0336, 'grad_norm': 7.590386390686035, 'learning_rate': 3.3158813263525307e-05, 'epoch': 1.01}\n",
      "{'loss': 3.1588, 'grad_norm': 8.410246849060059, 'learning_rate': 3.3151541593949973e-05, 'epoch': 1.01}\n",
      "{'loss': 3.0627, 'grad_norm': 7.869672775268555, 'learning_rate': 3.314426992437464e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2156, 'grad_norm': 8.573307991027832, 'learning_rate': 3.31369982547993e-05, 'epoch': 1.01}\n",
      "{'loss': 3.009, 'grad_norm': 8.642658233642578, 'learning_rate': 3.312972658522397e-05, 'epoch': 1.01}\n",
      "{'loss': 3.0014, 'grad_norm': 9.29666805267334, 'learning_rate': 3.3122454915648634e-05, 'epoch': 1.01}\n",
      "{'loss': 3.1826, 'grad_norm': 8.206674575805664, 'learning_rate': 3.31151832460733e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2068, 'grad_norm': 7.64562463760376, 'learning_rate': 3.310791157649796e-05, 'epoch': 1.01}\n",
      "{'loss': 3.1432, 'grad_norm': 8.352313041687012, 'learning_rate': 3.310063990692263e-05, 'epoch': 1.01}\n",
      "{'loss': 3.2348, 'grad_norm': 9.096404075622559, 'learning_rate': 3.3093368237347295e-05, 'epoch': 1.01}\n",
      "{'loss': 3.1004, 'grad_norm': 8.444154739379883, 'learning_rate': 3.308609656777196e-05, 'epoch': 1.01}\n",
      "{'loss': 3.1096, 'grad_norm': 8.079862594604492, 'learning_rate': 3.307882489819662e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1564, 'grad_norm': 8.116217613220215, 'learning_rate': 3.3071553228621296e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2988, 'grad_norm': 7.918710708618164, 'learning_rate': 3.3064281559045956e-05, 'epoch': 1.02}\n",
      "{'loss': 3.166, 'grad_norm': 8.378368377685547, 'learning_rate': 3.305700988947062e-05, 'epoch': 1.02}\n",
      "{'loss': 2.9629, 'grad_norm': 9.152379035949707, 'learning_rate': 3.304973821989529e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1404, 'grad_norm': 8.356199264526367, 'learning_rate': 3.3042466550319957e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0936, 'grad_norm': 8.805525779724121, 'learning_rate': 3.303519488074462e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0191, 'grad_norm': 8.31607437133789, 'learning_rate': 3.302792321116929e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1264, 'grad_norm': 8.356400489807129, 'learning_rate': 3.302065154159395e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1586, 'grad_norm': 8.898669242858887, 'learning_rate': 3.301337987201862e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1357, 'grad_norm': 8.434859275817871, 'learning_rate': 3.3006108202443284e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1814, 'grad_norm': 7.8716278076171875, 'learning_rate': 3.299883653286795e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0678, 'grad_norm': 8.916910171508789, 'learning_rate': 3.299156486329261e-05, 'epoch': 1.02}\n",
      "{'loss': 3.073, 'grad_norm': 9.078968048095703, 'learning_rate': 3.298429319371728e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1551, 'grad_norm': 7.674447059631348, 'learning_rate': 3.2977021524141945e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1457, 'grad_norm': 8.373427391052246, 'learning_rate': 3.296974985456661e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0789, 'grad_norm': 8.708617210388184, 'learning_rate': 3.296247818499127e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0248, 'grad_norm': 9.228001594543457, 'learning_rate': 3.2955206515415946e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1836, 'grad_norm': 8.311487197875977, 'learning_rate': 3.2947934845840606e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0271, 'grad_norm': 8.197856903076172, 'learning_rate': 3.294066317626527e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1004, 'grad_norm': 8.466194152832031, 'learning_rate': 3.293339150668994e-05, 'epoch': 1.02}\n",
      "{'loss': 3.0551, 'grad_norm': 8.09655475616455, 'learning_rate': 3.2926119837114607e-05, 'epoch': 1.02}\n",
      "{'loss': 3.1822, 'grad_norm': 8.774310111999512, 'learning_rate': 3.291884816753927e-05, 'epoch': 1.02}\n",
      "{'loss': 3.2561, 'grad_norm': 8.676827430725098, 'learning_rate': 3.2911576497963934e-05, 'epoch': 1.03}\n",
      " 34%|███████████▎                     | 23500/68760 [4:23:11<8:23:07,  1.50it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.33s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.73s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.148734, 'eval_rouge-2': 7.545256, 'eval_rouge-l': 25.418452, 'eval_bleu-4': 0.03486325175317413, 'eval_runtime': 25.7028, 'eval_samples_per_second': 1.945, 'eval_steps_per_second': 0.156, 'epoch': 1.03}\n",
      " 34%|███████████▎                     | 23500/68760 [4:23:37<8:23:07,  1.50it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:23<00:00,  7.00s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-23500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0609, 'grad_norm': 8.07009506225586, 'learning_rate': 3.29043048283886e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1795, 'grad_norm': 8.947975158691406, 'learning_rate': 3.289703315881327e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2176, 'grad_norm': 8.690415382385254, 'learning_rate': 3.288976148923793e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2129, 'grad_norm': 8.254060745239258, 'learning_rate': 3.28824898196626e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1262, 'grad_norm': 8.795238494873047, 'learning_rate': 3.287521815008726e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1631, 'grad_norm': 7.697626113891602, 'learning_rate': 3.286794648051193e-05, 'epoch': 1.03}\n",
      "{'loss': 3.0215, 'grad_norm': 7.980179786682129, 'learning_rate': 3.2860674810936595e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1207, 'grad_norm': 9.973343849182129, 'learning_rate': 3.285340314136126e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2012, 'grad_norm': 8.235068321228027, 'learning_rate': 3.284613147178592e-05, 'epoch': 1.03}\n",
      "{'loss': 3.0502, 'grad_norm': 7.696573734283447, 'learning_rate': 3.283885980221059e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1461, 'grad_norm': 7.898824691772461, 'learning_rate': 3.2831588132635256e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2123, 'grad_norm': 7.853653430938721, 'learning_rate': 3.2824316463059916e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2045, 'grad_norm': 10.04684066772461, 'learning_rate': 3.281704479348458e-05, 'epoch': 1.03}\n",
      "{'loss': 3.2926, 'grad_norm': 8.480783462524414, 'learning_rate': 3.280977312390925e-05, 'epoch': 1.03}\n",
      "{'loss': 3.0611, 'grad_norm': 8.050052642822266, 'learning_rate': 3.280250145433392e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1344, 'grad_norm': 8.30738639831543, 'learning_rate': 3.279522978475858e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1279, 'grad_norm': 8.14098072052002, 'learning_rate': 3.278795811518325e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1939, 'grad_norm': 7.716287136077881, 'learning_rate': 3.278068644560791e-05, 'epoch': 1.03}\n",
      "{'loss': 3.1207, 'grad_norm': 7.573678970336914, 'learning_rate': 3.277341477603258e-05, 'epoch': 1.03}\n",
      "{'loss': 3.0523, 'grad_norm': 10.146397590637207, 'learning_rate': 3.2766143106457244e-05, 'epoch': 1.03}\n",
      "{'loss': 3.0641, 'grad_norm': 9.161800384521484, 'learning_rate': 3.275887143688191e-05, 'epoch': 1.03}\n",
      "{'loss': 3.0602, 'grad_norm': 8.411911964416504, 'learning_rate': 3.275159976730657e-05, 'epoch': 1.03}\n",
      "{'loss': 3.16, 'grad_norm': 9.252402305603027, 'learning_rate': 3.2744328097731245e-05, 'epoch': 1.04}\n",
      "{'loss': 3.168, 'grad_norm': 8.176835060119629, 'learning_rate': 3.2737056428155905e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0828, 'grad_norm': 8.01111125946045, 'learning_rate': 3.272978475858057e-05, 'epoch': 1.04}\n",
      "{'loss': 2.948, 'grad_norm': 8.876648902893066, 'learning_rate': 3.272251308900524e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0836, 'grad_norm': 7.764787197113037, 'learning_rate': 3.2715241419429906e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1816, 'grad_norm': 11.020857810974121, 'learning_rate': 3.2707969749854566e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0916, 'grad_norm': 9.72779655456543, 'learning_rate': 3.270069808027923e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2502, 'grad_norm': 8.824885368347168, 'learning_rate': 3.26934264107039e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1139, 'grad_norm': 8.238787651062012, 'learning_rate': 3.268615474112857e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1168, 'grad_norm': 8.824479103088379, 'learning_rate': 3.267888307155323e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1299, 'grad_norm': 9.175180435180664, 'learning_rate': 3.26716114019779e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2826, 'grad_norm': 8.414942741394043, 'learning_rate': 3.266433973240256e-05, 'epoch': 1.04}\n",
      "{'loss': 3.292, 'grad_norm': 9.136401176452637, 'learning_rate': 3.265706806282723e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1271, 'grad_norm': 8.139152526855469, 'learning_rate': 3.2649796393251894e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0898, 'grad_norm': 8.063246726989746, 'learning_rate': 3.264252472367656e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0918, 'grad_norm': 7.869759559631348, 'learning_rate': 3.263525305410122e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0729, 'grad_norm': 7.854191780090332, 'learning_rate': 3.262798138452589e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2299, 'grad_norm': 8.807025909423828, 'learning_rate': 3.2620709714950555e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0186, 'grad_norm': 8.17080307006836, 'learning_rate': 3.261343804537522e-05, 'epoch': 1.04}\n",
      "{'loss': 3.3088, 'grad_norm': 9.549724578857422, 'learning_rate': 3.260616637579988e-05, 'epoch': 1.04}\n",
      "{'loss': 3.2033, 'grad_norm': 8.814227104187012, 'learning_rate': 3.2598894706224556e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1047, 'grad_norm': 9.190082550048828, 'learning_rate': 3.2591623036649216e-05, 'epoch': 1.04}\n",
      "{'loss': 3.0184, 'grad_norm': 8.898087501525879, 'learning_rate': 3.258435136707388e-05, 'epoch': 1.04}\n",
      "{'loss': 3.1508, 'grad_norm': 8.302957534790039, 'learning_rate': 3.257707969749855e-05, 'epoch': 1.05}\n",
      "{'loss': 3.0332, 'grad_norm': 8.654556274414062, 'learning_rate': 3.256980802792322e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1518, 'grad_norm': 8.043381690979004, 'learning_rate': 3.256253635834788e-05, 'epoch': 1.05}\n",
      "{'loss': 3.117, 'grad_norm': 8.02869701385498, 'learning_rate': 3.2555264688772544e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1361, 'grad_norm': 8.695639610290527, 'learning_rate': 3.254799301919721e-05, 'epoch': 1.05}\n",
      " 35%|███████████▌                     | 24000/68760 [4:28:48<7:33:59,  1.64it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.16s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.27s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.673049999999996, 'eval_rouge-2': 8.245758, 'eval_rouge-l': 26.443743999999995, 'eval_bleu-4': 0.03700798931035542, 'eval_runtime': 17.4519, 'eval_samples_per_second': 2.865, 'eval_steps_per_second': 0.229, 'epoch': 1.05}\n",
      " 35%|███████████▌                     | 24000/68760 [4:29:05<7:33:59,  1.64it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.29s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-24000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1715, 'grad_norm': 8.054855346679688, 'learning_rate': 3.254072134962187e-05, 'epoch': 1.05}\n",
      "{'loss': 3.157, 'grad_norm': 7.833219528198242, 'learning_rate': 3.253344968004654e-05, 'epoch': 1.05}\n",
      "{'loss': 2.9906, 'grad_norm': 8.504403114318848, 'learning_rate': 3.2526178010471204e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1947, 'grad_norm': 12.893717765808105, 'learning_rate': 3.251890634089587e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2893, 'grad_norm': 9.054051399230957, 'learning_rate': 3.251163467132053e-05, 'epoch': 1.05}\n",
      "{'loss': 3.0271, 'grad_norm': 8.10438346862793, 'learning_rate': 3.2504363001745205e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1771, 'grad_norm': 8.03592300415039, 'learning_rate': 3.2497091332169865e-05, 'epoch': 1.05}\n",
      "{'loss': 3.0877, 'grad_norm': 7.702360153198242, 'learning_rate': 3.248981966259453e-05, 'epoch': 1.05}\n",
      "{'loss': 3.0373, 'grad_norm': 7.803812503814697, 'learning_rate': 3.24825479930192e-05, 'epoch': 1.05}\n",
      "{'loss': 3.0811, 'grad_norm': 9.79120922088623, 'learning_rate': 3.2475276323443866e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2004, 'grad_norm': 7.773916244506836, 'learning_rate': 3.2468004653868526e-05, 'epoch': 1.05}\n",
      "{'loss': 3.0885, 'grad_norm': 7.986030578613281, 'learning_rate': 3.246073298429319e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1219, 'grad_norm': 8.041728973388672, 'learning_rate': 3.245346131471786e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1086, 'grad_norm': 8.386717796325684, 'learning_rate': 3.244618964514253e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2135, 'grad_norm': 8.921724319458008, 'learning_rate': 3.243891797556719e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1711, 'grad_norm': 7.8877034187316895, 'learning_rate': 3.243164630599186e-05, 'epoch': 1.05}\n",
      "{'loss': 3.2301, 'grad_norm': 7.882379055023193, 'learning_rate': 3.242437463641652e-05, 'epoch': 1.05}\n",
      "{'loss': 3.0799, 'grad_norm': 8.152450561523438, 'learning_rate': 3.241710296684119e-05, 'epoch': 1.05}\n",
      "{'loss': 3.1049, 'grad_norm': 8.510104179382324, 'learning_rate': 3.2409831297265854e-05, 'epoch': 1.06}\n",
      "{'loss': 3.0492, 'grad_norm': 9.140117645263672, 'learning_rate': 3.240255962769052e-05, 'epoch': 1.06}\n",
      "{'loss': 3.4131, 'grad_norm': 8.469855308532715, 'learning_rate': 3.239528795811518e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1832, 'grad_norm': 8.819181442260742, 'learning_rate': 3.2388016288539855e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1387, 'grad_norm': 8.623221397399902, 'learning_rate': 3.2380744618964515e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1467, 'grad_norm': 8.1483736038208, 'learning_rate': 3.237347294938918e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1723, 'grad_norm': 8.620976448059082, 'learning_rate': 3.236620127981385e-05, 'epoch': 1.06}\n",
      "{'loss': 3.132, 'grad_norm': 7.7771406173706055, 'learning_rate': 3.2358929610238516e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2182, 'grad_norm': 8.81428337097168, 'learning_rate': 3.2351657940663176e-05, 'epoch': 1.06}\n",
      "{'loss': 3.2039, 'grad_norm': 9.385285377502441, 'learning_rate': 3.234438627108784e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1922, 'grad_norm': 8.103568077087402, 'learning_rate': 3.233711460151251e-05, 'epoch': 1.06}\n",
      "{'loss': 3.0582, 'grad_norm': 8.369438171386719, 'learning_rate': 3.232984293193718e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1293, 'grad_norm': 8.565240859985352, 'learning_rate': 3.232257126236184e-05, 'epoch': 1.06}\n",
      "{'loss': 3.0504, 'grad_norm': 8.921337127685547, 'learning_rate': 3.231529959278651e-05, 'epoch': 1.06}\n",
      "{'loss': 3.0801, 'grad_norm': 7.914220809936523, 'learning_rate': 3.230802792321117e-05, 'epoch': 1.06}\n",
      "{'loss': 3.0986, 'grad_norm': 7.99212646484375, 'learning_rate': 3.230075625363584e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1631, 'grad_norm': 9.704071044921875, 'learning_rate': 3.2293484584060504e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1322, 'grad_norm': 8.760109901428223, 'learning_rate': 3.228621291448517e-05, 'epoch': 1.06}\n",
      "{'loss': 3.268, 'grad_norm': 10.014775276184082, 'learning_rate': 3.227894124490983e-05, 'epoch': 1.06}\n",
      "{'loss': 3.0418, 'grad_norm': 10.033585548400879, 'learning_rate': 3.22716695753345e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1949, 'grad_norm': 8.554483413696289, 'learning_rate': 3.2264397905759165e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1854, 'grad_norm': 7.44661808013916, 'learning_rate': 3.2257126236183825e-05, 'epoch': 1.06}\n",
      "{'loss': 3.1199, 'grad_norm': 9.172832489013672, 'learning_rate': 3.224985456660849e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1596, 'grad_norm': 8.545625686645508, 'learning_rate': 3.224258289703316e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1516, 'grad_norm': 8.747647285461426, 'learning_rate': 3.2235311227457826e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1852, 'grad_norm': 8.409173965454102, 'learning_rate': 3.2228039557882486e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2311, 'grad_norm': 8.437325477600098, 'learning_rate': 3.222076788830716e-05, 'epoch': 1.07}\n",
      "{'loss': 2.9688, 'grad_norm': 8.886825561523438, 'learning_rate': 3.221349621873182e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2586, 'grad_norm': 9.868172645568848, 'learning_rate': 3.220622454915649e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1375, 'grad_norm': 8.481508255004883, 'learning_rate': 3.2198952879581154e-05, 'epoch': 1.07}\n",
      "{'loss': 3.0293, 'grad_norm': 8.196361541748047, 'learning_rate': 3.219168121000582e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1783, 'grad_norm': 8.412668228149414, 'learning_rate': 3.218440954043048e-05, 'epoch': 1.07}\n",
      " 36%|███████████▊                     | 24500/68760 [4:34:20<8:08:34,  1.51it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:04,  4.79s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.19542, 'eval_rouge-2': 8.139513999999998, 'eval_rouge-l': 25.894807999999998, 'eval_bleu-4': 0.04065895863157519, 'eval_runtime': 28.6628, 'eval_samples_per_second': 1.744, 'eval_steps_per_second': 0.14, 'epoch': 1.07}\n",
      " 36%|███████████▊                     | 24500/68760 [4:34:49<8:08:34,  1.51it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  3.42s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-24500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0898, 'grad_norm': 8.213693618774414, 'learning_rate': 3.217713787085515e-05, 'epoch': 1.07}\n",
      "{'loss': 3.091, 'grad_norm': 9.97099781036377, 'learning_rate': 3.2169866201279815e-05, 'epoch': 1.07}\n",
      "{'loss': 3.058, 'grad_norm': 7.603181838989258, 'learning_rate': 3.216259453170448e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2146, 'grad_norm': 8.310478210449219, 'learning_rate': 3.215532286212914e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1523, 'grad_norm': 8.741969108581543, 'learning_rate': 3.2148051192553815e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1533, 'grad_norm': 9.459583282470703, 'learning_rate': 3.2140779522978475e-05, 'epoch': 1.07}\n",
      "{'loss': 3.0973, 'grad_norm': 9.804545402526855, 'learning_rate': 3.213350785340314e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1639, 'grad_norm': 8.474601745605469, 'learning_rate': 3.212623618382781e-05, 'epoch': 1.07}\n",
      "{'loss': 3.0793, 'grad_norm': 8.320444107055664, 'learning_rate': 3.2118964514252476e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1855, 'grad_norm': 8.597618103027344, 'learning_rate': 3.2111692844677136e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1092, 'grad_norm': 7.436195373535156, 'learning_rate': 3.210442117510181e-05, 'epoch': 1.07}\n",
      "{'loss': 3.2246, 'grad_norm': 8.53630542755127, 'learning_rate': 3.209714950552647e-05, 'epoch': 1.07}\n",
      "{'loss': 3.1729, 'grad_norm': 8.094773292541504, 'learning_rate': 3.208987783595114e-05, 'epoch': 1.07}\n",
      "{'loss': 3.0689, 'grad_norm': 8.38346004486084, 'learning_rate': 3.2082606166375804e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1268, 'grad_norm': 8.06756591796875, 'learning_rate': 3.207533449680047e-05, 'epoch': 1.08}\n",
      "{'loss': 3.09, 'grad_norm': 8.423660278320312, 'learning_rate': 3.206806282722513e-05, 'epoch': 1.08}\n",
      "{'loss': 3.0848, 'grad_norm': 8.148916244506836, 'learning_rate': 3.20607911576498e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2055, 'grad_norm': 8.112811088562012, 'learning_rate': 3.2053519488074465e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1135, 'grad_norm': 8.420392990112305, 'learning_rate': 3.204624781849913e-05, 'epoch': 1.08}\n",
      "{'loss': 3.0389, 'grad_norm': 8.460677146911621, 'learning_rate': 3.203897614892379e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1543, 'grad_norm': 8.873771667480469, 'learning_rate': 3.2031704479348465e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1576, 'grad_norm': 8.644503593444824, 'learning_rate': 3.2024432809773125e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2277, 'grad_norm': 8.320791244506836, 'learning_rate': 3.201716114019779e-05, 'epoch': 1.08}\n",
      "{'loss': 3.118, 'grad_norm': 8.775068283081055, 'learning_rate': 3.200988947062246e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1559, 'grad_norm': 8.67391586303711, 'learning_rate': 3.2002617801047126e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1838, 'grad_norm': 7.9985785484313965, 'learning_rate': 3.1995346131471786e-05, 'epoch': 1.08}\n",
      "{'loss': 3.0904, 'grad_norm': 9.077777862548828, 'learning_rate': 3.198807446189645e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1299, 'grad_norm': 8.61130142211914, 'learning_rate': 3.198080279232112e-05, 'epoch': 1.08}\n",
      "{'loss': 3.0607, 'grad_norm': 8.700233459472656, 'learning_rate': 3.197353112274578e-05, 'epoch': 1.08}\n",
      "{'loss': 3.0697, 'grad_norm': 8.295226097106934, 'learning_rate': 3.196625945317045e-05, 'epoch': 1.08}\n",
      "{'loss': 3.2141, 'grad_norm': 8.457711219787598, 'learning_rate': 3.1958987783595114e-05, 'epoch': 1.08}\n",
      "{'loss': 3.0561, 'grad_norm': 7.890778064727783, 'learning_rate': 3.195171611401978e-05, 'epoch': 1.08}\n",
      "{'loss': 2.9514, 'grad_norm': 8.696612358093262, 'learning_rate': 3.194444444444444e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1574, 'grad_norm': 7.921998977661133, 'learning_rate': 3.1937172774869115e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1553, 'grad_norm': 7.6412034034729, 'learning_rate': 3.1929901105293775e-05, 'epoch': 1.08}\n",
      "{'loss': 3.0246, 'grad_norm': 7.377431869506836, 'learning_rate': 3.192262943571844e-05, 'epoch': 1.08}\n",
      "{'loss': 3.1193, 'grad_norm': 8.124543190002441, 'learning_rate': 3.191535776614311e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2092, 'grad_norm': 8.722655296325684, 'learning_rate': 3.1908086096567775e-05, 'epoch': 1.09}\n",
      "{'loss': 3.0707, 'grad_norm': 8.634138107299805, 'learning_rate': 3.1900814426992435e-05, 'epoch': 1.09}\n",
      "{'loss': 3.0902, 'grad_norm': 9.738107681274414, 'learning_rate': 3.18935427574171e-05, 'epoch': 1.09}\n",
      "{'loss': 3.1154, 'grad_norm': 6.925045490264893, 'learning_rate': 3.188627108784177e-05, 'epoch': 1.09}\n",
      "{'loss': 3.1629, 'grad_norm': 8.322793006896973, 'learning_rate': 3.1878999418266436e-05, 'epoch': 1.09}\n",
      "{'loss': 3.0664, 'grad_norm': 8.228691101074219, 'learning_rate': 3.1871727748691096e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2072, 'grad_norm': 8.173569679260254, 'learning_rate': 3.186445607911577e-05, 'epoch': 1.09}\n",
      "{'loss': 3.1102, 'grad_norm': 8.499201774597168, 'learning_rate': 3.185718440954043e-05, 'epoch': 1.09}\n",
      "{'loss': 3.1541, 'grad_norm': 8.585569381713867, 'learning_rate': 3.18499127399651e-05, 'epoch': 1.09}\n",
      "{'loss': 3.218, 'grad_norm': 8.836889266967773, 'learning_rate': 3.1842641070389764e-05, 'epoch': 1.09}\n",
      "{'loss': 3.076, 'grad_norm': 7.332571506500244, 'learning_rate': 3.183536940081443e-05, 'epoch': 1.09}\n",
      "{'loss': 3.0654, 'grad_norm': 8.150394439697266, 'learning_rate': 3.182809773123909e-05, 'epoch': 1.09}\n",
      "{'loss': 3.0873, 'grad_norm': 8.760895729064941, 'learning_rate': 3.182082606166376e-05, 'epoch': 1.09}\n",
      " 36%|███████████▉                     | 25000/68760 [4:39:59<7:01:58,  1.73it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:04,  4.92s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.437286, 'eval_rouge-2': 7.748119999999999, 'eval_rouge-l': 24.570947999999998, 'eval_bleu-4': 0.03336355681155635, 'eval_runtime': 29.0339, 'eval_samples_per_second': 1.722, 'eval_steps_per_second': 0.138, 'epoch': 1.09}\n",
      " 36%|███████████▉                     | 25000/68760 [4:40:29<7:01:58,  1.73it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  3.52s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-25000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3408, 'grad_norm': 8.29867172241211, 'learning_rate': 3.1813554392088425e-05, 'epoch': 1.09}\n",
      "{'loss': 3.0906, 'grad_norm': 8.237222671508789, 'learning_rate': 3.180628272251309e-05, 'epoch': 1.09}\n",
      "{'loss': 3.084, 'grad_norm': 8.53747272491455, 'learning_rate': 3.179901105293775e-05, 'epoch': 1.09}\n",
      "{'loss': 3.1529, 'grad_norm': 8.569717407226562, 'learning_rate': 3.1791739383362425e-05, 'epoch': 1.09}\n",
      "{'loss': 3.148, 'grad_norm': 7.994786739349365, 'learning_rate': 3.1784467713787085e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2412, 'grad_norm': 7.9835052490234375, 'learning_rate': 3.177719604421175e-05, 'epoch': 1.09}\n",
      "{'loss': 2.9916, 'grad_norm': 8.100902557373047, 'learning_rate': 3.176992437463642e-05, 'epoch': 1.09}\n",
      "{'loss': 3.1213, 'grad_norm': 8.46727180480957, 'learning_rate': 3.1762652705061086e-05, 'epoch': 1.09}\n",
      "{'loss': 3.2154, 'grad_norm': 8.574214935302734, 'learning_rate': 3.1755381035485746e-05, 'epoch': 1.09}\n",
      "{'loss': 3.273, 'grad_norm': 8.551701545715332, 'learning_rate': 3.174810936591042e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1887, 'grad_norm': 8.849104881286621, 'learning_rate': 3.174083769633508e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1695, 'grad_norm': 7.658357620239258, 'learning_rate': 3.173356602675975e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1225, 'grad_norm': 8.415321350097656, 'learning_rate': 3.1726294357184414e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1141, 'grad_norm': 8.40346908569336, 'learning_rate': 3.171902268760908e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1951, 'grad_norm': 8.459437370300293, 'learning_rate': 3.171175101803374e-05, 'epoch': 1.1}\n",
      "{'loss': 3.191, 'grad_norm': 8.174118041992188, 'learning_rate': 3.170447934845841e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1658, 'grad_norm': 9.966192245483398, 'learning_rate': 3.1697207678883075e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1619, 'grad_norm': 7.938348293304443, 'learning_rate': 3.168993600930774e-05, 'epoch': 1.1}\n",
      "{'loss': 3.107, 'grad_norm': 8.12311840057373, 'learning_rate': 3.16826643397324e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1734, 'grad_norm': 8.080925941467285, 'learning_rate': 3.167539267015707e-05, 'epoch': 1.1}\n",
      "{'loss': 3.0922, 'grad_norm': 8.527878761291504, 'learning_rate': 3.1668121000581735e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1746, 'grad_norm': 11.982213020324707, 'learning_rate': 3.1660849331006396e-05, 'epoch': 1.1}\n",
      "{'loss': 3.2133, 'grad_norm': 9.106158256530762, 'learning_rate': 3.165357766143107e-05, 'epoch': 1.1}\n",
      "{'loss': 3.0064, 'grad_norm': 9.785888671875, 'learning_rate': 3.164630599185573e-05, 'epoch': 1.1}\n",
      "{'loss': 2.9584, 'grad_norm': 8.262145042419434, 'learning_rate': 3.1639034322280396e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1207, 'grad_norm': 8.317401885986328, 'learning_rate': 3.163176265270506e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1572, 'grad_norm': 9.024850845336914, 'learning_rate': 3.162449098312973e-05, 'epoch': 1.1}\n",
      "{'loss': 3.217, 'grad_norm': 8.66784954071045, 'learning_rate': 3.161721931355439e-05, 'epoch': 1.1}\n",
      "{'loss': 3.0324, 'grad_norm': 8.454729080200195, 'learning_rate': 3.160994764397906e-05, 'epoch': 1.1}\n",
      "{'loss': 3.0492, 'grad_norm': 9.181974411010742, 'learning_rate': 3.1602675974403724e-05, 'epoch': 1.1}\n",
      "{'loss': 3.0719, 'grad_norm': 8.085620880126953, 'learning_rate': 3.159540430482839e-05, 'epoch': 1.1}\n",
      "{'loss': 3.1215, 'grad_norm': 9.236066818237305, 'learning_rate': 3.158813263525305e-05, 'epoch': 1.1}\n",
      "{'loss': 3.0553, 'grad_norm': 8.646662712097168, 'learning_rate': 3.1580860965677725e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1805, 'grad_norm': 7.715661525726318, 'learning_rate': 3.1573589296102385e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2086, 'grad_norm': 7.549842834472656, 'learning_rate': 3.156631762652705e-05, 'epoch': 1.11}\n",
      "{'loss': 3.0379, 'grad_norm': 10.342391014099121, 'learning_rate': 3.155904595695172e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1932, 'grad_norm': 7.9942545890808105, 'learning_rate': 3.1551774287376385e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1131, 'grad_norm': 9.486289024353027, 'learning_rate': 3.1544502617801046e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1873, 'grad_norm': 7.793340682983398, 'learning_rate': 3.153723094822571e-05, 'epoch': 1.11}\n",
      "{'loss': 3.0488, 'grad_norm': 8.16153621673584, 'learning_rate': 3.152995927865038e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1604, 'grad_norm': 8.421932220458984, 'learning_rate': 3.1522687609075046e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1697, 'grad_norm': 9.302718162536621, 'learning_rate': 3.1515415939499706e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2309, 'grad_norm': 8.436810493469238, 'learning_rate': 3.150814426992438e-05, 'epoch': 1.11}\n",
      "{'loss': 3.3074, 'grad_norm': 8.552831649780273, 'learning_rate': 3.150087260034904e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1816, 'grad_norm': 8.590453147888184, 'learning_rate': 3.149360093077371e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1793, 'grad_norm': 8.390835762023926, 'learning_rate': 3.1486329261198374e-05, 'epoch': 1.11}\n",
      "{'loss': 3.0793, 'grad_norm': 8.28833293914795, 'learning_rate': 3.147905759162304e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1287, 'grad_norm': 9.0914888381958, 'learning_rate': 3.14717859220477e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2289, 'grad_norm': 9.346165657043457, 'learning_rate': 3.146451425247237e-05, 'epoch': 1.11}\n",
      "{'loss': 3.14, 'grad_norm': 8.454733848571777, 'learning_rate': 3.1457242582897035e-05, 'epoch': 1.11}\n",
      " 37%|████████████▏                    | 25500/68760 [4:45:38<7:06:47,  1.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.44s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.80s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.960364, 'eval_rouge-2': 7.725917999999999, 'eval_rouge-l': 23.936503999999996, 'eval_bleu-4': 0.03631369049857561, 'eval_runtime': 28.728, 'eval_samples_per_second': 1.74, 'eval_steps_per_second': 0.139, 'epoch': 1.11}\n",
      " 37%|████████████▏                    | 25500/68760 [4:46:07<7:06:47,  1.69it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  4.21s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-25500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1227, 'grad_norm': 8.957324981689453, 'learning_rate': 3.14499709133217e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1678, 'grad_norm': 8.74405574798584, 'learning_rate': 3.144269924374637e-05, 'epoch': 1.11}\n",
      "{'loss': 3.2314, 'grad_norm': 8.301802635192871, 'learning_rate': 3.1435427574171035e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1531, 'grad_norm': 8.145340919494629, 'learning_rate': 3.1428155904595696e-05, 'epoch': 1.11}\n",
      "{'loss': 3.1186, 'grad_norm': 8.675671577453613, 'learning_rate': 3.142088423502036e-05, 'epoch': 1.11}\n",
      "{'loss': 3.0287, 'grad_norm': 8.98160171508789, 'learning_rate': 3.141361256544503e-05, 'epoch': 1.12}\n",
      "{'loss': 3.0445, 'grad_norm': 9.176536560058594, 'learning_rate': 3.1406340895869696e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2248, 'grad_norm': 8.143385887145996, 'learning_rate': 3.1399069226294356e-05, 'epoch': 1.12}\n",
      "{'loss': 3.0371, 'grad_norm': 7.8471455574035645, 'learning_rate': 3.139179755671902e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1447, 'grad_norm': 8.808349609375, 'learning_rate': 3.138452588714369e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1428, 'grad_norm': 8.707926750183105, 'learning_rate': 3.137725421756835e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1219, 'grad_norm': 8.23862075805664, 'learning_rate': 3.1369982547993024e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1623, 'grad_norm': 8.281974792480469, 'learning_rate': 3.1362710878417684e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1258, 'grad_norm': 7.917909622192383, 'learning_rate': 3.135543920884235e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1355, 'grad_norm': 8.342031478881836, 'learning_rate': 3.134816753926702e-05, 'epoch': 1.12}\n",
      "{'loss': 3.14, 'grad_norm': 8.408086776733398, 'learning_rate': 3.1340895869691685e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1064, 'grad_norm': 8.706205368041992, 'learning_rate': 3.1333624200116345e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1746, 'grad_norm': 8.818565368652344, 'learning_rate': 3.132635253054101e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1662, 'grad_norm': 8.730372428894043, 'learning_rate': 3.131908086096568e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2125, 'grad_norm': 8.712871551513672, 'learning_rate': 3.1311809191390346e-05, 'epoch': 1.12}\n",
      "{'loss': 2.9818, 'grad_norm': 7.4337286949157715, 'learning_rate': 3.1304537521815006e-05, 'epoch': 1.12}\n",
      "{'loss': 3.0453, 'grad_norm': 8.084488868713379, 'learning_rate': 3.129726585223968e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1184, 'grad_norm': 8.675010681152344, 'learning_rate': 3.128999418266434e-05, 'epoch': 1.12}\n",
      "{'loss': 3.2498, 'grad_norm': 8.036345481872559, 'learning_rate': 3.1282722513089006e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1529, 'grad_norm': 8.303439140319824, 'learning_rate': 3.127545084351367e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1088, 'grad_norm': 9.084996223449707, 'learning_rate': 3.126817917393834e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1383, 'grad_norm': 11.252397537231445, 'learning_rate': 3.1260907504363e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1801, 'grad_norm': 8.407496452331543, 'learning_rate': 3.125363583478767e-05, 'epoch': 1.12}\n",
      "{'loss': 3.1635, 'grad_norm': 8.543229103088379, 'learning_rate': 3.1246364165212334e-05, 'epoch': 1.13}\n",
      "{'loss': 3.2395, 'grad_norm': 8.705621719360352, 'learning_rate': 3.1239092495637e-05, 'epoch': 1.13}\n",
      "{'loss': 3.2225, 'grad_norm': 8.745471000671387, 'learning_rate': 3.123182082606166e-05, 'epoch': 1.13}\n",
      "{'loss': 3.2521, 'grad_norm': 7.798417091369629, 'learning_rate': 3.1224549156486335e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1791, 'grad_norm': 9.690661430358887, 'learning_rate': 3.1217277486910995e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1766, 'grad_norm': 8.376548767089844, 'learning_rate': 3.121000581733566e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1723, 'grad_norm': 9.150450706481934, 'learning_rate': 3.120273414776033e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1428, 'grad_norm': 10.453620910644531, 'learning_rate': 3.1195462478184996e-05, 'epoch': 1.13}\n",
      "{'loss': 3.0609, 'grad_norm': 8.328337669372559, 'learning_rate': 3.1188190808609656e-05, 'epoch': 1.13}\n",
      "{'loss': 3.0656, 'grad_norm': 8.388619422912598, 'learning_rate': 3.118091913903432e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1078, 'grad_norm': 7.525603294372559, 'learning_rate': 3.117364746945899e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1547, 'grad_norm': 8.702967643737793, 'learning_rate': 3.1166375799883656e-05, 'epoch': 1.13}\n",
      "{'loss': 3.0934, 'grad_norm': 8.492318153381348, 'learning_rate': 3.1159104130308317e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1295, 'grad_norm': 9.242269515991211, 'learning_rate': 3.115183246073299e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1303, 'grad_norm': 7.8487725257873535, 'learning_rate': 3.114456079115765e-05, 'epoch': 1.13}\n",
      "{'loss': 3.2029, 'grad_norm': 8.800668716430664, 'learning_rate': 3.113728912158232e-05, 'epoch': 1.13}\n",
      "{'loss': 3.159, 'grad_norm': 7.762956619262695, 'learning_rate': 3.1130017452006984e-05, 'epoch': 1.13}\n",
      "{'loss': 3.0557, 'grad_norm': 9.082324981689453, 'learning_rate': 3.112274578243165e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1139, 'grad_norm': 7.83008337020874, 'learning_rate': 3.111547411285631e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1131, 'grad_norm': 8.977788925170898, 'learning_rate': 3.1108202443280985e-05, 'epoch': 1.13}\n",
      "{'loss': 3.1002, 'grad_norm': 9.442399978637695, 'learning_rate': 3.1100930773705645e-05, 'epoch': 1.13}\n",
      "{'loss': 3.0732, 'grad_norm': 7.807487964630127, 'learning_rate': 3.1093659104130305e-05, 'epoch': 1.13}\n",
      " 38%|████████████▍                    | 26000/68760 [4:51:19<7:38:13,  1.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.06s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.31s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.49422, 'eval_rouge-2': 6.95763, 'eval_rouge-l': 25.186390000000003, 'eval_bleu-4': 0.03522441337783664, 'eval_runtime': 17.3134, 'eval_samples_per_second': 2.888, 'eval_steps_per_second': 0.231, 'epoch': 1.13}\n",
      " 38%|████████████▍                    | 26000/68760 [4:51:37<7:38:13,  1.56it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.25s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-26000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1689, 'grad_norm': 8.082609176635742, 'learning_rate': 3.108638743455498e-05, 'epoch': 1.13}\n",
      "{'loss': 3.2516, 'grad_norm': 8.956978797912598, 'learning_rate': 3.107911576497964e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1656, 'grad_norm': 8.0506591796875, 'learning_rate': 3.1071844095404306e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1516, 'grad_norm': 7.982180118560791, 'learning_rate': 3.106457242582897e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1859, 'grad_norm': 9.01225757598877, 'learning_rate': 3.105730075625364e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1898, 'grad_norm': 7.94913911819458, 'learning_rate': 3.10500290866783e-05, 'epoch': 1.14}\n",
      "{'loss': 3.0703, 'grad_norm': 8.466941833496094, 'learning_rate': 3.1042757417102967e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1012, 'grad_norm': 9.825114250183105, 'learning_rate': 3.1035485747527633e-05, 'epoch': 1.14}\n",
      "{'loss': 3.0693, 'grad_norm': 8.626572608947754, 'learning_rate': 3.10282140779523e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1148, 'grad_norm': 8.318305015563965, 'learning_rate': 3.102094240837696e-05, 'epoch': 1.14}\n",
      "{'loss': 3.2158, 'grad_norm': 8.316656112670898, 'learning_rate': 3.1013670738801634e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1357, 'grad_norm': 8.089186668395996, 'learning_rate': 3.1006399069226294e-05, 'epoch': 1.14}\n",
      "{'loss': 3.2064, 'grad_norm': 7.667412757873535, 'learning_rate': 3.099912739965096e-05, 'epoch': 1.14}\n",
      "{'loss': 3.2451, 'grad_norm': 8.387910842895508, 'learning_rate': 3.099185573007563e-05, 'epoch': 1.14}\n",
      "{'loss': 3.067, 'grad_norm': 8.574371337890625, 'learning_rate': 3.0984584060500295e-05, 'epoch': 1.14}\n",
      "{'loss': 3.2445, 'grad_norm': 9.007704734802246, 'learning_rate': 3.0977312390924955e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1275, 'grad_norm': 8.4021635055542, 'learning_rate': 3.097004072134962e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1729, 'grad_norm': 11.25324821472168, 'learning_rate': 3.096276905177429e-05, 'epoch': 1.14}\n",
      "{'loss': 3.0566, 'grad_norm': 9.370906829833984, 'learning_rate': 3.0955497382198956e-05, 'epoch': 1.14}\n",
      "{'loss': 3.0697, 'grad_norm': 8.584539413452148, 'learning_rate': 3.0948225712623616e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1322, 'grad_norm': 8.154685974121094, 'learning_rate': 3.094095404304829e-05, 'epoch': 1.14}\n",
      "{'loss': 3.0727, 'grad_norm': 8.380616188049316, 'learning_rate': 3.093368237347295e-05, 'epoch': 1.14}\n",
      "{'loss': 3.2557, 'grad_norm': 8.279070854187012, 'learning_rate': 3.0926410703897617e-05, 'epoch': 1.14}\n",
      "{'loss': 3.1557, 'grad_norm': 8.089568138122559, 'learning_rate': 3.0919139034322283e-05, 'epoch': 1.14}\n",
      "{'loss': 3.115, 'grad_norm': 8.800952911376953, 'learning_rate': 3.091186736474695e-05, 'epoch': 1.15}\n",
      "{'loss': 3.2809, 'grad_norm': 8.848012924194336, 'learning_rate': 3.090459569517161e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1277, 'grad_norm': 9.937344551086426, 'learning_rate': 3.089732402559628e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1826, 'grad_norm': 8.498438835144043, 'learning_rate': 3.0890052356020944e-05, 'epoch': 1.15}\n",
      "{'loss': 3.0459, 'grad_norm': 8.594120025634766, 'learning_rate': 3.088278068644561e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1172, 'grad_norm': 8.504131317138672, 'learning_rate': 3.087550901687027e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1955, 'grad_norm': 8.411706924438477, 'learning_rate': 3.0868237347294945e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1287, 'grad_norm': 9.6392240524292, 'learning_rate': 3.0860965677719605e-05, 'epoch': 1.15}\n",
      "{'loss': 3.2268, 'grad_norm': 8.84835147857666, 'learning_rate': 3.085369400814427e-05, 'epoch': 1.15}\n",
      "{'loss': 3.0838, 'grad_norm': 8.626873970031738, 'learning_rate': 3.084642233856894e-05, 'epoch': 1.15}\n",
      "{'loss': 3.085, 'grad_norm': 9.538296699523926, 'learning_rate': 3.0839150668993606e-05, 'epoch': 1.15}\n",
      "{'loss': 3.0971, 'grad_norm': 7.928422451019287, 'learning_rate': 3.0831878999418266e-05, 'epoch': 1.15}\n",
      "{'loss': 3.2938, 'grad_norm': 7.858686447143555, 'learning_rate': 3.082460732984293e-05, 'epoch': 1.15}\n",
      "{'loss': 3.0158, 'grad_norm': 10.330586433410645, 'learning_rate': 3.08173356602676e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1986, 'grad_norm': 8.716248512268066, 'learning_rate': 3.081006399069226e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1721, 'grad_norm': 10.001714706420898, 'learning_rate': 3.0802792321116933e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1727, 'grad_norm': 9.927385330200195, 'learning_rate': 3.0795520651541594e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1336, 'grad_norm': 7.860605716705322, 'learning_rate': 3.078824898196626e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1262, 'grad_norm': 9.06496524810791, 'learning_rate': 3.078097731239093e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1811, 'grad_norm': 8.25429630279541, 'learning_rate': 3.0773705642815594e-05, 'epoch': 1.15}\n",
      "{'loss': 2.9373, 'grad_norm': 8.359715461730957, 'learning_rate': 3.0766433973240254e-05, 'epoch': 1.15}\n",
      "{'loss': 3.0881, 'grad_norm': 8.843174934387207, 'learning_rate': 3.075916230366492e-05, 'epoch': 1.15}\n",
      "{'loss': 3.1705, 'grad_norm': 7.787889003753662, 'learning_rate': 3.075189063408959e-05, 'epoch': 1.15}\n",
      "{'loss': 3.202, 'grad_norm': 9.00113582611084, 'learning_rate': 3.0744618964514255e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1107, 'grad_norm': 8.275160789489746, 'learning_rate': 3.0737347294938915e-05, 'epoch': 1.16}\n",
      "{'loss': 3.2287, 'grad_norm': 8.571451187133789, 'learning_rate': 3.073007562536359e-05, 'epoch': 1.16}\n",
      " 39%|████████████▋                    | 26500/68760 [4:56:47<7:31:22,  1.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.17s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.875518, 'eval_rouge-2': 8.123466, 'eval_rouge-l': 24.915385999999998, 'eval_bleu-4': 0.03631025912929013, 'eval_runtime': 35.1706, 'eval_samples_per_second': 1.422, 'eval_steps_per_second': 0.114, 'epoch': 1.16}\n",
      " 39%|████████████▋                    | 26500/68760 [4:57:22<7:31:22,  1.56it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:22<00:00,  6.02s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-26500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1211, 'grad_norm': 8.45112419128418, 'learning_rate': 3.072280395578825e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1654, 'grad_norm': 9.091200828552246, 'learning_rate': 3.0715532286212916e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1471, 'grad_norm': 8.595094680786133, 'learning_rate': 3.070826061663758e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1531, 'grad_norm': 8.83153247833252, 'learning_rate': 3.070098894706225e-05, 'epoch': 1.16}\n",
      "{'loss': 3.2398, 'grad_norm': 7.884189605712891, 'learning_rate': 3.069371727748691e-05, 'epoch': 1.16}\n",
      "{'loss': 3.177, 'grad_norm': 9.643299102783203, 'learning_rate': 3.068644560791158e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1078, 'grad_norm': 9.468435287475586, 'learning_rate': 3.0679173938336244e-05, 'epoch': 1.16}\n",
      "{'loss': 3.2484, 'grad_norm': 10.122117042541504, 'learning_rate': 3.067190226876091e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1564, 'grad_norm': 9.1072416305542, 'learning_rate': 3.066463059918557e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1531, 'grad_norm': 7.939150810241699, 'learning_rate': 3.0657358929610244e-05, 'epoch': 1.16}\n",
      "{'loss': 3.0998, 'grad_norm': 9.891499519348145, 'learning_rate': 3.0650087260034904e-05, 'epoch': 1.16}\n",
      "{'loss': 3.0695, 'grad_norm': 8.301471710205078, 'learning_rate': 3.064281559045957e-05, 'epoch': 1.16}\n",
      "{'loss': 3.165, 'grad_norm': 7.737100601196289, 'learning_rate': 3.063554392088424e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1215, 'grad_norm': 8.680887222290039, 'learning_rate': 3.0628272251308905e-05, 'epoch': 1.16}\n",
      "{'loss': 3.201, 'grad_norm': 9.292400360107422, 'learning_rate': 3.0621000581733565e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1707, 'grad_norm': 8.080568313598633, 'learning_rate': 3.061372891215823e-05, 'epoch': 1.16}\n",
      "{'loss': 3.2613, 'grad_norm': 8.497112274169922, 'learning_rate': 3.06064572425829e-05, 'epoch': 1.16}\n",
      "{'loss': 3.0729, 'grad_norm': 9.34305477142334, 'learning_rate': 3.0599185573007566e-05, 'epoch': 1.16}\n",
      "{'loss': 3.267, 'grad_norm': 9.019767761230469, 'learning_rate': 3.0591913903432226e-05, 'epoch': 1.16}\n",
      "{'loss': 3.182, 'grad_norm': 8.652566909790039, 'learning_rate': 3.05846422338569e-05, 'epoch': 1.16}\n",
      "{'loss': 3.1744, 'grad_norm': 8.365321159362793, 'learning_rate': 3.057737056428156e-05, 'epoch': 1.17}\n",
      "{'loss': 3.2463, 'grad_norm': 9.276028633117676, 'learning_rate': 3.057009889470623e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1242, 'grad_norm': 8.59835147857666, 'learning_rate': 3.0562827225130894e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1105, 'grad_norm': 9.069781303405762, 'learning_rate': 3.055555555555556e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1459, 'grad_norm': 9.125289916992188, 'learning_rate': 3.054828388598022e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1141, 'grad_norm': 8.204793930053711, 'learning_rate': 3.054101221640489e-05, 'epoch': 1.17}\n",
      "{'loss': 3.2023, 'grad_norm': 8.466859817504883, 'learning_rate': 3.0533740546829554e-05, 'epoch': 1.17}\n",
      "{'loss': 3.2555, 'grad_norm': 8.815381050109863, 'learning_rate': 3.052646887725422e-05, 'epoch': 1.17}\n",
      "{'loss': 3.2588, 'grad_norm': 8.43506908416748, 'learning_rate': 3.051919720767888e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1629, 'grad_norm': 8.371293067932129, 'learning_rate': 3.0511925538103548e-05, 'epoch': 1.17}\n",
      "{'loss': 3.2156, 'grad_norm': 8.843703269958496, 'learning_rate': 3.0504653868528215e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1383, 'grad_norm': 8.538630485534668, 'learning_rate': 3.049738219895288e-05, 'epoch': 1.17}\n",
      "{'loss': 3.201, 'grad_norm': 11.141315460205078, 'learning_rate': 3.0490110529377546e-05, 'epoch': 1.17}\n",
      "{'loss': 3.0207, 'grad_norm': 7.989269733428955, 'learning_rate': 3.048283885980221e-05, 'epoch': 1.17}\n",
      "{'loss': 3.0898, 'grad_norm': 8.02682113647461, 'learning_rate': 3.047556719022688e-05, 'epoch': 1.17}\n",
      "{'loss': 3.067, 'grad_norm': 8.557290077209473, 'learning_rate': 3.046829552065154e-05, 'epoch': 1.17}\n",
      "{'loss': 3.0205, 'grad_norm': 8.294220924377441, 'learning_rate': 3.046102385107621e-05, 'epoch': 1.17}\n",
      "{'loss': 3.19, 'grad_norm': 7.694112777709961, 'learning_rate': 3.0453752181500873e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1143, 'grad_norm': 8.136879920959473, 'learning_rate': 3.044648051192554e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1479, 'grad_norm': 8.610219955444336, 'learning_rate': 3.0439208842350204e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1975, 'grad_norm': 7.882920742034912, 'learning_rate': 3.043193717277487e-05, 'epoch': 1.17}\n",
      "{'loss': 3.1633, 'grad_norm': 8.173721313476562, 'learning_rate': 3.0424665503199534e-05, 'epoch': 1.17}\n",
      "{'loss': 3.2311, 'grad_norm': 7.857156276702881, 'learning_rate': 3.04173938336242e-05, 'epoch': 1.17}\n",
      "{'loss': 3.102, 'grad_norm': 8.440402030944824, 'learning_rate': 3.0410122164048864e-05, 'epoch': 1.18}\n",
      "{'loss': 3.2219, 'grad_norm': 9.063081741333008, 'learning_rate': 3.0402850494473535e-05, 'epoch': 1.18}\n",
      "{'loss': 3.0764, 'grad_norm': 8.211087226867676, 'learning_rate': 3.0395578824898195e-05, 'epoch': 1.18}\n",
      "{'loss': 3.0646, 'grad_norm': 7.862553119659424, 'learning_rate': 3.0388307155322865e-05, 'epoch': 1.18}\n",
      "{'loss': 3.2174, 'grad_norm': 8.072432518005371, 'learning_rate': 3.038103548574753e-05, 'epoch': 1.18}\n",
      "{'loss': 3.0443, 'grad_norm': 10.132996559143066, 'learning_rate': 3.0373763816172196e-05, 'epoch': 1.18}\n",
      "{'loss': 3.125, 'grad_norm': 8.322938919067383, 'learning_rate': 3.036649214659686e-05, 'epoch': 1.18}\n",
      " 39%|████████████▉                    | 27000/68760 [5:02:30<6:46:31,  1.71it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.22s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.402328000000004, 'eval_rouge-2': 7.318439999999998, 'eval_rouge-l': 24.805775999999994, 'eval_bleu-4': 0.03540330716898604, 'eval_runtime': 17.4971, 'eval_samples_per_second': 2.858, 'eval_steps_per_second': 0.229, 'epoch': 1.18}\n",
      " 39%|████████████▉                    | 27000/68760 [5:02:48<6:46:31,  1.71it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  2.99s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-27000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2193, 'grad_norm': 8.903974533081055, 'learning_rate': 3.0359220477021526e-05, 'epoch': 1.18}\n",
      "{'loss': 3.1145, 'grad_norm': 8.840782165527344, 'learning_rate': 3.035194880744619e-05, 'epoch': 1.18}\n",
      "{'loss': 3.1197, 'grad_norm': 7.963713645935059, 'learning_rate': 3.034467713787086e-05, 'epoch': 1.18}\n",
      "{'loss': 3.1479, 'grad_norm': 9.903555870056152, 'learning_rate': 3.033740546829552e-05, 'epoch': 1.18}\n",
      "{'loss': 3.1557, 'grad_norm': 9.383496284484863, 'learning_rate': 3.033013379872019e-05, 'epoch': 1.18}\n",
      "{'loss': 3.0098, 'grad_norm': 8.639128684997559, 'learning_rate': 3.0322862129144854e-05, 'epoch': 1.18}\n",
      "{'loss': 3.1592, 'grad_norm': 8.961874008178711, 'learning_rate': 3.031559045956952e-05, 'epoch': 1.18}\n",
      "{'loss': 3.1227, 'grad_norm': 9.141550064086914, 'learning_rate': 3.0308318789994184e-05, 'epoch': 1.18}\n",
      "{'loss': 3.227, 'grad_norm': 8.785228729248047, 'learning_rate': 3.030104712041885e-05, 'epoch': 1.18}\n",
      "{'loss': 3.2182, 'grad_norm': 9.2850341796875, 'learning_rate': 3.0293775450843514e-05, 'epoch': 1.18}\n",
      "{'loss': 3.1047, 'grad_norm': 8.959816932678223, 'learning_rate': 3.028650378126818e-05, 'epoch': 1.18}\n",
      "{'loss': 3.177, 'grad_norm': 8.134343147277832, 'learning_rate': 3.0279232111692845e-05, 'epoch': 1.18}\n",
      "{'loss': 3.101, 'grad_norm': 8.499496459960938, 'learning_rate': 3.0271960442117515e-05, 'epoch': 1.18}\n",
      "{'loss': 3.2955, 'grad_norm': 8.210349082946777, 'learning_rate': 3.0264688772542175e-05, 'epoch': 1.18}\n",
      "{'loss': 3.127, 'grad_norm': 8.211102485656738, 'learning_rate': 3.0257417102966846e-05, 'epoch': 1.18}\n",
      "{'loss': 3.3318, 'grad_norm': 8.974435806274414, 'learning_rate': 3.025014543339151e-05, 'epoch': 1.18}\n",
      "{'loss': 2.859, 'grad_norm': 8.511687278747559, 'learning_rate': 3.0242873763816176e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1846, 'grad_norm': 8.525845527648926, 'learning_rate': 3.023560209424084e-05, 'epoch': 1.19}\n",
      "{'loss': 2.9832, 'grad_norm': 8.226161003112793, 'learning_rate': 3.0228330424665503e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0863, 'grad_norm': 9.891919136047363, 'learning_rate': 3.022105875509017e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1244, 'grad_norm': 9.533313751220703, 'learning_rate': 3.0213787085514833e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0701, 'grad_norm': 8.708754539489746, 'learning_rate': 3.02065154159395e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1535, 'grad_norm': 8.52708625793457, 'learning_rate': 3.0199243746364164e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1437, 'grad_norm': 9.036510467529297, 'learning_rate': 3.0191972076788834e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1416, 'grad_norm': 8.331254005432129, 'learning_rate': 3.0184700407213494e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0668, 'grad_norm': 10.174165725708008, 'learning_rate': 3.0177428737638164e-05, 'epoch': 1.19}\n",
      "{'loss': 3.2051, 'grad_norm': 9.419986724853516, 'learning_rate': 3.0170157068062828e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0643, 'grad_norm': 8.320937156677246, 'learning_rate': 3.0162885398487495e-05, 'epoch': 1.19}\n",
      "{'loss': 3.2896, 'grad_norm': 8.717225074768066, 'learning_rate': 3.015561372891216e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1506, 'grad_norm': 8.635077476501465, 'learning_rate': 3.0148342059336825e-05, 'epoch': 1.19}\n",
      "{'loss': 3.2088, 'grad_norm': 8.173141479492188, 'learning_rate': 3.014107038976149e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1904, 'grad_norm': 8.341059684753418, 'learning_rate': 3.0133798720186156e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1846, 'grad_norm': 9.214815139770508, 'learning_rate': 3.012652705061082e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0943, 'grad_norm': 9.217279434204102, 'learning_rate': 3.011925538103549e-05, 'epoch': 1.19}\n",
      "{'loss': 3.2414, 'grad_norm': 9.935723304748535, 'learning_rate': 3.011198371146015e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0992, 'grad_norm': 9.007061958312988, 'learning_rate': 3.010471204188482e-05, 'epoch': 1.19}\n",
      "{'loss': 3.2092, 'grad_norm': 9.09979248046875, 'learning_rate': 3.0097440372309483e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1189, 'grad_norm': 8.333052635192871, 'learning_rate': 3.009016870273415e-05, 'epoch': 1.19}\n",
      "{'loss': 3.1154, 'grad_norm': 8.408803939819336, 'learning_rate': 3.0082897033158814e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1232, 'grad_norm': 8.385496139526367, 'learning_rate': 3.007562536358348e-05, 'epoch': 1.2}\n",
      "{'loss': 3.0773, 'grad_norm': 8.512747764587402, 'learning_rate': 3.0068353694008144e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1826, 'grad_norm': 8.591079711914062, 'learning_rate': 3.0061082024432814e-05, 'epoch': 1.2}\n",
      "{'loss': 3.2453, 'grad_norm': 8.223448753356934, 'learning_rate': 3.0053810354857475e-05, 'epoch': 1.2}\n",
      "{'loss': 3.3256, 'grad_norm': 9.022480964660645, 'learning_rate': 3.0046538685282145e-05, 'epoch': 1.2}\n",
      "{'loss': 3.199, 'grad_norm': 9.447504043579102, 'learning_rate': 3.003926701570681e-05, 'epoch': 1.2}\n",
      "{'loss': 2.9279, 'grad_norm': 8.323596000671387, 'learning_rate': 3.0031995346131475e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1984, 'grad_norm': 8.587873458862305, 'learning_rate': 3.002472367655614e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1057, 'grad_norm': 8.373387336730957, 'learning_rate': 3.0017452006980806e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1975, 'grad_norm': 8.552750587463379, 'learning_rate': 3.001018033740547e-05, 'epoch': 1.2}\n",
      "{'loss': 3.3107, 'grad_norm': 8.024449348449707, 'learning_rate': 3.0002908667830136e-05, 'epoch': 1.2}\n",
      " 40%|█████████████▏                   | 27500/68760 [5:07:57<6:56:53,  1.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.05s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.25s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.985486, 'eval_rouge-2': 8.331754000000002, 'eval_rouge-l': 26.140362000000003, 'eval_bleu-4': 0.03990460118625056, 'eval_runtime': 10.4439, 'eval_samples_per_second': 4.787, 'eval_steps_per_second': 0.383, 'epoch': 1.2}\n",
      " 40%|█████████████▏                   | 27500/68760 [5:08:08<6:56:53,  1.65it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  2.51s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-27500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1439, 'grad_norm': 8.256341934204102, 'learning_rate': 2.99956369982548e-05, 'epoch': 1.2}\n",
      "{'loss': 3.2219, 'grad_norm': 8.531989097595215, 'learning_rate': 2.998836532867947e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1514, 'grad_norm': 8.043118476867676, 'learning_rate': 2.998109365910413e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1361, 'grad_norm': 8.84052562713623, 'learning_rate': 2.99738219895288e-05, 'epoch': 1.2}\n",
      "{'loss': 3.0096, 'grad_norm': 8.191699981689453, 'learning_rate': 2.9966550319953464e-05, 'epoch': 1.2}\n",
      "{'loss': 3.0998, 'grad_norm': 8.707243919372559, 'learning_rate': 2.995927865037813e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1938, 'grad_norm': 8.178421020507812, 'learning_rate': 2.9952006980802794e-05, 'epoch': 1.2}\n",
      "{'loss': 3.0186, 'grad_norm': 8.8512601852417, 'learning_rate': 2.994473531122746e-05, 'epoch': 1.2}\n",
      "{'loss': 3.1221, 'grad_norm': 9.10998249053955, 'learning_rate': 2.9937463641652125e-05, 'epoch': 1.2}\n",
      "{'loss': 3.0258, 'grad_norm': 8.534374237060547, 'learning_rate': 2.9930191972076788e-05, 'epoch': 1.2}\n",
      "{'loss': 3.191, 'grad_norm': 8.957968711853027, 'learning_rate': 2.9922920302501455e-05, 'epoch': 1.2}\n",
      "{'loss': 3.0684, 'grad_norm': 8.381637573242188, 'learning_rate': 2.991564863292612e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1293, 'grad_norm': 8.919351577758789, 'learning_rate': 2.990837696335079e-05, 'epoch': 1.21}\n",
      "{'loss': 3.2891, 'grad_norm': 8.17531967163086, 'learning_rate': 2.990110529377545e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1732, 'grad_norm': 8.52224063873291, 'learning_rate': 2.989383362420012e-05, 'epoch': 1.21}\n",
      "{'loss': 3.0842, 'grad_norm': 8.93917465209961, 'learning_rate': 2.9886561954624783e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1479, 'grad_norm': 8.811896324157715, 'learning_rate': 2.987929028504945e-05, 'epoch': 1.21}\n",
      "{'loss': 3.0812, 'grad_norm': 8.806097984313965, 'learning_rate': 2.9872018615474113e-05, 'epoch': 1.21}\n",
      "{'loss': 3.26, 'grad_norm': 7.896914005279541, 'learning_rate': 2.986474694589878e-05, 'epoch': 1.21}\n",
      "{'loss': 3.2227, 'grad_norm': 8.82071304321289, 'learning_rate': 2.9857475276323443e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1207, 'grad_norm': 8.714516639709473, 'learning_rate': 2.985020360674811e-05, 'epoch': 1.21}\n",
      "{'loss': 3.143, 'grad_norm': 8.05774211883545, 'learning_rate': 2.9842931937172774e-05, 'epoch': 1.21}\n",
      "{'loss': 3.0846, 'grad_norm': 9.320826530456543, 'learning_rate': 2.9835660267597444e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1805, 'grad_norm': 7.820530414581299, 'learning_rate': 2.9828388598022104e-05, 'epoch': 1.21}\n",
      "{'loss': 3.2604, 'grad_norm': 9.050859451293945, 'learning_rate': 2.9821116928446775e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1117, 'grad_norm': 8.697827339172363, 'learning_rate': 2.9813845258871438e-05, 'epoch': 1.21}\n",
      "{'loss': 3.0508, 'grad_norm': 8.421032905578613, 'learning_rate': 2.9806573589296105e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1645, 'grad_norm': 8.944842338562012, 'learning_rate': 2.979930191972077e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1176, 'grad_norm': 7.860266208648682, 'learning_rate': 2.9792030250145435e-05, 'epoch': 1.21}\n",
      "{'loss': 3.2553, 'grad_norm': 8.576343536376953, 'learning_rate': 2.97847585805701e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1805, 'grad_norm': 8.985112190246582, 'learning_rate': 2.9777486910994766e-05, 'epoch': 1.21}\n",
      "{'loss': 3.0428, 'grad_norm': 9.793519973754883, 'learning_rate': 2.977021524141943e-05, 'epoch': 1.21}\n",
      "{'loss': 3.1752, 'grad_norm': 8.623175621032715, 'learning_rate': 2.97629435718441e-05, 'epoch': 1.21}\n",
      "{'loss': 3.2508, 'grad_norm': 9.449326515197754, 'learning_rate': 2.975567190226876e-05, 'epoch': 1.21}\n",
      "{'loss': 3.3191, 'grad_norm': 10.08148193359375, 'learning_rate': 2.974840023269343e-05, 'epoch': 1.22}\n",
      "{'loss': 3.2422, 'grad_norm': 9.408531188964844, 'learning_rate': 2.9741128563118093e-05, 'epoch': 1.22}\n",
      "{'loss': 3.0398, 'grad_norm': 9.949889183044434, 'learning_rate': 2.973385689354276e-05, 'epoch': 1.22}\n",
      "{'loss': 3.1393, 'grad_norm': 8.363719940185547, 'learning_rate': 2.9726585223967424e-05, 'epoch': 1.22}\n",
      "{'loss': 3.1771, 'grad_norm': 8.673408508300781, 'learning_rate': 2.971931355439209e-05, 'epoch': 1.22}\n",
      "{'loss': 3.0414, 'grad_norm': 8.43618392944336, 'learning_rate': 2.9712041884816754e-05, 'epoch': 1.22}\n",
      "{'loss': 3.118, 'grad_norm': 7.999131202697754, 'learning_rate': 2.9704770215241425e-05, 'epoch': 1.22}\n",
      "{'loss': 3.1561, 'grad_norm': 8.346574783325195, 'learning_rate': 2.9697498545666085e-05, 'epoch': 1.22}\n",
      "{'loss': 3.1199, 'grad_norm': 9.33698558807373, 'learning_rate': 2.9690226876090755e-05, 'epoch': 1.22}\n",
      "{'loss': 3.3188, 'grad_norm': 9.328497886657715, 'learning_rate': 2.968295520651542e-05, 'epoch': 1.22}\n",
      "{'loss': 3.1129, 'grad_norm': 8.8090181350708, 'learning_rate': 2.9675683536940085e-05, 'epoch': 1.22}\n",
      "{'loss': 3.1172, 'grad_norm': 8.920844078063965, 'learning_rate': 2.966841186736475e-05, 'epoch': 1.22}\n",
      "{'loss': 2.9318, 'grad_norm': 8.363761901855469, 'learning_rate': 2.9661140197789416e-05, 'epoch': 1.22}\n",
      "{'loss': 3.2215, 'grad_norm': 8.48564624786377, 'learning_rate': 2.965386852821408e-05, 'epoch': 1.22}\n",
      "{'loss': 3.1217, 'grad_norm': 8.347701072692871, 'learning_rate': 2.9646596858638743e-05, 'epoch': 1.22}\n",
      "{'loss': 3.0955, 'grad_norm': 8.760443687438965, 'learning_rate': 2.963932518906341e-05, 'epoch': 1.22}\n",
      " 41%|█████████████▍                   | 28000/68760 [5:13:18<6:53:08,  1.64it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.21s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.66s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.091842, 'eval_rouge-2': 7.710204, 'eval_rouge-l': 24.695214, 'eval_bleu-4': 0.034154518700229654, 'eval_runtime': 28.1771, 'eval_samples_per_second': 1.774, 'eval_steps_per_second': 0.142, 'epoch': 1.22}\n",
      " 41%|█████████████▍                   | 28000/68760 [5:13:46<6:53:08,  1.64it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  4.10s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-28000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1211, 'grad_norm': 8.991827011108398, 'learning_rate': 2.9632053519488073e-05, 'epoch': 1.22}\n",
      "{'loss': 3.1338, 'grad_norm': 8.726675033569336, 'learning_rate': 2.962478184991274e-05, 'epoch': 1.22}\n",
      "{'loss': 3.2139, 'grad_norm': 8.345657348632812, 'learning_rate': 2.9617510180337404e-05, 'epoch': 1.22}\n",
      "{'loss': 3.2484, 'grad_norm': 8.703309059143066, 'learning_rate': 2.9610238510762074e-05, 'epoch': 1.22}\n",
      "{'loss': 3.127, 'grad_norm': 8.052922248840332, 'learning_rate': 2.9602966841186734e-05, 'epoch': 1.22}\n",
      "{'loss': 2.9844, 'grad_norm': 7.938387870788574, 'learning_rate': 2.9595695171611404e-05, 'epoch': 1.22}\n",
      "{'loss': 3.099, 'grad_norm': 9.06533432006836, 'learning_rate': 2.9588423502036068e-05, 'epoch': 1.22}\n",
      "{'loss': 3.3213, 'grad_norm': 8.554598808288574, 'learning_rate': 2.9581151832460735e-05, 'epoch': 1.23}\n",
      "{'loss': 3.0746, 'grad_norm': 8.008451461791992, 'learning_rate': 2.9573880162885398e-05, 'epoch': 1.23}\n",
      "{'loss': 2.9053, 'grad_norm': 8.583542823791504, 'learning_rate': 2.9566608493310065e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1805, 'grad_norm': 8.772324562072754, 'learning_rate': 2.955933682373473e-05, 'epoch': 1.23}\n",
      "{'loss': 3.132, 'grad_norm': 8.600882530212402, 'learning_rate': 2.95520651541594e-05, 'epoch': 1.23}\n",
      "{'loss': 3.2941, 'grad_norm': 9.05580997467041, 'learning_rate': 2.954479348458406e-05, 'epoch': 1.23}\n",
      "{'loss': 3.0961, 'grad_norm': 8.467686653137207, 'learning_rate': 2.953752181500873e-05, 'epoch': 1.23}\n",
      "{'loss': 3.2293, 'grad_norm': 8.820762634277344, 'learning_rate': 2.9530250145433393e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1494, 'grad_norm': 9.244622230529785, 'learning_rate': 2.952297847585806e-05, 'epoch': 1.23}\n",
      "{'loss': 3.0723, 'grad_norm': 8.497062683105469, 'learning_rate': 2.9515706806282723e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1676, 'grad_norm': 8.96127986907959, 'learning_rate': 2.950843513670739e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1656, 'grad_norm': 8.974983215332031, 'learning_rate': 2.9501163467132054e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1133, 'grad_norm': 7.517065048217773, 'learning_rate': 2.949389179755672e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1682, 'grad_norm': 9.027360916137695, 'learning_rate': 2.9486620127981384e-05, 'epoch': 1.23}\n",
      "{'loss': 3.157, 'grad_norm': 8.622472763061523, 'learning_rate': 2.9479348458406054e-05, 'epoch': 1.23}\n",
      "{'loss': 3.0992, 'grad_norm': 7.797355651855469, 'learning_rate': 2.9472076788830714e-05, 'epoch': 1.23}\n",
      "{'loss': 3.0518, 'grad_norm': 8.276999473571777, 'learning_rate': 2.9464805119255385e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1094, 'grad_norm': 8.189592361450195, 'learning_rate': 2.9457533449680048e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1988, 'grad_norm': 7.837403774261475, 'learning_rate': 2.9450261780104715e-05, 'epoch': 1.23}\n",
      "{'loss': 3.157, 'grad_norm': 7.9924845695495605, 'learning_rate': 2.944299011052938e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1975, 'grad_norm': 8.166119575500488, 'learning_rate': 2.9435718440954045e-05, 'epoch': 1.23}\n",
      "{'loss': 3.1646, 'grad_norm': 8.3828763961792, 'learning_rate': 2.942844677137871e-05, 'epoch': 1.23}\n",
      "{'loss': 3.015, 'grad_norm': 8.491689682006836, 'learning_rate': 2.942117510180338e-05, 'epoch': 1.23}\n",
      "{'loss': 3.3014, 'grad_norm': 8.744601249694824, 'learning_rate': 2.941390343222804e-05, 'epoch': 1.24}\n",
      "{'loss': 3.0879, 'grad_norm': 8.884367942810059, 'learning_rate': 2.940663176265271e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2549, 'grad_norm': 8.930731773376465, 'learning_rate': 2.9399360093077373e-05, 'epoch': 1.24}\n",
      "{'loss': 3.1221, 'grad_norm': 8.418478012084961, 'learning_rate': 2.939208842350204e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2385, 'grad_norm': 9.286792755126953, 'learning_rate': 2.9384816753926704e-05, 'epoch': 1.24}\n",
      "{'loss': 3.1146, 'grad_norm': 9.45289134979248, 'learning_rate': 2.937754508435137e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2049, 'grad_norm': 9.173514366149902, 'learning_rate': 2.9370273414776034e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2213, 'grad_norm': 10.386663436889648, 'learning_rate': 2.93630017452007e-05, 'epoch': 1.24}\n",
      "{'loss': 3.168, 'grad_norm': 8.934828758239746, 'learning_rate': 2.9355730075625364e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2381, 'grad_norm': 8.485557556152344, 'learning_rate': 2.9348458406050028e-05, 'epoch': 1.24}\n",
      "{'loss': 3.1637, 'grad_norm': 8.73209285736084, 'learning_rate': 2.9341186736474695e-05, 'epoch': 1.24}\n",
      "{'loss': 3.1182, 'grad_norm': 9.618294715881348, 'learning_rate': 2.9333915066899358e-05, 'epoch': 1.24}\n",
      "{'loss': 3.1092, 'grad_norm': 13.956551551818848, 'learning_rate': 2.932664339732403e-05, 'epoch': 1.24}\n",
      "{'loss': 3.0969, 'grad_norm': 8.864660263061523, 'learning_rate': 2.931937172774869e-05, 'epoch': 1.24}\n",
      "{'loss': 3.1914, 'grad_norm': 8.596961975097656, 'learning_rate': 2.931210005817336e-05, 'epoch': 1.24}\n",
      "{'loss': 3.0998, 'grad_norm': 8.324222564697266, 'learning_rate': 2.9304828388598022e-05, 'epoch': 1.24}\n",
      "{'loss': 3.1965, 'grad_norm': 7.5609025955200195, 'learning_rate': 2.929755671902269e-05, 'epoch': 1.24}\n",
      "{'loss': 3.1246, 'grad_norm': 9.318721771240234, 'learning_rate': 2.9290285049447353e-05, 'epoch': 1.24}\n",
      "{'loss': 2.9959, 'grad_norm': 9.578587532043457, 'learning_rate': 2.928301337987202e-05, 'epoch': 1.24}\n",
      "{'loss': 3.0809, 'grad_norm': 9.126482009887695, 'learning_rate': 2.9275741710296683e-05, 'epoch': 1.24}\n",
      " 41%|█████████████▋                   | 28500/68760 [5:18:56<6:49:32,  1.64it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.03it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.26s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.910984000000006, 'eval_rouge-2': 8.192992, 'eval_rouge-l': 26.229338, 'eval_bleu-4': 0.03715129396120692, 'eval_runtime': 6.2299, 'eval_samples_per_second': 8.026, 'eval_steps_per_second': 0.642, 'epoch': 1.24}\n",
      " 41%|█████████████▋                   | 28500/68760 [5:19:02<6:49:32,  1.64it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.13s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-28500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0801, 'grad_norm': 9.229636192321777, 'learning_rate': 2.9268470040721354e-05, 'epoch': 1.24}\n",
      "{'loss': 3.11, 'grad_norm': 8.849518775939941, 'learning_rate': 2.9261198371146014e-05, 'epoch': 1.24}\n",
      "{'loss': 3.2078, 'grad_norm': 9.940533638000488, 'learning_rate': 2.9253926701570684e-05, 'epoch': 1.24}\n",
      "{'loss': 3.0965, 'grad_norm': 8.685866355895996, 'learning_rate': 2.9246655031995347e-05, 'epoch': 1.25}\n",
      "{'loss': 2.9705, 'grad_norm': 9.56650447845459, 'learning_rate': 2.9239383362420014e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0285, 'grad_norm': 8.143230438232422, 'learning_rate': 2.9232111692844678e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0908, 'grad_norm': 9.612092018127441, 'learning_rate': 2.9224840023269345e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0209, 'grad_norm': 8.983379364013672, 'learning_rate': 2.9217568353694008e-05, 'epoch': 1.25}\n",
      "{'loss': 3.1604, 'grad_norm': 8.978453636169434, 'learning_rate': 2.9210296684118675e-05, 'epoch': 1.25}\n",
      "{'loss': 3.2814, 'grad_norm': 8.901095390319824, 'learning_rate': 2.920302501454334e-05, 'epoch': 1.25}\n",
      "{'loss': 3.1449, 'grad_norm': 9.703779220581055, 'learning_rate': 2.919575334496801e-05, 'epoch': 1.25}\n",
      "{'loss': 3.2672, 'grad_norm': 8.936591148376465, 'learning_rate': 2.918848167539267e-05, 'epoch': 1.25}\n",
      "{'loss': 3.1674, 'grad_norm': 8.05313777923584, 'learning_rate': 2.918121000581734e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0035, 'grad_norm': 7.831406593322754, 'learning_rate': 2.9173938336242003e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0816, 'grad_norm': 7.97512149810791, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n",
      "{'loss': 3.1221, 'grad_norm': 9.222521781921387, 'learning_rate': 2.9159394997091333e-05, 'epoch': 1.25}\n",
      "{'loss': 3.3254, 'grad_norm': 8.80849838256836, 'learning_rate': 2.9152123327516e-05, 'epoch': 1.25}\n",
      "{'loss': 2.9604, 'grad_norm': 8.388254165649414, 'learning_rate': 2.9144851657940664e-05, 'epoch': 1.25}\n",
      "{'loss': 3.1039, 'grad_norm': 10.026642799377441, 'learning_rate': 2.913757998836533e-05, 'epoch': 1.25}\n",
      "{'loss': 3.19, 'grad_norm': 8.35643196105957, 'learning_rate': 2.9130308318789994e-05, 'epoch': 1.25}\n",
      "{'loss': 3.108, 'grad_norm': 8.810976028442383, 'learning_rate': 2.9123036649214664e-05, 'epoch': 1.25}\n",
      "{'loss': 3.1654, 'grad_norm': 8.645103454589844, 'learning_rate': 2.9115764979639324e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0334, 'grad_norm': 9.005330085754395, 'learning_rate': 2.9108493310063995e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0846, 'grad_norm': 9.682640075683594, 'learning_rate': 2.9101221640488658e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0922, 'grad_norm': 8.673519134521484, 'learning_rate': 2.9093949970913325e-05, 'epoch': 1.25}\n",
      "{'loss': 3.1324, 'grad_norm': 8.105881690979004, 'learning_rate': 2.908667830133799e-05, 'epoch': 1.25}\n",
      "{'loss': 3.0473, 'grad_norm': 8.772095680236816, 'learning_rate': 2.9079406631762656e-05, 'epoch': 1.26}\n",
      "{'loss': 2.999, 'grad_norm': 8.704320907592773, 'learning_rate': 2.907213496218732e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2949, 'grad_norm': 9.791708946228027, 'learning_rate': 2.9064863292611983e-05, 'epoch': 1.26}\n",
      "{'loss': 3.0926, 'grad_norm': 8.431543350219727, 'learning_rate': 2.905759162303665e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2514, 'grad_norm': 8.719588279724121, 'learning_rate': 2.9050319953461313e-05, 'epoch': 1.26}\n",
      "{'loss': 3.1154, 'grad_norm': 8.000182151794434, 'learning_rate': 2.9043048283885983e-05, 'epoch': 1.26}\n",
      "{'loss': 3.091, 'grad_norm': 9.110058784484863, 'learning_rate': 2.9035776614310643e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2953, 'grad_norm': 8.74045467376709, 'learning_rate': 2.9028504944735314e-05, 'epoch': 1.26}\n",
      "{'loss': 2.9883, 'grad_norm': 8.820484161376953, 'learning_rate': 2.9021233275159977e-05, 'epoch': 1.26}\n",
      "{'loss': 3.0932, 'grad_norm': 8.816519737243652, 'learning_rate': 2.9013961605584644e-05, 'epoch': 1.26}\n",
      "{'loss': 3.1633, 'grad_norm': 8.434082984924316, 'learning_rate': 2.9006689936009308e-05, 'epoch': 1.26}\n",
      "{'loss': 3.1213, 'grad_norm': 7.82573127746582, 'learning_rate': 2.8999418266433974e-05, 'epoch': 1.26}\n",
      "{'loss': 3.0795, 'grad_norm': 8.211356163024902, 'learning_rate': 2.8992146596858638e-05, 'epoch': 1.26}\n",
      "{'loss': 3.0139, 'grad_norm': 8.250472068786621, 'learning_rate': 2.8984874927283305e-05, 'epoch': 1.26}\n",
      "{'loss': 3.0283, 'grad_norm': 8.057022094726562, 'learning_rate': 2.897760325770797e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2678, 'grad_norm': 8.90545654296875, 'learning_rate': 2.897033158813264e-05, 'epoch': 1.26}\n",
      "{'loss': 3.1947, 'grad_norm': 8.469709396362305, 'learning_rate': 2.89630599185573e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2275, 'grad_norm': 9.744342803955078, 'learning_rate': 2.895578824898197e-05, 'epoch': 1.26}\n",
      "{'loss': 3.1707, 'grad_norm': 9.620217323303223, 'learning_rate': 2.8948516579406633e-05, 'epoch': 1.26}\n",
      "{'loss': 3.177, 'grad_norm': 9.223281860351562, 'learning_rate': 2.89412449098313e-05, 'epoch': 1.26}\n",
      "{'loss': 3.2012, 'grad_norm': 9.276947975158691, 'learning_rate': 2.8933973240255963e-05, 'epoch': 1.26}\n",
      "{'loss': 3.0395, 'grad_norm': 8.6580171585083, 'learning_rate': 2.892670157068063e-05, 'epoch': 1.26}\n",
      "{'loss': 3.059, 'grad_norm': 8.330517768859863, 'learning_rate': 2.8919429901105293e-05, 'epoch': 1.26}\n",
      "{'loss': 3.0779, 'grad_norm': 9.381836891174316, 'learning_rate': 2.8912158231529964e-05, 'epoch': 1.27}\n",
      " 42%|█████████████▉                   | 29000/68760 [5:24:10<6:50:35,  1.61it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.02it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.32s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.98041, 'eval_rouge-2': 7.361203999999999, 'eval_rouge-l': 25.863386000000006, 'eval_bleu-4': 0.036784784547915804, 'eval_runtime': 17.578, 'eval_samples_per_second': 2.844, 'eval_steps_per_second': 0.228, 'epoch': 1.27}\n",
      " 42%|█████████████▉                   | 29000/68760 [5:24:27<6:50:35,  1.61it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.37s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-29000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1604, 'grad_norm': 7.728908061981201, 'learning_rate': 2.8904886561954624e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1799, 'grad_norm': 8.720551490783691, 'learning_rate': 2.8897614892379294e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1246, 'grad_norm': 8.388818740844727, 'learning_rate': 2.8890343222803958e-05, 'epoch': 1.27}\n",
      "{'loss': 3.183, 'grad_norm': 8.555887222290039, 'learning_rate': 2.8883071553228624e-05, 'epoch': 1.27}\n",
      "{'loss': 3.2543, 'grad_norm': 10.041836738586426, 'learning_rate': 2.8875799883653288e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1738, 'grad_norm': 8.784666061401367, 'learning_rate': 2.8868528214077955e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1281, 'grad_norm': 8.082513809204102, 'learning_rate': 2.886125654450262e-05, 'epoch': 1.27}\n",
      "{'loss': 3.141, 'grad_norm': 8.244446754455566, 'learning_rate': 2.8853984874927285e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1539, 'grad_norm': 8.589558601379395, 'learning_rate': 2.884671320535195e-05, 'epoch': 1.27}\n",
      "{'loss': 2.9395, 'grad_norm': 8.677797317504883, 'learning_rate': 2.883944153577662e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1057, 'grad_norm': 8.091522216796875, 'learning_rate': 2.883216986620128e-05, 'epoch': 1.27}\n",
      "{'loss': 3.2004, 'grad_norm': 8.581092834472656, 'learning_rate': 2.882489819662595e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1691, 'grad_norm': 8.782665252685547, 'learning_rate': 2.8817626527050613e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1146, 'grad_norm': 8.213445663452148, 'learning_rate': 2.881035485747528e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1846, 'grad_norm': 8.527215957641602, 'learning_rate': 2.8803083187899943e-05, 'epoch': 1.27}\n",
      "{'loss': 3.0725, 'grad_norm': 8.633840560913086, 'learning_rate': 2.879581151832461e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1617, 'grad_norm': 8.508037567138672, 'learning_rate': 2.8788539848749274e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1029, 'grad_norm': 8.556882858276367, 'learning_rate': 2.8781268179173944e-05, 'epoch': 1.27}\n",
      "{'loss': 3.2049, 'grad_norm': 8.972868919372559, 'learning_rate': 2.8773996509598604e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1777, 'grad_norm': 7.973196506500244, 'learning_rate': 2.8766724840023268e-05, 'epoch': 1.27}\n",
      "{'loss': 3.0891, 'grad_norm': 8.060123443603516, 'learning_rate': 2.8759453170447938e-05, 'epoch': 1.27}\n",
      "{'loss': 3.1795, 'grad_norm': 10.198872566223145, 'learning_rate': 2.8752181500872598e-05, 'epoch': 1.27}\n",
      "{'loss': 3.0725, 'grad_norm': 8.747315406799316, 'learning_rate': 2.874490983129727e-05, 'epoch': 1.28}\n",
      "{'loss': 3.109, 'grad_norm': 9.082053184509277, 'learning_rate': 2.8737638161721932e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0127, 'grad_norm': 8.879305839538574, 'learning_rate': 2.87303664921466e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0887, 'grad_norm': 8.781014442443848, 'learning_rate': 2.8723094822571262e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0744, 'grad_norm': 9.274027824401855, 'learning_rate': 2.871582315299593e-05, 'epoch': 1.28}\n",
      "{'loss': 3.2359, 'grad_norm': 8.018682479858398, 'learning_rate': 2.8708551483420593e-05, 'epoch': 1.28}\n",
      "{'loss': 3.2381, 'grad_norm': 8.60247802734375, 'learning_rate': 2.870127981384526e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0572, 'grad_norm': 9.482771873474121, 'learning_rate': 2.8694008144269923e-05, 'epoch': 1.28}\n",
      "{'loss': 3.1104, 'grad_norm': 8.732314109802246, 'learning_rate': 2.8686736474694593e-05, 'epoch': 1.28}\n",
      "{'loss': 3.1887, 'grad_norm': 9.135965347290039, 'learning_rate': 2.8679464805119253e-05, 'epoch': 1.28}\n",
      "{'loss': 3.201, 'grad_norm': 8.08017349243164, 'learning_rate': 2.8672193135543924e-05, 'epoch': 1.28}\n",
      "{'loss': 3.2363, 'grad_norm': 7.85793924331665, 'learning_rate': 2.8664921465968587e-05, 'epoch': 1.28}\n",
      "{'loss': 3.2221, 'grad_norm': 9.404497146606445, 'learning_rate': 2.8657649796393254e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0377, 'grad_norm': 8.662437438964844, 'learning_rate': 2.8650378126817918e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0637, 'grad_norm': 8.581071853637695, 'learning_rate': 2.8643106457242585e-05, 'epoch': 1.28}\n",
      "{'loss': 3.132, 'grad_norm': 9.225240707397461, 'learning_rate': 2.8635834787667248e-05, 'epoch': 1.28}\n",
      "{'loss': 3.1836, 'grad_norm': 8.195633888244629, 'learning_rate': 2.862856311809192e-05, 'epoch': 1.28}\n",
      "{'loss': 3.3408, 'grad_norm': 9.156872749328613, 'learning_rate': 2.862129144851658e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0514, 'grad_norm': 8.442574501037598, 'learning_rate': 2.861401977894125e-05, 'epoch': 1.28}\n",
      "{'loss': 3.1154, 'grad_norm': 9.54942512512207, 'learning_rate': 2.8606748109365912e-05, 'epoch': 1.28}\n",
      "{'loss': 3.1588, 'grad_norm': 8.474605560302734, 'learning_rate': 2.859947643979058e-05, 'epoch': 1.28}\n",
      "{'loss': 3.107, 'grad_norm': 8.509048461914062, 'learning_rate': 2.8592204770215243e-05, 'epoch': 1.28}\n",
      "{'loss': 3.049, 'grad_norm': 8.656373023986816, 'learning_rate': 2.858493310063991e-05, 'epoch': 1.28}\n",
      "{'loss': 3.0182, 'grad_norm': 9.220564842224121, 'learning_rate': 2.8577661431064573e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1033, 'grad_norm': 8.980884552001953, 'learning_rate': 2.857038976148924e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1098, 'grad_norm': 7.711849689483643, 'learning_rate': 2.8563118091913903e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1395, 'grad_norm': 8.141913414001465, 'learning_rate': 2.8555846422338574e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1861, 'grad_norm': 8.540736198425293, 'learning_rate': 2.8548574752763234e-05, 'epoch': 1.29}\n",
      " 43%|██████████████▏                  | 29500/68760 [5:29:37<6:19:27,  1.72it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.03it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.31s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.703612, 'eval_rouge-2': 8.697686000000001, 'eval_rouge-l': 26.726246000000007, 'eval_bleu-4': 0.04282308710932659, 'eval_runtime': 17.4083, 'eval_samples_per_second': 2.872, 'eval_steps_per_second': 0.23, 'epoch': 1.29}\n",
      " 43%|██████████████▏                  | 29500/68760 [5:29:54<6:19:27,  1.72it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.30s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-29500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.066, 'grad_norm': 8.492959022521973, 'learning_rate': 2.8541303083187904e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1453, 'grad_norm': 9.325565338134766, 'learning_rate': 2.8534031413612568e-05, 'epoch': 1.29}\n",
      "{'loss': 3.0432, 'grad_norm': 8.786806106567383, 'learning_rate': 2.8526759744037235e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1629, 'grad_norm': 9.450418472290039, 'learning_rate': 2.8519488074461898e-05, 'epoch': 1.29}\n",
      "{'loss': 3.0479, 'grad_norm': 7.842670440673828, 'learning_rate': 2.8512216404886565e-05, 'epoch': 1.29}\n",
      "{'loss': 3.0225, 'grad_norm': 8.498801231384277, 'learning_rate': 2.850494473531123e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1998, 'grad_norm': 9.034674644470215, 'learning_rate': 2.8497673065735895e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1051, 'grad_norm': 8.700932502746582, 'learning_rate': 2.849040139616056e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1084, 'grad_norm': 9.479512214660645, 'learning_rate': 2.8483129726585222e-05, 'epoch': 1.29}\n",
      "{'loss': 3.092, 'grad_norm': 8.452860832214355, 'learning_rate': 2.847585805700989e-05, 'epoch': 1.29}\n",
      "{'loss': 3.0199, 'grad_norm': 8.533245086669922, 'learning_rate': 2.8468586387434553e-05, 'epoch': 1.29}\n",
      "{'loss': 3.09, 'grad_norm': 8.743856430053711, 'learning_rate': 2.8461314717859223e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1437, 'grad_norm': 8.48118782043457, 'learning_rate': 2.8454043048283887e-05, 'epoch': 1.29}\n",
      "{'loss': 3.0688, 'grad_norm': 7.919100761413574, 'learning_rate': 2.8446771378708553e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1486, 'grad_norm': 9.37233829498291, 'learning_rate': 2.8439499709133217e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1684, 'grad_norm': 8.377388000488281, 'learning_rate': 2.8432228039557884e-05, 'epoch': 1.29}\n",
      "{'loss': 3.0154, 'grad_norm': 8.382102012634277, 'learning_rate': 2.8424956369982547e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1205, 'grad_norm': 9.983254432678223, 'learning_rate': 2.8417684700407214e-05, 'epoch': 1.29}\n",
      "{'loss': 3.1359, 'grad_norm': 8.37492847442627, 'learning_rate': 2.8410413030831878e-05, 'epoch': 1.3}\n",
      "{'loss': 3.2309, 'grad_norm': 9.751524925231934, 'learning_rate': 2.8403141361256548e-05, 'epoch': 1.3}\n",
      "{'loss': 3.1248, 'grad_norm': 8.441601753234863, 'learning_rate': 2.8395869691681208e-05, 'epoch': 1.3}\n",
      "{'loss': 3.0766, 'grad_norm': 8.336862564086914, 'learning_rate': 2.838859802210588e-05, 'epoch': 1.3}\n",
      "{'loss': 3.2131, 'grad_norm': 8.485832214355469, 'learning_rate': 2.8381326352530542e-05, 'epoch': 1.3}\n",
      "{'loss': 3.1762, 'grad_norm': 7.760749816894531, 'learning_rate': 2.837405468295521e-05, 'epoch': 1.3}\n",
      "{'loss': 3.0652, 'grad_norm': 8.376827239990234, 'learning_rate': 2.8366783013379872e-05, 'epoch': 1.3}\n",
      "{'loss': 3.024, 'grad_norm': 8.252642631530762, 'learning_rate': 2.835951134380454e-05, 'epoch': 1.3}\n",
      "{'loss': 3.1447, 'grad_norm': 9.04112720489502, 'learning_rate': 2.8352239674229203e-05, 'epoch': 1.3}\n",
      "{'loss': 3.0439, 'grad_norm': 8.481008529663086, 'learning_rate': 2.834496800465387e-05, 'epoch': 1.3}\n",
      "{'loss': 3.102, 'grad_norm': 8.055267333984375, 'learning_rate': 2.8337696335078533e-05, 'epoch': 1.3}\n",
      "{'loss': 3.2098, 'grad_norm': 8.377723693847656, 'learning_rate': 2.8330424665503203e-05, 'epoch': 1.3}\n",
      "{'loss': 3.166, 'grad_norm': 8.677301406860352, 'learning_rate': 2.8323152995927864e-05, 'epoch': 1.3}\n",
      "{'loss': 3.2195, 'grad_norm': 8.507747650146484, 'learning_rate': 2.8315881326352534e-05, 'epoch': 1.3}\n",
      "{'loss': 3.1697, 'grad_norm': 10.447554588317871, 'learning_rate': 2.8308609656777197e-05, 'epoch': 1.3}\n",
      "{'loss': 3.1889, 'grad_norm': 8.622026443481445, 'learning_rate': 2.8301337987201864e-05, 'epoch': 1.3}\n",
      "{'loss': 3.1592, 'grad_norm': 9.113487243652344, 'learning_rate': 2.8294066317626528e-05, 'epoch': 1.3}\n",
      "{'loss': 2.9904, 'grad_norm': 8.167515754699707, 'learning_rate': 2.8286794648051195e-05, 'epoch': 1.3}\n",
      "{'loss': 3.151, 'grad_norm': 8.414701461791992, 'learning_rate': 2.8279522978475858e-05, 'epoch': 1.3}\n",
      "{'loss': 3.0811, 'grad_norm': 8.83094596862793, 'learning_rate': 2.827225130890053e-05, 'epoch': 1.3}\n",
      "{'loss': 3.0309, 'grad_norm': 10.331997871398926, 'learning_rate': 2.826497963932519e-05, 'epoch': 1.3}\n",
      "{'loss': 3.118, 'grad_norm': 8.60489559173584, 'learning_rate': 2.825770796974986e-05, 'epoch': 1.3}\n",
      "{'loss': 3.175, 'grad_norm': 12.771001815795898, 'learning_rate': 2.8250436300174522e-05, 'epoch': 1.3}\n",
      "{'loss': 3.0477, 'grad_norm': 8.72635555267334, 'learning_rate': 2.824316463059919e-05, 'epoch': 1.31}\n",
      "{'loss': 3.0346, 'grad_norm': 8.472579956054688, 'learning_rate': 2.8235892961023853e-05, 'epoch': 1.31}\n",
      "{'loss': 3.126, 'grad_norm': 8.036691665649414, 'learning_rate': 2.822862129144852e-05, 'epoch': 1.31}\n",
      "{'loss': 3.0867, 'grad_norm': 10.019308090209961, 'learning_rate': 2.8221349621873183e-05, 'epoch': 1.31}\n",
      "{'loss': 3.0209, 'grad_norm': 8.433220863342285, 'learning_rate': 2.821407795229785e-05, 'epoch': 1.31}\n",
      "{'loss': 3.2014, 'grad_norm': 8.829615592956543, 'learning_rate': 2.8206806282722514e-05, 'epoch': 1.31}\n",
      "{'loss': 3.2098, 'grad_norm': 8.536118507385254, 'learning_rate': 2.8199534613147184e-05, 'epoch': 1.31}\n",
      "{'loss': 3.0225, 'grad_norm': 8.708192825317383, 'learning_rate': 2.8192262943571844e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1301, 'grad_norm': 8.821940422058105, 'learning_rate': 2.8184991273996508e-05, 'epoch': 1.31}\n",
      " 44%|██████████████▍                  | 30000/68760 [5:35:08<7:05:48,  1.52it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:24<00:08,  8.50s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.826642, 'eval_rouge-2': 6.998678, 'eval_rouge-l': 23.841362, 'eval_bleu-4': 0.03326442970794283, 'eval_runtime': 37.7667, 'eval_samples_per_second': 1.324, 'eval_steps_per_second': 0.106, 'epoch': 1.31}\n",
      " 44%|██████████████▍                  | 30000/68760 [5:35:46<7:05:48,  1.52it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  5.81s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-30000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1088, 'grad_norm': 8.851007461547852, 'learning_rate': 2.8177719604421178e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1857, 'grad_norm': 8.776747703552246, 'learning_rate': 2.8170447934845838e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1258, 'grad_norm': 8.71420669555664, 'learning_rate': 2.8163176265270508e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1818, 'grad_norm': 8.799551010131836, 'learning_rate': 2.8155904595695172e-05, 'epoch': 1.31}\n",
      "{'loss': 3.0156, 'grad_norm': 10.133147239685059, 'learning_rate': 2.814863292611984e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1348, 'grad_norm': 8.027864456176758, 'learning_rate': 2.8141361256544502e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1119, 'grad_norm': 8.217683792114258, 'learning_rate': 2.813408958696917e-05, 'epoch': 1.31}\n",
      "{'loss': 3.2637, 'grad_norm': 9.559041976928711, 'learning_rate': 2.8126817917393833e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1549, 'grad_norm': 7.942538738250732, 'learning_rate': 2.8119546247818503e-05, 'epoch': 1.31}\n",
      "{'loss': 3.0361, 'grad_norm': 8.326730728149414, 'learning_rate': 2.8112274578243163e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1117, 'grad_norm': 9.854253768920898, 'learning_rate': 2.8105002908667833e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1064, 'grad_norm': 8.76386547088623, 'learning_rate': 2.8097731239092497e-05, 'epoch': 1.31}\n",
      "{'loss': 3.1344, 'grad_norm': 10.877848625183105, 'learning_rate': 2.8090459569517164e-05, 'epoch': 1.31}\n",
      "{'loss': 3.0727, 'grad_norm': 8.590546607971191, 'learning_rate': 2.8083187899941827e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1906, 'grad_norm': 8.636749267578125, 'learning_rate': 2.8075916230366494e-05, 'epoch': 1.32}\n",
      "{'loss': 3.016, 'grad_norm': 8.128100395202637, 'learning_rate': 2.8068644560791158e-05, 'epoch': 1.32}\n",
      "{'loss': 3.0969, 'grad_norm': 9.60588264465332, 'learning_rate': 2.8061372891215824e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1025, 'grad_norm': 8.997651100158691, 'learning_rate': 2.8054101221640488e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1936, 'grad_norm': 9.326480865478516, 'learning_rate': 2.8046829552065158e-05, 'epoch': 1.32}\n",
      "{'loss': 3.0549, 'grad_norm': 9.487807273864746, 'learning_rate': 2.803955788248982e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1928, 'grad_norm': 8.838759422302246, 'learning_rate': 2.803228621291449e-05, 'epoch': 1.32}\n",
      "{'loss': 3.2643, 'grad_norm': 10.230304718017578, 'learning_rate': 2.8025014543339152e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1279, 'grad_norm': 8.17953872680664, 'learning_rate': 2.801774287376382e-05, 'epoch': 1.32}\n",
      "{'loss': 3.0908, 'grad_norm': 8.49860668182373, 'learning_rate': 2.8010471204188483e-05, 'epoch': 1.32}\n",
      "{'loss': 3.0043, 'grad_norm': 8.852993965148926, 'learning_rate': 2.800319953461315e-05, 'epoch': 1.32}\n",
      "{'loss': 3.315, 'grad_norm': 8.670801162719727, 'learning_rate': 2.7995927865037813e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1682, 'grad_norm': 9.34531021118164, 'learning_rate': 2.7988656195462483e-05, 'epoch': 1.32}\n",
      "{'loss': 3.0607, 'grad_norm': 9.134696960449219, 'learning_rate': 2.7981384525887143e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1469, 'grad_norm': 8.779626846313477, 'learning_rate': 2.7974112856311814e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1713, 'grad_norm': 8.826784133911133, 'learning_rate': 2.7966841186736477e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1088, 'grad_norm': 9.118345260620117, 'learning_rate': 2.7959569517161144e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1432, 'grad_norm': 7.784488677978516, 'learning_rate': 2.7952297847585808e-05, 'epoch': 1.32}\n",
      "{'loss': 3.2637, 'grad_norm': 8.100812911987305, 'learning_rate': 2.7945026178010474e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1254, 'grad_norm': 7.762326240539551, 'learning_rate': 2.7937754508435138e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1881, 'grad_norm': 9.540828704833984, 'learning_rate': 2.7930482838859805e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1215, 'grad_norm': 7.7272162437438965, 'learning_rate': 2.792321116928447e-05, 'epoch': 1.32}\n",
      "{'loss': 3.1627, 'grad_norm': 9.089588165283203, 'learning_rate': 2.791593949970914e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0727, 'grad_norm': 9.099923133850098, 'learning_rate': 2.79086678301338e-05, 'epoch': 1.33}\n",
      "{'loss': 3.2205, 'grad_norm': 9.207891464233398, 'learning_rate': 2.7901396160558462e-05, 'epoch': 1.33}\n",
      "{'loss': 3.1154, 'grad_norm': 9.349178314208984, 'learning_rate': 2.7894124490983133e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0781, 'grad_norm': 9.012606620788574, 'learning_rate': 2.7886852821407793e-05, 'epoch': 1.33}\n",
      "{'loss': 3.1924, 'grad_norm': 8.892796516418457, 'learning_rate': 2.7879581151832463e-05, 'epoch': 1.33}\n",
      "{'loss': 2.9857, 'grad_norm': 9.02547550201416, 'learning_rate': 2.7872309482257126e-05, 'epoch': 1.33}\n",
      "{'loss': 3.151, 'grad_norm': 8.504432678222656, 'learning_rate': 2.7865037812681793e-05, 'epoch': 1.33}\n",
      "{'loss': 3.1762, 'grad_norm': 9.245866775512695, 'learning_rate': 2.7857766143106457e-05, 'epoch': 1.33}\n",
      "{'loss': 3.1598, 'grad_norm': 8.866012573242188, 'learning_rate': 2.7850494473531124e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0703, 'grad_norm': 11.14439582824707, 'learning_rate': 2.7843222803955787e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0869, 'grad_norm': 9.733723640441895, 'learning_rate': 2.7835951134380454e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0469, 'grad_norm': 9.198517799377441, 'learning_rate': 2.7828679464805118e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0758, 'grad_norm': 9.495882987976074, 'learning_rate': 2.7821407795229788e-05, 'epoch': 1.33}\n",
      " 44%|██████████████▋                  | 30500/68760 [5:40:54<6:54:43,  1.54it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.05s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.47s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.586798, 'eval_rouge-2': 8.027692, 'eval_rouge-l': 25.5071, 'eval_bleu-4': 0.038575344580840984, 'eval_runtime': 17.6459, 'eval_samples_per_second': 2.834, 'eval_steps_per_second': 0.227, 'epoch': 1.33}\n",
      " 44%|██████████████▋                  | 30500/68760 [5:41:12<6:54:43,  1.54it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.34s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-30500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0736, 'grad_norm': 11.344385147094727, 'learning_rate': 2.7814136125654448e-05, 'epoch': 1.33}\n",
      "{'loss': 3.1637, 'grad_norm': 28.000720977783203, 'learning_rate': 2.7806864456079118e-05, 'epoch': 1.33}\n",
      "{'loss': 3.1684, 'grad_norm': 8.869254112243652, 'learning_rate': 2.7799592786503782e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0969, 'grad_norm': 10.007807731628418, 'learning_rate': 2.779232111692845e-05, 'epoch': 1.33}\n",
      "{'loss': 3.141, 'grad_norm': 8.578152656555176, 'learning_rate': 2.7785049447353112e-05, 'epoch': 1.33}\n",
      "{'loss': 3.1926, 'grad_norm': 7.9024224281311035, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n",
      "{'loss': 3.2037, 'grad_norm': 9.479446411132812, 'learning_rate': 2.7770506108202443e-05, 'epoch': 1.33}\n",
      "{'loss': 3.1734, 'grad_norm': 9.39840030670166, 'learning_rate': 2.7763234438627113e-05, 'epoch': 1.33}\n",
      "{'loss': 3.0, 'grad_norm': 8.112757682800293, 'learning_rate': 2.7755962769051773e-05, 'epoch': 1.33}\n",
      "{'loss': 3.2375, 'grad_norm': 8.567570686340332, 'learning_rate': 2.7748691099476443e-05, 'epoch': 1.34}\n",
      "{'loss': 3.0449, 'grad_norm': 7.794107437133789, 'learning_rate': 2.7741419429901107e-05, 'epoch': 1.34}\n",
      "{'loss': 3.1359, 'grad_norm': 9.202953338623047, 'learning_rate': 2.7734147760325774e-05, 'epoch': 1.34}\n",
      "{'loss': 2.9914, 'grad_norm': 8.206319808959961, 'learning_rate': 2.7726876090750437e-05, 'epoch': 1.34}\n",
      "{'loss': 3.1412, 'grad_norm': 9.16988754272461, 'learning_rate': 2.7719604421175104e-05, 'epoch': 1.34}\n",
      "{'loss': 3.2047, 'grad_norm': 8.764877319335938, 'learning_rate': 2.7712332751599768e-05, 'epoch': 1.34}\n",
      "{'loss': 3.1455, 'grad_norm': 9.01794147491455, 'learning_rate': 2.7705061082024435e-05, 'epoch': 1.34}\n",
      "{'loss': 3.0516, 'grad_norm': 9.482080459594727, 'learning_rate': 2.7697789412449098e-05, 'epoch': 1.34}\n",
      "{'loss': 3.0719, 'grad_norm': 8.82605266571045, 'learning_rate': 2.7690517742873768e-05, 'epoch': 1.34}\n",
      "{'loss': 3.042, 'grad_norm': 9.088677406311035, 'learning_rate': 2.768324607329843e-05, 'epoch': 1.34}\n",
      "{'loss': 3.1756, 'grad_norm': 9.374858856201172, 'learning_rate': 2.76759744037231e-05, 'epoch': 1.34}\n",
      "{'loss': 3.0746, 'grad_norm': 9.183676719665527, 'learning_rate': 2.7668702734147762e-05, 'epoch': 1.34}\n",
      "{'loss': 3.1311, 'grad_norm': 8.942560195922852, 'learning_rate': 2.766143106457243e-05, 'epoch': 1.34}\n",
      "{'loss': 3.3906, 'grad_norm': 9.135398864746094, 'learning_rate': 2.7654159394997093e-05, 'epoch': 1.34}\n",
      "{'loss': 3.15, 'grad_norm': 9.1344575881958, 'learning_rate': 2.764688772542176e-05, 'epoch': 1.34}\n",
      "{'loss': 3.1582, 'grad_norm': 9.397398948669434, 'learning_rate': 2.7639616055846423e-05, 'epoch': 1.34}\n",
      "{'loss': 3.2434, 'grad_norm': 9.7523193359375, 'learning_rate': 2.7632344386271093e-05, 'epoch': 1.34}\n",
      "{'loss': 3.0623, 'grad_norm': 8.4782133102417, 'learning_rate': 2.7625072716695753e-05, 'epoch': 1.34}\n",
      "{'loss': 3.0021, 'grad_norm': 8.694393157958984, 'learning_rate': 2.7617801047120424e-05, 'epoch': 1.34}\n",
      "{'loss': 3.1045, 'grad_norm': 9.918962478637695, 'learning_rate': 2.7610529377545087e-05, 'epoch': 1.34}\n",
      "{'loss': 3.21, 'grad_norm': 9.053019523620605, 'learning_rate': 2.7603257707969747e-05, 'epoch': 1.34}\n",
      "{'loss': 3.0467, 'grad_norm': 9.146224975585938, 'learning_rate': 2.7595986038394418e-05, 'epoch': 1.34}\n",
      "{'loss': 3.1137, 'grad_norm': 8.44571304321289, 'learning_rate': 2.758871436881908e-05, 'epoch': 1.34}\n",
      "{'loss': 2.9854, 'grad_norm': 8.977663040161133, 'learning_rate': 2.7581442699243748e-05, 'epoch': 1.35}\n",
      "{'loss': 3.0934, 'grad_norm': 9.086438179016113, 'learning_rate': 2.757417102966841e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1512, 'grad_norm': 9.01416301727295, 'learning_rate': 2.756689936009308e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1207, 'grad_norm': 8.668549537658691, 'learning_rate': 2.7559627690517742e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1828, 'grad_norm': 8.102300643920898, 'learning_rate': 2.755235602094241e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1186, 'grad_norm': 8.149381637573242, 'learning_rate': 2.7545084351367072e-05, 'epoch': 1.35}\n",
      "{'loss': 3.0775, 'grad_norm': 8.729619979858398, 'learning_rate': 2.7537812681791743e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1406, 'grad_norm': 9.829203605651855, 'learning_rate': 2.7530541012216403e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1574, 'grad_norm': 9.160252571105957, 'learning_rate': 2.7523269342641073e-05, 'epoch': 1.35}\n",
      "{'loss': 3.0809, 'grad_norm': 8.676140785217285, 'learning_rate': 2.7515997673065737e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1213, 'grad_norm': 8.556539535522461, 'learning_rate': 2.7508726003490403e-05, 'epoch': 1.35}\n",
      "{'loss': 3.0441, 'grad_norm': 9.500754356384277, 'learning_rate': 2.7501454333915067e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1336, 'grad_norm': 9.315279960632324, 'learning_rate': 2.7494182664339734e-05, 'epoch': 1.35}\n",
      "{'loss': 3.2057, 'grad_norm': 9.490272521972656, 'learning_rate': 2.7486910994764397e-05, 'epoch': 1.35}\n",
      "{'loss': 3.2166, 'grad_norm': 10.022317886352539, 'learning_rate': 2.7479639325189068e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1424, 'grad_norm': 9.247979164123535, 'learning_rate': 2.7472367655613728e-05, 'epoch': 1.35}\n",
      "{'loss': 3.2213, 'grad_norm': 8.62003231048584, 'learning_rate': 2.7465095986038398e-05, 'epoch': 1.35}\n",
      "{'loss': 3.1537, 'grad_norm': 8.570825576782227, 'learning_rate': 2.745782431646306e-05, 'epoch': 1.35}\n",
      " 45%|██████████████▉                  | 31000/68760 [5:46:21<6:57:50,  1.51it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.96s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:06,  6.10s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.740916, 'eval_rouge-2': 7.498208, 'eval_rouge-l': 25.011552000000002, 'eval_bleu-4': 0.03443237966265239, 'eval_runtime': 29.0762, 'eval_samples_per_second': 1.72, 'eval_steps_per_second': 0.138, 'epoch': 1.35}\n",
      " 45%|██████████████▉                  | 31000/68760 [5:46:50<6:57:50,  1.51it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  4.15s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-31000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 2.9633, 'grad_norm': 8.971527099609375, 'learning_rate': 2.745055264688773e-05, 'epoch': 1.35}\n",
      "{'loss': 3.2283, 'grad_norm': 8.411802291870117, 'learning_rate': 2.7443280977312392e-05, 'epoch': 1.35}\n",
      "{'loss': 3.2437, 'grad_norm': 9.285907745361328, 'learning_rate': 2.743600930773706e-05, 'epoch': 1.35}\n",
      "{'loss': 3.0893, 'grad_norm': 8.62821102142334, 'learning_rate': 2.7428737638161722e-05, 'epoch': 1.35}\n",
      "{'loss': 3.0664, 'grad_norm': 8.04035758972168, 'learning_rate': 2.742146596858639e-05, 'epoch': 1.35}\n",
      "{'loss': 3.0541, 'grad_norm': 9.098660469055176, 'learning_rate': 2.7414194299011053e-05, 'epoch': 1.36}\n",
      "{'loss': 3.1506, 'grad_norm': 8.299083709716797, 'learning_rate': 2.7406922629435723e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2215, 'grad_norm': 9.435908317565918, 'learning_rate': 2.7399650959860383e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2059, 'grad_norm': 10.314136505126953, 'learning_rate': 2.7392379290285053e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2832, 'grad_norm': 8.993348121643066, 'learning_rate': 2.7385107620709717e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2398, 'grad_norm': 9.363517761230469, 'learning_rate': 2.7377835951134384e-05, 'epoch': 1.36}\n",
      "{'loss': 3.1354, 'grad_norm': 8.427644729614258, 'learning_rate': 2.7370564281559047e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2482, 'grad_norm': 9.857681274414062, 'learning_rate': 2.7363292611983714e-05, 'epoch': 1.36}\n",
      "{'loss': 3.0447, 'grad_norm': 9.094223976135254, 'learning_rate': 2.7356020942408378e-05, 'epoch': 1.36}\n",
      "{'loss': 3.0352, 'grad_norm': 9.061036109924316, 'learning_rate': 2.7348749272833048e-05, 'epoch': 1.36}\n",
      "{'loss': 3.1832, 'grad_norm': 8.670442581176758, 'learning_rate': 2.7341477603257708e-05, 'epoch': 1.36}\n",
      "{'loss': 3.1211, 'grad_norm': 8.299778938293457, 'learning_rate': 2.733420593368238e-05, 'epoch': 1.36}\n",
      "{'loss': 3.1396, 'grad_norm': 9.656577110290527, 'learning_rate': 2.7326934264107042e-05, 'epoch': 1.36}\n",
      "{'loss': 3.1313, 'grad_norm': 10.500730514526367, 'learning_rate': 2.7319662594531702e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2301, 'grad_norm': 8.671271324157715, 'learning_rate': 2.7312390924956372e-05, 'epoch': 1.36}\n",
      "{'loss': 2.9123, 'grad_norm': 8.825830459594727, 'learning_rate': 2.7305119255381036e-05, 'epoch': 1.36}\n",
      "{'loss': 3.1084, 'grad_norm': 8.184926986694336, 'learning_rate': 2.7297847585805703e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2941, 'grad_norm': 8.199902534484863, 'learning_rate': 2.7290575916230366e-05, 'epoch': 1.36}\n",
      "{'loss': 3.0615, 'grad_norm': 9.281917572021484, 'learning_rate': 2.7283304246655033e-05, 'epoch': 1.36}\n",
      "{'loss': 3.0941, 'grad_norm': 8.525064468383789, 'learning_rate': 2.7276032577079697e-05, 'epoch': 1.36}\n",
      "{'loss': 3.2186, 'grad_norm': 8.822504997253418, 'learning_rate': 2.7268760907504364e-05, 'epoch': 1.36}\n",
      "{'loss': 3.0182, 'grad_norm': 9.047396659851074, 'learning_rate': 2.7261489237929027e-05, 'epoch': 1.36}\n",
      "{'loss': 3.1006, 'grad_norm': 9.097972869873047, 'learning_rate': 2.7254217568353697e-05, 'epoch': 1.36}\n",
      "{'loss': 3.0812, 'grad_norm': 9.611851692199707, 'learning_rate': 2.7246945898778357e-05, 'epoch': 1.37}\n",
      "{'loss': 3.21, 'grad_norm': 8.775752067565918, 'learning_rate': 2.7239674229203028e-05, 'epoch': 1.37}\n",
      "{'loss': 3.0844, 'grad_norm': 9.64883804321289, 'learning_rate': 2.723240255962769e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1025, 'grad_norm': 7.8814568519592285, 'learning_rate': 2.7225130890052358e-05, 'epoch': 1.37}\n",
      "{'loss': 3.0986, 'grad_norm': 9.122488021850586, 'learning_rate': 2.721785922047702e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1994, 'grad_norm': 9.892658233642578, 'learning_rate': 2.721058755090169e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2809, 'grad_norm': 8.199362754821777, 'learning_rate': 2.7203315881326352e-05, 'epoch': 1.37}\n",
      "{'loss': 3.0998, 'grad_norm': 8.648764610290527, 'learning_rate': 2.719604421175102e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1621, 'grad_norm': 9.587850570678711, 'learning_rate': 2.7188772542175682e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2166, 'grad_norm': 8.699241638183594, 'learning_rate': 2.7181500872600353e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1561, 'grad_norm': 8.846275329589844, 'learning_rate': 2.7174229203025013e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1861, 'grad_norm': 8.872539520263672, 'learning_rate': 2.7166957533449683e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1352, 'grad_norm': 8.583894729614258, 'learning_rate': 2.7159685863874347e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2313, 'grad_norm': 8.700014114379883, 'learning_rate': 2.7152414194299014e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2115, 'grad_norm': 9.05023193359375, 'learning_rate': 2.7145142524723677e-05, 'epoch': 1.37}\n",
      "{'loss': 3.0088, 'grad_norm': 8.579229354858398, 'learning_rate': 2.7137870855148344e-05, 'epoch': 1.37}\n",
      "{'loss': 3.0221, 'grad_norm': 9.627492904663086, 'learning_rate': 2.7130599185573007e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1391, 'grad_norm': 8.85533332824707, 'learning_rate': 2.7123327515997678e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1457, 'grad_norm': 8.679927825927734, 'learning_rate': 2.7116055846422338e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2318, 'grad_norm': 9.399453163146973, 'learning_rate': 2.7108784176847008e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1078, 'grad_norm': 10.72187614440918, 'learning_rate': 2.710151250727167e-05, 'epoch': 1.37}\n",
      "{'loss': 3.1363, 'grad_norm': 9.335179328918457, 'learning_rate': 2.709424083769634e-05, 'epoch': 1.37}\n",
      " 46%|███████████████                  | 31500/68760 [5:52:00<6:16:59,  1.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.01it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:05,  5.53s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.941716, 'eval_rouge-2': 7.3265519999999995, 'eval_rouge-l': 25.093690000000002, 'eval_bleu-4': 0.03505343889239796, 'eval_runtime': 17.3581, 'eval_samples_per_second': 2.881, 'eval_steps_per_second': 0.23, 'epoch': 1.37}\n",
      " 46%|███████████████                  | 31500/68760 [5:52:18<6:16:59,  1.65it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.95s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-31500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0674, 'grad_norm': 9.007964134216309, 'learning_rate': 2.7086969168121002e-05, 'epoch': 1.37}\n",
      "{'loss': 3.2049, 'grad_norm': 9.276481628417969, 'learning_rate': 2.707969749854567e-05, 'epoch': 1.38}\n",
      "{'loss': 3.0693, 'grad_norm': 8.188454627990723, 'learning_rate': 2.7072425828970332e-05, 'epoch': 1.38}\n",
      "{'loss': 3.2188, 'grad_norm': 8.716219902038574, 'learning_rate': 2.7065154159395e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1105, 'grad_norm': 9.191126823425293, 'learning_rate': 2.7057882489819663e-05, 'epoch': 1.38}\n",
      "{'loss': 3.041, 'grad_norm': 9.088433265686035, 'learning_rate': 2.7050610820244333e-05, 'epoch': 1.38}\n",
      "{'loss': 2.9934, 'grad_norm': 9.20833969116211, 'learning_rate': 2.7043339150668993e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1213, 'grad_norm': 9.311419486999512, 'learning_rate': 2.7036067481093664e-05, 'epoch': 1.38}\n",
      "{'loss': 3.133, 'grad_norm': 7.886387348175049, 'learning_rate': 2.7028795811518327e-05, 'epoch': 1.38}\n",
      "{'loss': 3.0879, 'grad_norm': 9.174400329589844, 'learning_rate': 2.7021524141942987e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1953, 'grad_norm': 8.768197059631348, 'learning_rate': 2.7014252472367657e-05, 'epoch': 1.38}\n",
      "{'loss': 3.0975, 'grad_norm': 10.41132926940918, 'learning_rate': 2.700698080279232e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1086, 'grad_norm': 8.651776313781738, 'learning_rate': 2.6999709133216988e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1031, 'grad_norm': 8.615708351135254, 'learning_rate': 2.699243746364165e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1467, 'grad_norm': 8.528485298156738, 'learning_rate': 2.6985165794066318e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1299, 'grad_norm': 9.23856258392334, 'learning_rate': 2.6977894124490982e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1201, 'grad_norm': 8.22691535949707, 'learning_rate': 2.6970622454915652e-05, 'epoch': 1.38}\n",
      "{'loss': 3.15, 'grad_norm': 9.315717697143555, 'learning_rate': 2.6963350785340312e-05, 'epoch': 1.38}\n",
      "{'loss': 3.0871, 'grad_norm': 8.63226318359375, 'learning_rate': 2.6956079115764982e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1104, 'grad_norm': 8.977982521057129, 'learning_rate': 2.6948807446189646e-05, 'epoch': 1.38}\n",
      "{'loss': 3.0605, 'grad_norm': 8.287080764770508, 'learning_rate': 2.6941535776614313e-05, 'epoch': 1.38}\n",
      "{'loss': 3.1592, 'grad_norm': 9.591361045837402, 'learning_rate': 2.6934264107038976e-05, 'epoch': 1.38}\n",
      "{'loss': 3.0426, 'grad_norm': 8.928166389465332, 'learning_rate': 2.6926992437463643e-05, 'epoch': 1.38}\n",
      "{'loss': 3.0996, 'grad_norm': 9.15782642364502, 'learning_rate': 2.6919720767888307e-05, 'epoch': 1.38}\n",
      "{'loss': 3.0229, 'grad_norm': 9.152308464050293, 'learning_rate': 2.6912449098312974e-05, 'epoch': 1.39}\n",
      "{'loss': 3.0949, 'grad_norm': 9.29520320892334, 'learning_rate': 2.6905177428737637e-05, 'epoch': 1.39}\n",
      "{'loss': 3.066, 'grad_norm': 9.99172592163086, 'learning_rate': 2.6897905759162307e-05, 'epoch': 1.39}\n",
      "{'loss': 3.0947, 'grad_norm': 12.143030166625977, 'learning_rate': 2.6890634089586968e-05, 'epoch': 1.39}\n",
      "{'loss': 3.0543, 'grad_norm': 8.830052375793457, 'learning_rate': 2.6883362420011638e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1891, 'grad_norm': 9.406673431396484, 'learning_rate': 2.68760907504363e-05, 'epoch': 1.39}\n",
      "{'loss': 3.11, 'grad_norm': 9.530437469482422, 'learning_rate': 2.6868819080860968e-05, 'epoch': 1.39}\n",
      "{'loss': 3.0602, 'grad_norm': 8.255804061889648, 'learning_rate': 2.6861547411285632e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1187, 'grad_norm': 8.306598663330078, 'learning_rate': 2.68542757417103e-05, 'epoch': 1.39}\n",
      "{'loss': 2.9648, 'grad_norm': 8.770137786865234, 'learning_rate': 2.6847004072134962e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2215, 'grad_norm': 8.449187278747559, 'learning_rate': 2.6839732402559632e-05, 'epoch': 1.39}\n",
      "{'loss': 3.2434, 'grad_norm': 8.787162780761719, 'learning_rate': 2.6832460732984293e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1535, 'grad_norm': 8.651965141296387, 'learning_rate': 2.6825189063408963e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1385, 'grad_norm': 12.394303321838379, 'learning_rate': 2.6817917393833626e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1924, 'grad_norm': 8.793971061706543, 'learning_rate': 2.6810645724258293e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1002, 'grad_norm': 8.246744155883789, 'learning_rate': 2.6803374054682957e-05, 'epoch': 1.39}\n",
      "{'loss': 3.0973, 'grad_norm': 8.869019508361816, 'learning_rate': 2.6796102385107624e-05, 'epoch': 1.39}\n",
      "{'loss': 3.0984, 'grad_norm': 10.000347137451172, 'learning_rate': 2.6788830715532287e-05, 'epoch': 1.39}\n",
      "{'loss': 2.9793, 'grad_norm': 9.01266098022461, 'learning_rate': 2.6781559045956954e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1211, 'grad_norm': 9.06569766998291, 'learning_rate': 2.6774287376381618e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1484, 'grad_norm': 9.255799293518066, 'learning_rate': 2.6767015706806288e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1172, 'grad_norm': 8.494510650634766, 'learning_rate': 2.6759744037230948e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1182, 'grad_norm': 9.203654289245605, 'learning_rate': 2.6752472367655618e-05, 'epoch': 1.39}\n",
      "{'loss': 3.1678, 'grad_norm': 8.812450408935547, 'learning_rate': 2.6745200698080282e-05, 'epoch': 1.4}\n",
      "{'loss': 3.1412, 'grad_norm': 8.855809211730957, 'learning_rate': 2.6737929028504942e-05, 'epoch': 1.4}\n",
      "{'loss': 3.1146, 'grad_norm': 9.13697624206543, 'learning_rate': 2.6730657358929612e-05, 'epoch': 1.4}\n",
      " 47%|███████████████▎                 | 32000/68760 [5:57:25<6:21:09,  1.61it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:05<00:05,  2.67s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.36s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.168596, 'eval_rouge-2': 7.888452, 'eval_rouge-l': 24.938516, 'eval_bleu-4': 0.03711797343284224, 'eval_runtime': 20.9651, 'eval_samples_per_second': 2.385, 'eval_steps_per_second': 0.191, 'epoch': 1.4}\n",
      " 47%|███████████████▎                 | 32000/68760 [5:57:46<6:21:09,  1.61it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:08<00:00,  1.97s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-32000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0066, 'grad_norm': 10.002035140991211, 'learning_rate': 2.6723385689354276e-05, 'epoch': 1.4}\n",
      "{'loss': 3.0379, 'grad_norm': 8.536822319030762, 'learning_rate': 2.6716114019778943e-05, 'epoch': 1.4}\n",
      "{'loss': 3.0432, 'grad_norm': 10.120183944702148, 'learning_rate': 2.6708842350203606e-05, 'epoch': 1.4}\n",
      "{'loss': 3.2713, 'grad_norm': 8.847977638244629, 'learning_rate': 2.6701570680628273e-05, 'epoch': 1.4}\n",
      "{'loss': 3.0459, 'grad_norm': 9.267660140991211, 'learning_rate': 2.6694299011052936e-05, 'epoch': 1.4}\n",
      "{'loss': 3.1775, 'grad_norm': 9.599350929260254, 'learning_rate': 2.6687027341477607e-05, 'epoch': 1.4}\n",
      "{'loss': 2.9369, 'grad_norm': 9.196784973144531, 'learning_rate': 2.6679755671902267e-05, 'epoch': 1.4}\n",
      "{'loss': 3.1885, 'grad_norm': 8.283158302307129, 'learning_rate': 2.6672484002326937e-05, 'epoch': 1.4}\n",
      "{'loss': 3.208, 'grad_norm': 9.370085716247559, 'learning_rate': 2.66652123327516e-05, 'epoch': 1.4}\n",
      "{'loss': 3.0969, 'grad_norm': 9.085253715515137, 'learning_rate': 2.6657940663176268e-05, 'epoch': 1.4}\n",
      "{'loss': 3.202, 'grad_norm': 8.671364784240723, 'learning_rate': 2.665066899360093e-05, 'epoch': 1.4}\n",
      "{'loss': 3.2137, 'grad_norm': 8.624822616577148, 'learning_rate': 2.6643397324025598e-05, 'epoch': 1.4}\n",
      "{'loss': 3.1844, 'grad_norm': 10.786352157592773, 'learning_rate': 2.663612565445026e-05, 'epoch': 1.4}\n",
      "{'loss': 2.9994, 'grad_norm': 9.056947708129883, 'learning_rate': 2.662885398487493e-05, 'epoch': 1.4}\n",
      "{'loss': 3.1201, 'grad_norm': 10.94328784942627, 'learning_rate': 2.6621582315299592e-05, 'epoch': 1.4}\n",
      "{'loss': 3.033, 'grad_norm': 8.566251754760742, 'learning_rate': 2.6614310645724262e-05, 'epoch': 1.4}\n",
      "{'loss': 3.1963, 'grad_norm': 10.235359191894531, 'learning_rate': 2.6607038976148922e-05, 'epoch': 1.4}\n",
      "{'loss': 3.2004, 'grad_norm': 9.384410858154297, 'learning_rate': 2.6599767306573593e-05, 'epoch': 1.4}\n",
      "{'loss': 3.2437, 'grad_norm': 8.41061019897461, 'learning_rate': 2.6592495636998256e-05, 'epoch': 1.4}\n",
      "{'loss': 3.1279, 'grad_norm': 9.021933555603027, 'learning_rate': 2.6585223967422923e-05, 'epoch': 1.4}\n",
      "{'loss': 2.9684, 'grad_norm': 9.871919631958008, 'learning_rate': 2.6577952297847586e-05, 'epoch': 1.41}\n",
      "{'loss': 3.0447, 'grad_norm': 8.36746883392334, 'learning_rate': 2.6570680628272253e-05, 'epoch': 1.41}\n",
      "{'loss': 3.0873, 'grad_norm': 8.61984634399414, 'learning_rate': 2.6563408958696917e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1635, 'grad_norm': 9.343605041503906, 'learning_rate': 2.6556137289121584e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1055, 'grad_norm': 9.208586692810059, 'learning_rate': 2.6548865619546247e-05, 'epoch': 1.41}\n",
      "{'loss': 3.123, 'grad_norm': 9.122231483459473, 'learning_rate': 2.6541593949970918e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1811, 'grad_norm': 8.769768714904785, 'learning_rate': 2.6534322280395578e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1869, 'grad_norm': 8.778319358825684, 'learning_rate': 2.6527050610820248e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1113, 'grad_norm': 8.621688842773438, 'learning_rate': 2.651977894124491e-05, 'epoch': 1.41}\n",
      "{'loss': 3.2117, 'grad_norm': 8.454385757446289, 'learning_rate': 2.651250727166958e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1174, 'grad_norm': 9.092178344726562, 'learning_rate': 2.6505235602094242e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1051, 'grad_norm': 8.129413604736328, 'learning_rate': 2.649796393251891e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1787, 'grad_norm': 10.427876472473145, 'learning_rate': 2.6490692262943572e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1303, 'grad_norm': 8.146224975585938, 'learning_rate': 2.6483420593368243e-05, 'epoch': 1.41}\n",
      "{'loss': 2.9842, 'grad_norm': 9.7533540725708, 'learning_rate': 2.6476148923792903e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1293, 'grad_norm': 9.802328109741211, 'learning_rate': 2.6468877254217573e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1783, 'grad_norm': 8.66550064086914, 'learning_rate': 2.6461605584642236e-05, 'epoch': 1.41}\n",
      "{'loss': 3.0295, 'grad_norm': 8.740933418273926, 'learning_rate': 2.6454333915066903e-05, 'epoch': 1.41}\n",
      "{'loss': 3.3117, 'grad_norm': 9.022873878479004, 'learning_rate': 2.6447062245491567e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1568, 'grad_norm': 9.067805290222168, 'learning_rate': 2.643979057591623e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1973, 'grad_norm': 10.878968238830566, 'learning_rate': 2.6432518906340897e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1322, 'grad_norm': 9.395944595336914, 'learning_rate': 2.642524723676556e-05, 'epoch': 1.41}\n",
      "{'loss': 3.099, 'grad_norm': 8.978856086730957, 'learning_rate': 2.6417975567190228e-05, 'epoch': 1.41}\n",
      "{'loss': 3.1311, 'grad_norm': 8.171889305114746, 'learning_rate': 2.641070389761489e-05, 'epoch': 1.42}\n",
      "{'loss': 3.1404, 'grad_norm': 8.72242259979248, 'learning_rate': 2.6403432228039558e-05, 'epoch': 1.42}\n",
      "{'loss': 3.2457, 'grad_norm': 9.032931327819824, 'learning_rate': 2.639616055846422e-05, 'epoch': 1.42}\n",
      "{'loss': 3.2154, 'grad_norm': 8.33487606048584, 'learning_rate': 2.6388888888888892e-05, 'epoch': 1.42}\n",
      "{'loss': 3.1166, 'grad_norm': 9.23241138458252, 'learning_rate': 2.6381617219313552e-05, 'epoch': 1.42}\n",
      "{'loss': 3.1613, 'grad_norm': 9.550558090209961, 'learning_rate': 2.6374345549738222e-05, 'epoch': 1.42}\n",
      "{'loss': 3.2035, 'grad_norm': 8.64432430267334, 'learning_rate': 2.6367073880162886e-05, 'epoch': 1.42}\n",
      " 47%|███████████████▌                 | 32500/68760 [6:02:58<6:29:07,  1.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.00s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.26s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.79122799999999, 'eval_rouge-2': 8.118972, 'eval_rouge-l': 26.118502, 'eval_bleu-4': 0.03734477711973456, 'eval_runtime': 25.1761, 'eval_samples_per_second': 1.986, 'eval_steps_per_second': 0.159, 'epoch': 1.42}\n",
      " 47%|███████████████▌                 | 32500/68760 [6:03:23<6:29:07,  1.55it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  4.20s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-32500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0119, 'grad_norm': 9.042604446411133, 'learning_rate': 2.6359802210587553e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0643, 'grad_norm': 9.142851829528809, 'learning_rate': 2.6352530541012216e-05, 'epoch': 1.42}\n",
      "{'loss': 3.1437, 'grad_norm': 8.646519660949707, 'learning_rate': 2.6345258871436883e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0893, 'grad_norm': 8.677849769592285, 'learning_rate': 2.6337987201861547e-05, 'epoch': 1.42}\n",
      "{'loss': 3.1797, 'grad_norm': 8.40941333770752, 'learning_rate': 2.6330715532286217e-05, 'epoch': 1.42}\n",
      "{'loss': 3.1303, 'grad_norm': 9.093962669372559, 'learning_rate': 2.6323443862710877e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0424, 'grad_norm': 8.497711181640625, 'learning_rate': 2.6316172193135547e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0951, 'grad_norm': 8.505592346191406, 'learning_rate': 2.630890052356021e-05, 'epoch': 1.42}\n",
      "{'loss': 2.9438, 'grad_norm': 9.30176830291748, 'learning_rate': 2.6301628853984878e-05, 'epoch': 1.42}\n",
      "{'loss': 3.2752, 'grad_norm': 9.2341890335083, 'learning_rate': 2.629435718440954e-05, 'epoch': 1.42}\n",
      "{'loss': 3.123, 'grad_norm': 8.699234962463379, 'learning_rate': 2.6287085514834208e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0643, 'grad_norm': 8.630422592163086, 'learning_rate': 2.627981384525887e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0854, 'grad_norm': 8.14228630065918, 'learning_rate': 2.627254217568354e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0799, 'grad_norm': 9.193320274353027, 'learning_rate': 2.6265270506108202e-05, 'epoch': 1.42}\n",
      "{'loss': 3.1582, 'grad_norm': 11.09876823425293, 'learning_rate': 2.6257998836532872e-05, 'epoch': 1.42}\n",
      "{'loss': 3.2092, 'grad_norm': 9.06644058227539, 'learning_rate': 2.6250727166957532e-05, 'epoch': 1.42}\n",
      "{'loss': 3.0859, 'grad_norm': 8.759095191955566, 'learning_rate': 2.6243455497382203e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1146, 'grad_norm': 9.257797241210938, 'learning_rate': 2.6236183827806866e-05, 'epoch': 1.43}\n",
      "{'loss': 3.21, 'grad_norm': 8.66778564453125, 'learning_rate': 2.6228912158231533e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2295, 'grad_norm': 9.130892753601074, 'learning_rate': 2.6221640488656197e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1867, 'grad_norm': 9.480825424194336, 'learning_rate': 2.6214368819080863e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2428, 'grad_norm': 8.87329387664795, 'learning_rate': 2.6207097149505527e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2732, 'grad_norm': 9.652994155883789, 'learning_rate': 2.6199825479930197e-05, 'epoch': 1.43}\n",
      "{'loss': 3.0918, 'grad_norm': 9.303669929504395, 'learning_rate': 2.6192553810354857e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1465, 'grad_norm': 9.683516502380371, 'learning_rate': 2.6185282140779528e-05, 'epoch': 1.43}\n",
      "{'loss': 3.0221, 'grad_norm': 9.306522369384766, 'learning_rate': 2.617801047120419e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1453, 'grad_norm': 8.803129196166992, 'learning_rate': 2.6170738801628858e-05, 'epoch': 1.43}\n",
      "{'loss': 3.0379, 'grad_norm': 8.660438537597656, 'learning_rate': 2.616346713205352e-05, 'epoch': 1.43}\n",
      "{'loss': 2.999, 'grad_norm': 9.680885314941406, 'learning_rate': 2.6156195462478185e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1197, 'grad_norm': 9.782671928405762, 'learning_rate': 2.6148923792902852e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1658, 'grad_norm': 8.01001262664795, 'learning_rate': 2.6141652123327515e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2568, 'grad_norm': 8.665736198425293, 'learning_rate': 2.6134380453752182e-05, 'epoch': 1.43}\n",
      "{'loss': 2.9934, 'grad_norm': 9.593024253845215, 'learning_rate': 2.6127108784176846e-05, 'epoch': 1.43}\n",
      "{'loss': 3.2777, 'grad_norm': 12.276541709899902, 'learning_rate': 2.6119837114601513e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1773, 'grad_norm': 8.511016845703125, 'learning_rate': 2.6112565445026176e-05, 'epoch': 1.43}\n",
      "{'loss': 3.0664, 'grad_norm': 8.462756156921387, 'learning_rate': 2.6105293775450847e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1934, 'grad_norm': 9.606941223144531, 'learning_rate': 2.6098022105875507e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1191, 'grad_norm': 9.590600967407227, 'learning_rate': 2.6090750436300177e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1682, 'grad_norm': 8.84753704071045, 'learning_rate': 2.608347876672484e-05, 'epoch': 1.43}\n",
      "{'loss': 3.1434, 'grad_norm': 9.077094078063965, 'learning_rate': 2.6076207097149507e-05, 'epoch': 1.44}\n",
      "{'loss': 3.2756, 'grad_norm': 9.389695167541504, 'learning_rate': 2.606893542757417e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0885, 'grad_norm': 9.005212783813477, 'learning_rate': 2.6061663757998838e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0715, 'grad_norm': 9.027181625366211, 'learning_rate': 2.60543920884235e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0693, 'grad_norm': 8.387338638305664, 'learning_rate': 2.604712041884817e-05, 'epoch': 1.44}\n",
      "{'loss': 3.115, 'grad_norm': 9.883139610290527, 'learning_rate': 2.603984874927283e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1801, 'grad_norm': 9.309051513671875, 'learning_rate': 2.6032577079697502e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0967, 'grad_norm': 9.131451606750488, 'learning_rate': 2.6025305410122165e-05, 'epoch': 1.44}\n",
      "{'loss': 2.9395, 'grad_norm': 8.546968460083008, 'learning_rate': 2.6018033740546832e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0504, 'grad_norm': 8.685970306396484, 'learning_rate': 2.6010762070971496e-05, 'epoch': 1.44}\n",
      "{'loss': 2.9432, 'grad_norm': 9.347867965698242, 'learning_rate': 2.6003490401396163e-05, 'epoch': 1.44}\n",
      " 48%|███████████████▊                 | 33000/68760 [6:08:32<5:58:48,  1.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.04it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.27s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.896408, 'eval_rouge-2': 7.952323999999999, 'eval_rouge-l': 25.656456, 'eval_bleu-4': 0.04096313814318162, 'eval_runtime': 8.8422, 'eval_samples_per_second': 5.655, 'eval_steps_per_second': 0.452, 'epoch': 1.44}\n",
      " 48%|███████████████▊                 | 33000/68760 [6:08:41<5:58:48,  1.66it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.11s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-33000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0654, 'grad_norm': 8.766498565673828, 'learning_rate': 2.5996218731820826e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0914, 'grad_norm': 8.705791473388672, 'learning_rate': 2.5988947062245493e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0395, 'grad_norm': 8.499728202819824, 'learning_rate': 2.5981675392670157e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1039, 'grad_norm': 8.897494316101074, 'learning_rate': 2.5974403723094827e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1629, 'grad_norm': 8.534592628479004, 'learning_rate': 2.5967132053519487e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0342, 'grad_norm': 8.444018363952637, 'learning_rate': 2.5959860383944157e-05, 'epoch': 1.44}\n",
      "{'loss': 3.2898, 'grad_norm': 9.1058988571167, 'learning_rate': 2.595258871436882e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1689, 'grad_norm': 9.6571626663208, 'learning_rate': 2.5945317044793488e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1748, 'grad_norm': 8.840170860290527, 'learning_rate': 2.593804537521815e-05, 'epoch': 1.44}\n",
      "{'loss': 3.234, 'grad_norm': 9.320682525634766, 'learning_rate': 2.5930773705642818e-05, 'epoch': 1.44}\n",
      "{'loss': 3.0682, 'grad_norm': 9.088996887207031, 'learning_rate': 2.592350203606748e-05, 'epoch': 1.44}\n",
      "{'loss': 3.1359, 'grad_norm': 8.86880874633789, 'learning_rate': 2.591623036649215e-05, 'epoch': 1.45}\n",
      "{'loss': 3.0764, 'grad_norm': 9.654778480529785, 'learning_rate': 2.5908958696916812e-05, 'epoch': 1.45}\n",
      "{'loss': 3.148, 'grad_norm': 8.714086532592773, 'learning_rate': 2.5901687027341482e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1361, 'grad_norm': 8.948606491088867, 'learning_rate': 2.5894415357766142e-05, 'epoch': 1.45}\n",
      "{'loss': 3.168, 'grad_norm': 9.174019813537598, 'learning_rate': 2.5887143688190813e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1822, 'grad_norm': 10.087990760803223, 'learning_rate': 2.5879872018615476e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1758, 'grad_norm': 9.229105949401855, 'learning_rate': 2.5872600349040143e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1854, 'grad_norm': 9.39940071105957, 'learning_rate': 2.5865328679464807e-05, 'epoch': 1.45}\n",
      "{'loss': 2.9426, 'grad_norm': 9.04154109954834, 'learning_rate': 2.585805700988947e-05, 'epoch': 1.45}\n",
      "{'loss': 3.027, 'grad_norm': 9.470792770385742, 'learning_rate': 2.5850785340314137e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1662, 'grad_norm': 8.884516716003418, 'learning_rate': 2.58435136707388e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1211, 'grad_norm': 9.18045711517334, 'learning_rate': 2.5836242001163467e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2893, 'grad_norm': 9.466048240661621, 'learning_rate': 2.582897033158813e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1486, 'grad_norm': 8.617477416992188, 'learning_rate': 2.58216986620128e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1164, 'grad_norm': 10.631606101989746, 'learning_rate': 2.581442699243746e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1855, 'grad_norm': 10.442179679870605, 'learning_rate': 2.580715532286213e-05, 'epoch': 1.45}\n",
      "{'loss': 3.0678, 'grad_norm': 9.092118263244629, 'learning_rate': 2.5799883653286795e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1229, 'grad_norm': 9.49984073638916, 'learning_rate': 2.5792611983711462e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1791, 'grad_norm': 8.915397644042969, 'learning_rate': 2.5785340314136126e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2137, 'grad_norm': 9.366636276245117, 'learning_rate': 2.5778068644560792e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1549, 'grad_norm': 9.210843086242676, 'learning_rate': 2.5770796974985456e-05, 'epoch': 1.45}\n",
      "{'loss': 3.0787, 'grad_norm': 9.476852416992188, 'learning_rate': 2.5763525305410123e-05, 'epoch': 1.45}\n",
      "{'loss': 3.1564, 'grad_norm': 8.96505355834961, 'learning_rate': 2.5756253635834786e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2674, 'grad_norm': 8.819016456604004, 'learning_rate': 2.5748981966259457e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1896, 'grad_norm': 8.466134071350098, 'learning_rate': 2.5741710296684117e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1822, 'grad_norm': 7.928816795349121, 'learning_rate': 2.5734438627108787e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1104, 'grad_norm': 8.38139820098877, 'learning_rate': 2.572716695753345e-05, 'epoch': 1.46}\n",
      "{'loss': 3.108, 'grad_norm': 8.629920959472656, 'learning_rate': 2.5719895287958117e-05, 'epoch': 1.46}\n",
      "{'loss': 2.9902, 'grad_norm': 9.874394416809082, 'learning_rate': 2.571262361838278e-05, 'epoch': 1.46}\n",
      "{'loss': 3.0875, 'grad_norm': 9.361742973327637, 'learning_rate': 2.5705351948807448e-05, 'epoch': 1.46}\n",
      "{'loss': 3.2068, 'grad_norm': 8.516301155090332, 'learning_rate': 2.569808027923211e-05, 'epoch': 1.46}\n",
      "{'loss': 3.2363, 'grad_norm': 9.386908531188965, 'learning_rate': 2.569080860965678e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1211, 'grad_norm': 9.893268585205078, 'learning_rate': 2.5683536940081442e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1969, 'grad_norm': 9.560019493103027, 'learning_rate': 2.5676265270506112e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1588, 'grad_norm': 9.288851737976074, 'learning_rate': 2.5668993600930776e-05, 'epoch': 1.46}\n",
      "{'loss': 3.0594, 'grad_norm': 9.722566604614258, 'learning_rate': 2.5661721931355442e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1812, 'grad_norm': 9.081548690795898, 'learning_rate': 2.5654450261780106e-05, 'epoch': 1.46}\n",
      "{'loss': 3.2141, 'grad_norm': 9.528566360473633, 'learning_rate': 2.5647178592204773e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1648, 'grad_norm': 9.4199800491333, 'learning_rate': 2.5639906922629436e-05, 'epoch': 1.46}\n",
      " 49%|████████████████                 | 33500/68760 [6:13:48<6:18:05,  1.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.18it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:05,  5.45s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.827056, 'eval_rouge-2': 8.039556, 'eval_rouge-l': 25.629735999999998, 'eval_bleu-4': 0.035980407446232016, 'eval_runtime': 27.0051, 'eval_samples_per_second': 1.851, 'eval_steps_per_second': 0.148, 'epoch': 1.46}\n",
      " 49%|████████████████                 | 33500/68760 [6:14:15<6:18:05,  1.55it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.80s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-33500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0965, 'grad_norm': 9.298446655273438, 'learning_rate': 2.5632635253054103e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1988, 'grad_norm': 12.436366081237793, 'learning_rate': 2.5625363583478767e-05, 'epoch': 1.46}\n",
      "{'loss': 3.0955, 'grad_norm': 8.583362579345703, 'learning_rate': 2.5618091913903437e-05, 'epoch': 1.46}\n",
      "{'loss': 3.0975, 'grad_norm': 8.437336921691895, 'learning_rate': 2.5610820244328097e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1824, 'grad_norm': 8.582086563110352, 'learning_rate': 2.5603548574752767e-05, 'epoch': 1.46}\n",
      "{'loss': 3.2035, 'grad_norm': 9.15135669708252, 'learning_rate': 2.559627690517743e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1207, 'grad_norm': 9.131858825683594, 'learning_rate': 2.5589005235602098e-05, 'epoch': 1.46}\n",
      "{'loss': 3.1342, 'grad_norm': 9.821269989013672, 'learning_rate': 2.558173356602676e-05, 'epoch': 1.47}\n",
      "{'loss': 3.0631, 'grad_norm': 9.261871337890625, 'learning_rate': 2.5574461896451425e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1633, 'grad_norm': 8.714095115661621, 'learning_rate': 2.5567190226876092e-05, 'epoch': 1.47}\n",
      "{'loss': 3.2207, 'grad_norm': 9.101728439331055, 'learning_rate': 2.5559918557300755e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1715, 'grad_norm': 10.324162483215332, 'learning_rate': 2.5552646887725422e-05, 'epoch': 1.47}\n",
      "{'loss': 3.0486, 'grad_norm': 8.805535316467285, 'learning_rate': 2.5545375218150086e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1406, 'grad_norm': 8.129365921020508, 'learning_rate': 2.5538103548574756e-05, 'epoch': 1.47}\n",
      "{'loss': 3.0291, 'grad_norm': 9.122651100158691, 'learning_rate': 2.5530831878999416e-05, 'epoch': 1.47}\n",
      "{'loss': 2.9916, 'grad_norm': 9.578580856323242, 'learning_rate': 2.5523560209424086e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1539, 'grad_norm': 9.999524116516113, 'learning_rate': 2.551628853984875e-05, 'epoch': 1.47}\n",
      "{'loss': 3.226, 'grad_norm': 9.026020050048828, 'learning_rate': 2.5509016870273417e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1795, 'grad_norm': 8.92068099975586, 'learning_rate': 2.550174520069808e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1172, 'grad_norm': 8.884480476379395, 'learning_rate': 2.5494473531122747e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1764, 'grad_norm': 9.398726463317871, 'learning_rate': 2.548720186154741e-05, 'epoch': 1.47}\n",
      "{'loss': 3.167, 'grad_norm': 8.921658515930176, 'learning_rate': 2.5479930191972078e-05, 'epoch': 1.47}\n",
      "{'loss': 3.0838, 'grad_norm': 9.642807960510254, 'learning_rate': 2.547265852239674e-05, 'epoch': 1.47}\n",
      "{'loss': 3.0717, 'grad_norm': 8.446049690246582, 'learning_rate': 2.546538685282141e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1777, 'grad_norm': 8.767833709716797, 'learning_rate': 2.545811518324607e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1281, 'grad_norm': 10.693439483642578, 'learning_rate': 2.5450843513670742e-05, 'epoch': 1.47}\n",
      "{'loss': 3.2172, 'grad_norm': 9.506049156188965, 'learning_rate': 2.5443571844095405e-05, 'epoch': 1.47}\n",
      "{'loss': 3.2652, 'grad_norm': 8.76954174041748, 'learning_rate': 2.5436300174520072e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1156, 'grad_norm': 8.574664115905762, 'learning_rate': 2.5429028504944736e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1602, 'grad_norm': 8.805685997009277, 'learning_rate': 2.5421756835369403e-05, 'epoch': 1.47}\n",
      "{'loss': 3.1135, 'grad_norm': 9.068984031677246, 'learning_rate': 2.5414485165794066e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0111, 'grad_norm': 8.590673446655273, 'learning_rate': 2.5407213496218736e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1625, 'grad_norm': 8.515443801879883, 'learning_rate': 2.5399941826643396e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0801, 'grad_norm': 8.921560287475586, 'learning_rate': 2.5392670157068067e-05, 'epoch': 1.48}\n",
      "{'loss': 3.2135, 'grad_norm': 9.39750862121582, 'learning_rate': 2.538539848749273e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1967, 'grad_norm': 8.616498947143555, 'learning_rate': 2.5378126817917397e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0822, 'grad_norm': 9.626104354858398, 'learning_rate': 2.537085514834206e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1602, 'grad_norm': 8.686609268188477, 'learning_rate': 2.5363583478766728e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1098, 'grad_norm': 9.303567886352539, 'learning_rate': 2.535631180919139e-05, 'epoch': 1.48}\n",
      "{'loss': 3.2021, 'grad_norm': 9.588727951049805, 'learning_rate': 2.5349040139616058e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0801, 'grad_norm': 9.453237533569336, 'learning_rate': 2.534176847004072e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1437, 'grad_norm': 9.801567077636719, 'learning_rate': 2.5334496800465392e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1334, 'grad_norm': 9.331988334655762, 'learning_rate': 2.5327225130890052e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1404, 'grad_norm': 10.29226016998291, 'learning_rate': 2.5319953461314722e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1557, 'grad_norm': 8.4446382522583, 'learning_rate': 2.5312681791739386e-05, 'epoch': 1.48}\n",
      "{'loss': 3.2125, 'grad_norm': 9.630228996276855, 'learning_rate': 2.5305410122164053e-05, 'epoch': 1.48}\n",
      "{'loss': 3.1781, 'grad_norm': 8.641915321350098, 'learning_rate': 2.5298138452588716e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0734, 'grad_norm': 9.136085510253906, 'learning_rate': 2.5290866783013383e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0484, 'grad_norm': 9.402539253234863, 'learning_rate': 2.5283595113438046e-05, 'epoch': 1.48}\n",
      "{'loss': 2.9756, 'grad_norm': 9.060858726501465, 'learning_rate': 2.527632344386271e-05, 'epoch': 1.48}\n",
      " 49%|████████████████▎                | 34000/68760 [6:19:19<6:11:07,  1.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.23s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.35s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.509482000000006, 'eval_rouge-2': 7.628990000000001, 'eval_rouge-l': 26.011266, 'eval_bleu-4': 0.038448094228739516, 'eval_runtime': 7.4915, 'eval_samples_per_second': 6.674, 'eval_steps_per_second': 0.534, 'epoch': 1.48}\n",
      " 49%|████████████████▎                | 34000/68760 [6:19:27<6:11:07,  1.56it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.32s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-34000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0578, 'grad_norm': 9.499759674072266, 'learning_rate': 2.5269051774287377e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0127, 'grad_norm': 9.853803634643555, 'learning_rate': 2.526178010471204e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0762, 'grad_norm': 10.116658210754395, 'learning_rate': 2.5254508435136707e-05, 'epoch': 1.48}\n",
      "{'loss': 3.0713, 'grad_norm': 9.769061088562012, 'learning_rate': 2.524723676556137e-05, 'epoch': 1.49}\n",
      "{'loss': 3.0512, 'grad_norm': 10.472904205322266, 'learning_rate': 2.523996509598604e-05, 'epoch': 1.49}\n",
      "{'loss': 3.0611, 'grad_norm': 9.323663711547852, 'learning_rate': 2.52326934264107e-05, 'epoch': 1.49}\n",
      "{'loss': 3.099, 'grad_norm': 9.341460227966309, 'learning_rate': 2.522542175683537e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1168, 'grad_norm': 9.300025939941406, 'learning_rate': 2.5218150087260035e-05, 'epoch': 1.49}\n",
      "{'loss': 3.0254, 'grad_norm': 9.121115684509277, 'learning_rate': 2.5210878417684702e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1248, 'grad_norm': 8.912120819091797, 'learning_rate': 2.5203606748109365e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1332, 'grad_norm': 8.452801704406738, 'learning_rate': 2.5196335078534032e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1523, 'grad_norm': 9.376947402954102, 'learning_rate': 2.5189063408958696e-05, 'epoch': 1.49}\n",
      "{'loss': 3.042, 'grad_norm': 9.520987510681152, 'learning_rate': 2.5181791739383366e-05, 'epoch': 1.49}\n",
      "{'loss': 3.018, 'grad_norm': 9.470457077026367, 'learning_rate': 2.5174520069808026e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1422, 'grad_norm': 8.600412368774414, 'learning_rate': 2.5167248400232696e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1762, 'grad_norm': 8.879642486572266, 'learning_rate': 2.515997673065736e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1424, 'grad_norm': 9.747700691223145, 'learning_rate': 2.5152705061082027e-05, 'epoch': 1.49}\n",
      "{'loss': 3.057, 'grad_norm': 8.926936149597168, 'learning_rate': 2.514543339150669e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1393, 'grad_norm': 9.433915138244629, 'learning_rate': 2.5138161721931357e-05, 'epoch': 1.49}\n",
      "{'loss': 3.223, 'grad_norm': 8.395071983337402, 'learning_rate': 2.513089005235602e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1006, 'grad_norm': 12.401951789855957, 'learning_rate': 2.5123618382780688e-05, 'epoch': 1.49}\n",
      "{'loss': 3.0482, 'grad_norm': 9.06727409362793, 'learning_rate': 2.511634671320535e-05, 'epoch': 1.49}\n",
      "{'loss': 3.1746, 'grad_norm': 8.5482816696167, 'learning_rate': 2.510907504363002e-05, 'epoch': 1.49}\n",
      "{'loss': 3.2342, 'grad_norm': 9.687895774841309, 'learning_rate': 2.510180337405468e-05, 'epoch': 1.49}\n",
      "{'loss': 3.068, 'grad_norm': 9.8064603805542, 'learning_rate': 2.5094531704479352e-05, 'epoch': 1.49}\n",
      "{'loss': 3.0604, 'grad_norm': 9.367977142333984, 'learning_rate': 2.5087260034904015e-05, 'epoch': 1.49}\n",
      "{'loss': 3.0775, 'grad_norm': 9.657711029052734, 'learning_rate': 2.5079988365328682e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0082, 'grad_norm': 8.395317077636719, 'learning_rate': 2.5072716695753346e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0152, 'grad_norm': 8.45728874206543, 'learning_rate': 2.5065445026178013e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2104, 'grad_norm': 9.00243091583252, 'learning_rate': 2.5058173356602676e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2201, 'grad_norm': 8.198565483093262, 'learning_rate': 2.5050901687027346e-05, 'epoch': 1.5}\n",
      "{'loss': 3.1895, 'grad_norm': 9.338725090026855, 'learning_rate': 2.5043630017452007e-05, 'epoch': 1.5}\n",
      "{'loss': 3.1824, 'grad_norm': 9.2175931930542, 'learning_rate': 2.5036358347876677e-05, 'epoch': 1.5}\n",
      "{'loss': 3.1506, 'grad_norm': 12.579231262207031, 'learning_rate': 2.502908667830134e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0668, 'grad_norm': 8.529518127441406, 'learning_rate': 2.5021815008726007e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2092, 'grad_norm': 10.095401763916016, 'learning_rate': 2.501454333915067e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0758, 'grad_norm': 8.579144477844238, 'learning_rate': 2.5007271669575338e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0359, 'grad_norm': 9.753552436828613, 'learning_rate': 2.5e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2463, 'grad_norm': 9.832927703857422, 'learning_rate': 2.4992728330424668e-05, 'epoch': 1.5}\n",
      "{'loss': 3.1033, 'grad_norm': 9.617066383361816, 'learning_rate': 2.498545666084933e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0703, 'grad_norm': 9.09569263458252, 'learning_rate': 2.4978184991274e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0957, 'grad_norm': 9.43118953704834, 'learning_rate': 2.4970913321698662e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0998, 'grad_norm': 8.882214546203613, 'learning_rate': 2.496364165212333e-05, 'epoch': 1.5}\n",
      "{'loss': 3.2051, 'grad_norm': 9.314024925231934, 'learning_rate': 2.4956369982547996e-05, 'epoch': 1.5}\n",
      "{'loss': 3.1564, 'grad_norm': 8.672115325927734, 'learning_rate': 2.494909831297266e-05, 'epoch': 1.5}\n",
      "{'loss': 3.009, 'grad_norm': 9.934258460998535, 'learning_rate': 2.4941826643397326e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0891, 'grad_norm': 9.423405647277832, 'learning_rate': 2.493455497382199e-05, 'epoch': 1.5}\n",
      "{'loss': 3.1344, 'grad_norm': 8.50539493560791, 'learning_rate': 2.4927283304246657e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0412, 'grad_norm': 8.593273162841797, 'learning_rate': 2.4920011634671323e-05, 'epoch': 1.5}\n",
      "{'loss': 3.0893, 'grad_norm': 8.15798282623291, 'learning_rate': 2.4912739965095987e-05, 'epoch': 1.51}\n",
      " 50%|████████████████▌                | 34500/68760 [6:24:39<5:40:56,  1.67it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.16s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.844500000000004, 'eval_rouge-2': 7.7323580000000005, 'eval_rouge-l': 25.692076, 'eval_bleu-4': 0.03775559433492695, 'eval_runtime': 16.4852, 'eval_samples_per_second': 3.033, 'eval_steps_per_second': 0.243, 'epoch': 1.51}\n",
      " 50%|████████████████▌                | 34500/68760 [6:24:56<5:40:56,  1.67it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  2.98s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-34500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0965, 'grad_norm': 10.212384223937988, 'learning_rate': 2.4905468295520654e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2346, 'grad_norm': 9.614431381225586, 'learning_rate': 2.489819662594532e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1453, 'grad_norm': 8.57448673248291, 'learning_rate': 2.4890924956369984e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1844, 'grad_norm': 9.417745590209961, 'learning_rate': 2.488365328679465e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1133, 'grad_norm': 8.903387069702148, 'learning_rate': 2.4876381617219315e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1291, 'grad_norm': 8.653570175170898, 'learning_rate': 2.486910994764398e-05, 'epoch': 1.51}\n",
      "{'loss': 3.0695, 'grad_norm': 8.831512451171875, 'learning_rate': 2.486183827806865e-05, 'epoch': 1.51}\n",
      "{'loss': 3.0469, 'grad_norm': 8.697898864746094, 'learning_rate': 2.485456660849331e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1469, 'grad_norm': 8.662592887878418, 'learning_rate': 2.4847294938917976e-05, 'epoch': 1.51}\n",
      "{'loss': 3.134, 'grad_norm': 9.226423263549805, 'learning_rate': 2.4840023269342642e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2834, 'grad_norm': 9.361199378967285, 'learning_rate': 2.4832751599767306e-05, 'epoch': 1.51}\n",
      "{'loss': 3.107, 'grad_norm': 10.25455093383789, 'learning_rate': 2.4825479930191973e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2225, 'grad_norm': 8.407913208007812, 'learning_rate': 2.4818208260616636e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1707, 'grad_norm': 9.685506820678711, 'learning_rate': 2.4810936591041303e-05, 'epoch': 1.51}\n",
      "{'loss': 3.0084, 'grad_norm': 10.457019805908203, 'learning_rate': 2.480366492146597e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1047, 'grad_norm': 9.329787254333496, 'learning_rate': 2.4796393251890634e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2283, 'grad_norm': 9.517354965209961, 'learning_rate': 2.47891215823153e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1441, 'grad_norm': 9.263633728027344, 'learning_rate': 2.4781849912739964e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2793, 'grad_norm': 8.803145408630371, 'learning_rate': 2.477457824316463e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1672, 'grad_norm': 8.222095489501953, 'learning_rate': 2.4767306573589298e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1404, 'grad_norm': 9.030336380004883, 'learning_rate': 2.476003490401396e-05, 'epoch': 1.51}\n",
      "{'loss': 3.2359, 'grad_norm': 9.23719310760498, 'learning_rate': 2.4752763234438628e-05, 'epoch': 1.51}\n",
      "{'loss': 3.1617, 'grad_norm': 9.347953796386719, 'learning_rate': 2.4745491564863295e-05, 'epoch': 1.52}\n",
      "{'loss': 3.0459, 'grad_norm': 11.035603523254395, 'learning_rate': 2.473821989528796e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1832, 'grad_norm': 8.616141319274902, 'learning_rate': 2.4730948225712626e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1703, 'grad_norm': 9.195950508117676, 'learning_rate': 2.472367655613729e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1793, 'grad_norm': 9.646039009094238, 'learning_rate': 2.4716404886561956e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1092, 'grad_norm': 9.383211135864258, 'learning_rate': 2.4709133216986623e-05, 'epoch': 1.52}\n",
      "{'loss': 3.0055, 'grad_norm': 10.088325500488281, 'learning_rate': 2.4701861547411286e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1324, 'grad_norm': 8.91346263885498, 'learning_rate': 2.4694589877835953e-05, 'epoch': 1.52}\n",
      "{'loss': 3.0553, 'grad_norm': 9.293838500976562, 'learning_rate': 2.4687318208260617e-05, 'epoch': 1.52}\n",
      "{'loss': 3.007, 'grad_norm': 8.75641918182373, 'learning_rate': 2.4680046538685284e-05, 'epoch': 1.52}\n",
      "{'loss': 3.0775, 'grad_norm': 9.01545238494873, 'learning_rate': 2.467277486910995e-05, 'epoch': 1.52}\n",
      "{'loss': 3.0451, 'grad_norm': 9.597847938537598, 'learning_rate': 2.4665503199534614e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2291, 'grad_norm': 8.81793212890625, 'learning_rate': 2.465823152995928e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1303, 'grad_norm': 8.39167594909668, 'learning_rate': 2.4650959860383944e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2209, 'grad_norm': 10.379892349243164, 'learning_rate': 2.464368819080861e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1227, 'grad_norm': 9.734944343566895, 'learning_rate': 2.4636416521233278e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2348, 'grad_norm': 8.934161186218262, 'learning_rate': 2.4629144851657942e-05, 'epoch': 1.52}\n",
      "{'loss': 3.0738, 'grad_norm': 8.672285079956055, 'learning_rate': 2.462187318208261e-05, 'epoch': 1.52}\n",
      "{'loss': 3.0857, 'grad_norm': 9.023411750793457, 'learning_rate': 2.4614601512507272e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1928, 'grad_norm': 8.750829696655273, 'learning_rate': 2.460732984293194e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2176, 'grad_norm': 9.598932266235352, 'learning_rate': 2.4600058173356606e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1418, 'grad_norm': 9.17931079864502, 'learning_rate': 2.459278650378127e-05, 'epoch': 1.52}\n",
      "{'loss': 3.1467, 'grad_norm': 9.423125267028809, 'learning_rate': 2.4585514834205936e-05, 'epoch': 1.52}\n",
      "{'loss': 3.2, 'grad_norm': 8.81566333770752, 'learning_rate': 2.4578243164630603e-05, 'epoch': 1.53}\n",
      "{'loss': 3.0125, 'grad_norm': 8.928074836730957, 'learning_rate': 2.4570971495055267e-05, 'epoch': 1.53}\n",
      "{'loss': 2.9852, 'grad_norm': 8.900253295898438, 'learning_rate': 2.456369982547993e-05, 'epoch': 1.53}\n",
      "{'loss': 3.1943, 'grad_norm': 9.903757095336914, 'learning_rate': 2.4556428155904597e-05, 'epoch': 1.53}\n",
      "{'loss': 3.2338, 'grad_norm': 9.523401260375977, 'learning_rate': 2.454915648632926e-05, 'epoch': 1.53}\n",
      " 51%|████████████████▊                | 35000/68760 [6:30:06<6:00:00,  1.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.16s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.26585600000001, 'eval_rouge-2': 7.908002, 'eval_rouge-l': 25.888133999999994, 'eval_bleu-4': 0.03781775530219595, 'eval_runtime': 16.8627, 'eval_samples_per_second': 2.965, 'eval_steps_per_second': 0.237, 'epoch': 1.53}\n",
      " 51%|████████████████▊                | 35000/68760 [6:30:23<6:00:00,  1.56it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.10s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-35000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1473, 'grad_norm': 9.412345886230469, 'learning_rate': 2.4541884816753928e-05, 'epoch': 1.53}\n",
      "{'loss': 2.9441, 'grad_norm': 8.814056396484375, 'learning_rate': 2.453461314717859e-05, 'epoch': 1.53}\n",
      "{'loss': 2.9689, 'grad_norm': 9.915855407714844, 'learning_rate': 2.4527341477603258e-05, 'epoch': 1.53}\n",
      "{'loss': 3.2824, 'grad_norm': 9.30471420288086, 'learning_rate': 2.4520069808027925e-05, 'epoch': 1.53}\n",
      "{'loss': 3.0164, 'grad_norm': 9.113855361938477, 'learning_rate': 2.451279813845259e-05, 'epoch': 1.53}\n",
      "{'loss': 3.2559, 'grad_norm': 9.879032135009766, 'learning_rate': 2.4505526468877255e-05, 'epoch': 1.53}\n",
      "{'loss': 3.1488, 'grad_norm': 9.329928398132324, 'learning_rate': 2.449825479930192e-05, 'epoch': 1.53}\n",
      "{'loss': 3.1734, 'grad_norm': 10.455867767333984, 'learning_rate': 2.4490983129726586e-05, 'epoch': 1.53}\n",
      "{'loss': 3.0514, 'grad_norm': 8.519428253173828, 'learning_rate': 2.4483711460151253e-05, 'epoch': 1.53}\n",
      "{'loss': 3.0354, 'grad_norm': 8.682098388671875, 'learning_rate': 2.4476439790575916e-05, 'epoch': 1.53}\n",
      "{'loss': 3.0979, 'grad_norm': 9.180248260498047, 'learning_rate': 2.4469168121000583e-05, 'epoch': 1.53}\n",
      "{'loss': 3.1039, 'grad_norm': 10.326480865478516, 'learning_rate': 2.4461896451425246e-05, 'epoch': 1.53}\n",
      "{'loss': 3.2145, 'grad_norm': 9.466595649719238, 'learning_rate': 2.4454624781849913e-05, 'epoch': 1.53}\n",
      "{'loss': 3.1186, 'grad_norm': 8.42875862121582, 'learning_rate': 2.444735311227458e-05, 'epoch': 1.53}\n",
      "{'loss': 2.8619, 'grad_norm': 8.735581398010254, 'learning_rate': 2.4440081442699244e-05, 'epoch': 1.53}\n",
      "{'loss': 3.0928, 'grad_norm': 10.470029830932617, 'learning_rate': 2.443280977312391e-05, 'epoch': 1.53}\n",
      "{'loss': 3.0359, 'grad_norm': 9.359933853149414, 'learning_rate': 2.4425538103548578e-05, 'epoch': 1.53}\n",
      "{'loss': 3.1711, 'grad_norm': 8.946884155273438, 'learning_rate': 2.441826643397324e-05, 'epoch': 1.53}\n",
      "{'loss': 3.26, 'grad_norm': 9.509115219116211, 'learning_rate': 2.4410994764397908e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1721, 'grad_norm': 8.81852912902832, 'learning_rate': 2.440372309482257e-05, 'epoch': 1.54}\n",
      "{'loss': 3.16, 'grad_norm': 9.84439754486084, 'learning_rate': 2.439645142524724e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1215, 'grad_norm': 9.499499320983887, 'learning_rate': 2.4389179755671905e-05, 'epoch': 1.54}\n",
      "{'loss': 3.2223, 'grad_norm': 9.128060340881348, 'learning_rate': 2.438190808609657e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1596, 'grad_norm': 9.587613105773926, 'learning_rate': 2.4374636416521236e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1057, 'grad_norm': 9.259099960327148, 'learning_rate': 2.43673647469459e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1779, 'grad_norm': 8.831072807312012, 'learning_rate': 2.4360093077370566e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1039, 'grad_norm': 10.04252815246582, 'learning_rate': 2.4352821407795233e-05, 'epoch': 1.54}\n",
      "{'loss': 3.0871, 'grad_norm': 8.791829109191895, 'learning_rate': 2.4345549738219896e-05, 'epoch': 1.54}\n",
      "{'loss': 3.025, 'grad_norm': 9.6951265335083, 'learning_rate': 2.4338278068644563e-05, 'epoch': 1.54}\n",
      "{'loss': 3.0873, 'grad_norm': 9.034029960632324, 'learning_rate': 2.4331006399069227e-05, 'epoch': 1.54}\n",
      "{'loss': 3.126, 'grad_norm': 10.2719144821167, 'learning_rate': 2.4323734729493894e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1146, 'grad_norm': 9.728448867797852, 'learning_rate': 2.431646305991856e-05, 'epoch': 1.54}\n",
      "{'loss': 3.0645, 'grad_norm': 8.86862850189209, 'learning_rate': 2.4309191390343224e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1309, 'grad_norm': 10.058428764343262, 'learning_rate': 2.430191972076789e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1434, 'grad_norm': 9.150471687316895, 'learning_rate': 2.4294648051192555e-05, 'epoch': 1.54}\n",
      "{'loss': 3.2131, 'grad_norm': 8.621009826660156, 'learning_rate': 2.428737638161722e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1264, 'grad_norm': 9.372063636779785, 'learning_rate': 2.428010471204189e-05, 'epoch': 1.54}\n",
      "{'loss': 3.2287, 'grad_norm': 11.680731773376465, 'learning_rate': 2.427283304246655e-05, 'epoch': 1.54}\n",
      "{'loss': 3.2055, 'grad_norm': 9.264114379882812, 'learning_rate': 2.4265561372891215e-05, 'epoch': 1.54}\n",
      "{'loss': 3.1822, 'grad_norm': 8.697297096252441, 'learning_rate': 2.4258289703315882e-05, 'epoch': 1.54}\n",
      "{'loss': 3.2574, 'grad_norm': 9.36916446685791, 'learning_rate': 2.4251018033740546e-05, 'epoch': 1.54}\n",
      "{'loss': 2.9824, 'grad_norm': 9.358266830444336, 'learning_rate': 2.4243746364165213e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2738, 'grad_norm': 11.72370719909668, 'learning_rate': 2.423647469458988e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0713, 'grad_norm': 9.404770851135254, 'learning_rate': 2.4229203025014543e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1221, 'grad_norm': 9.930278778076172, 'learning_rate': 2.422193135543921e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1336, 'grad_norm': 8.895809173583984, 'learning_rate': 2.4214659685863873e-05, 'epoch': 1.55}\n",
      "{'loss': 2.9803, 'grad_norm': 9.265847206115723, 'learning_rate': 2.420738801628854e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2035, 'grad_norm': 8.594801902770996, 'learning_rate': 2.4200116346713207e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2078, 'grad_norm': 8.924631118774414, 'learning_rate': 2.419284467713787e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0561, 'grad_norm': 9.429875373840332, 'learning_rate': 2.4185573007562538e-05, 'epoch': 1.55}\n",
      " 52%|█████████████████                | 35500/68760 [6:35:34<5:34:29,  1.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.32s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.44s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.178217999999994, 'eval_rouge-2': 7.635662000000001, 'eval_rouge-l': 25.784126, 'eval_bleu-4': 0.03673740553483396, 'eval_runtime': 23.5984, 'eval_samples_per_second': 2.119, 'eval_steps_per_second': 0.17, 'epoch': 1.55}\n",
      " 52%|█████████████████                | 35500/68760 [6:35:57<5:34:29,  1.66it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:11<00:00,  3.51s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-35500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0045, 'grad_norm': 9.70744514465332, 'learning_rate': 2.41783013379872e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0551, 'grad_norm': 8.746370315551758, 'learning_rate': 2.4171029668411868e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0072, 'grad_norm': 9.991959571838379, 'learning_rate': 2.4163757998836535e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0818, 'grad_norm': 10.1297607421875, 'learning_rate': 2.41564863292612e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1799, 'grad_norm': 10.785225868225098, 'learning_rate': 2.4149214659685865e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1682, 'grad_norm': 9.149396896362305, 'learning_rate': 2.414194299011053e-05, 'epoch': 1.55}\n",
      "{'loss': 3.09, 'grad_norm': 8.355264663696289, 'learning_rate': 2.4134671320535196e-05, 'epoch': 1.55}\n",
      "{'loss': 3.0984, 'grad_norm': 9.51220989227295, 'learning_rate': 2.4127399650959863e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1453, 'grad_norm': 8.531655311584473, 'learning_rate': 2.4120127981384526e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1498, 'grad_norm': 8.558743476867676, 'learning_rate': 2.4112856311809193e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1594, 'grad_norm': 9.306478500366211, 'learning_rate': 2.410558464223386e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2918, 'grad_norm': 8.96019172668457, 'learning_rate': 2.4098312972658523e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1416, 'grad_norm': 8.596320152282715, 'learning_rate': 2.409104130308319e-05, 'epoch': 1.55}\n",
      "{'loss': 3.1021, 'grad_norm': 8.621785163879395, 'learning_rate': 2.4083769633507854e-05, 'epoch': 1.55}\n",
      "{'loss': 3.2625, 'grad_norm': 9.954669952392578, 'learning_rate': 2.407649796393252e-05, 'epoch': 1.56}\n",
      "{'loss': 3.0701, 'grad_norm': 8.26193618774414, 'learning_rate': 2.4069226294357188e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1719, 'grad_norm': 8.73497200012207, 'learning_rate': 2.406195462478185e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1703, 'grad_norm': 10.727910995483398, 'learning_rate': 2.4054682955206518e-05, 'epoch': 1.56}\n",
      "{'loss': 3.0775, 'grad_norm': 9.851502418518066, 'learning_rate': 2.404741128563118e-05, 'epoch': 1.56}\n",
      "{'loss': 3.0199, 'grad_norm': 9.007699966430664, 'learning_rate': 2.404013961605585e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1541, 'grad_norm': 10.438783645629883, 'learning_rate': 2.4032867946480515e-05, 'epoch': 1.56}\n",
      "{'loss': 3.0398, 'grad_norm': 8.74796199798584, 'learning_rate': 2.402559627690518e-05, 'epoch': 1.56}\n",
      "{'loss': 3.2172, 'grad_norm': 9.137033462524414, 'learning_rate': 2.4018324607329846e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1916, 'grad_norm': 9.278307914733887, 'learning_rate': 2.401105293775451e-05, 'epoch': 1.56}\n",
      "{'loss': 3.2043, 'grad_norm': 9.554621696472168, 'learning_rate': 2.4003781268179176e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1928, 'grad_norm': 9.139082908630371, 'learning_rate': 2.3996509598603843e-05, 'epoch': 1.56}\n",
      "{'loss': 3.0971, 'grad_norm': 9.084973335266113, 'learning_rate': 2.3989237929028507e-05, 'epoch': 1.56}\n",
      "{'loss': 3.042, 'grad_norm': 9.52774429321289, 'learning_rate': 2.398196625945317e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1957, 'grad_norm': 9.478424072265625, 'learning_rate': 2.3974694589877837e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1031, 'grad_norm': 9.610973358154297, 'learning_rate': 2.39674229203025e-05, 'epoch': 1.56}\n",
      "{'loss': 3.033, 'grad_norm': 9.466572761535645, 'learning_rate': 2.3960151250727167e-05, 'epoch': 1.56}\n",
      "{'loss': 3.0951, 'grad_norm': 8.834794998168945, 'learning_rate': 2.395287958115183e-05, 'epoch': 1.56}\n",
      "{'loss': 3.0215, 'grad_norm': 10.651203155517578, 'learning_rate': 2.3945607911576498e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1432, 'grad_norm': 9.345842361450195, 'learning_rate': 2.3938336242001165e-05, 'epoch': 1.56}\n",
      "{'loss': 2.9623, 'grad_norm': 8.385075569152832, 'learning_rate': 2.3931064572425828e-05, 'epoch': 1.56}\n",
      "{'loss': 3.0068, 'grad_norm': 9.886194229125977, 'learning_rate': 2.3923792902850495e-05, 'epoch': 1.56}\n",
      "{'loss': 3.1527, 'grad_norm': 9.02235221862793, 'learning_rate': 2.3916521233275162e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0369, 'grad_norm': 9.03396224975586, 'learning_rate': 2.3909249563699825e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0303, 'grad_norm': 10.768548965454102, 'learning_rate': 2.3901977894124492e-05, 'epoch': 1.57}\n",
      "{'loss': 3.2113, 'grad_norm': 10.197526931762695, 'learning_rate': 2.3894706224549156e-05, 'epoch': 1.57}\n",
      "{'loss': 3.133, 'grad_norm': 8.722138404846191, 'learning_rate': 2.3887434554973823e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0773, 'grad_norm': 8.189898490905762, 'learning_rate': 2.388016288539849e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0873, 'grad_norm': 9.186307907104492, 'learning_rate': 2.3872891215823153e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0828, 'grad_norm': 8.64911937713623, 'learning_rate': 2.386561954624782e-05, 'epoch': 1.57}\n",
      "{'loss': 3.1043, 'grad_norm': 8.656471252441406, 'learning_rate': 2.3858347876672484e-05, 'epoch': 1.57}\n",
      "{'loss': 3.1146, 'grad_norm': 9.045624732971191, 'learning_rate': 2.385107620709715e-05, 'epoch': 1.57}\n",
      "{'loss': 3.1908, 'grad_norm': 9.733688354492188, 'learning_rate': 2.3843804537521817e-05, 'epoch': 1.57}\n",
      "{'loss': 2.9766, 'grad_norm': 10.22603988647461, 'learning_rate': 2.383653286794648e-05, 'epoch': 1.57}\n",
      "{'loss': 3.1467, 'grad_norm': 9.306568145751953, 'learning_rate': 2.3829261198371148e-05, 'epoch': 1.57}\n",
      "{'loss': 2.9861, 'grad_norm': 9.894647598266602, 'learning_rate': 2.382198952879581e-05, 'epoch': 1.57}\n",
      " 52%|█████████████████▎               | 36000/68760 [6:41:08<5:26:58,  1.67it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.07s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.84s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.341016, 'eval_rouge-2': 8.15584, 'eval_rouge-l': 25.942745999999996, 'eval_bleu-4': 0.03940821074047796, 'eval_runtime': 9.5117, 'eval_samples_per_second': 5.257, 'eval_steps_per_second': 0.421, 'epoch': 1.57}\n",
      " 52%|█████████████████▎               | 36000/68760 [6:41:17<5:26:58,  1.67it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.61s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-36000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2326, 'grad_norm': 9.542771339416504, 'learning_rate': 2.3814717859220478e-05, 'epoch': 1.57}\n",
      "{'loss': 3.2582, 'grad_norm': 9.174798011779785, 'learning_rate': 2.3807446189645145e-05, 'epoch': 1.57}\n",
      "{'loss': 3.1076, 'grad_norm': 9.347732543945312, 'learning_rate': 2.380017452006981e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0746, 'grad_norm': 8.826709747314453, 'learning_rate': 2.3792902850494475e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0576, 'grad_norm': 9.371465682983398, 'learning_rate': 2.3785631180919142e-05, 'epoch': 1.57}\n",
      "{'loss': 3.0439, 'grad_norm': 9.61032772064209, 'learning_rate': 2.3778359511343806e-05, 'epoch': 1.57}\n",
      "{'loss': 3.1436, 'grad_norm': 9.363569259643555, 'learning_rate': 2.3771087841768473e-05, 'epoch': 1.57}\n",
      "{'loss': 2.9723, 'grad_norm': 10.48630142211914, 'learning_rate': 2.3763816172193136e-05, 'epoch': 1.57}\n",
      "{'loss': 3.042, 'grad_norm': 9.974977493286133, 'learning_rate': 2.3756544502617803e-05, 'epoch': 1.57}\n",
      "{'loss': 2.9346, 'grad_norm': 9.38549518585205, 'learning_rate': 2.374927283304247e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1672, 'grad_norm': 10.404440879821777, 'learning_rate': 2.3742001163467134e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1539, 'grad_norm': 9.444696426391602, 'learning_rate': 2.37347294938918e-05, 'epoch': 1.58}\n",
      "{'loss': 3.233, 'grad_norm': 8.783534049987793, 'learning_rate': 2.3727457824316464e-05, 'epoch': 1.58}\n",
      "{'loss': 2.973, 'grad_norm': 9.546930313110352, 'learning_rate': 2.372018615474113e-05, 'epoch': 1.58}\n",
      "{'loss': 3.0537, 'grad_norm': 9.989166259765625, 'learning_rate': 2.3712914485165798e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1004, 'grad_norm': 8.775607109069824, 'learning_rate': 2.370564281559046e-05, 'epoch': 1.58}\n",
      "{'loss': 3.0824, 'grad_norm': 9.712213516235352, 'learning_rate': 2.3698371146015128e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1973, 'grad_norm': 8.771710395812988, 'learning_rate': 2.369109947643979e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1615, 'grad_norm': 8.320221900939941, 'learning_rate': 2.3683827806864455e-05, 'epoch': 1.58}\n",
      "{'loss': 3.2201, 'grad_norm': 8.508391380310059, 'learning_rate': 2.3676556137289122e-05, 'epoch': 1.58}\n",
      "{'loss': 3.0262, 'grad_norm': 10.090014457702637, 'learning_rate': 2.3669284467713786e-05, 'epoch': 1.58}\n",
      "{'loss': 3.185, 'grad_norm': 8.912653923034668, 'learning_rate': 2.3662012798138452e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1408, 'grad_norm': 8.937853813171387, 'learning_rate': 2.365474112856312e-05, 'epoch': 1.58}\n",
      "{'loss': 2.9713, 'grad_norm': 9.810460090637207, 'learning_rate': 2.3647469458987783e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1857, 'grad_norm': 9.621208190917969, 'learning_rate': 2.364019778941245e-05, 'epoch': 1.58}\n",
      "{'loss': 3.0732, 'grad_norm': 10.196380615234375, 'learning_rate': 2.3632926119837113e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1795, 'grad_norm': 10.572859764099121, 'learning_rate': 2.362565445026178e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1242, 'grad_norm': 9.762304306030273, 'learning_rate': 2.3618382780686447e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1354, 'grad_norm': 8.663175582885742, 'learning_rate': 2.361111111111111e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1014, 'grad_norm': 9.186714172363281, 'learning_rate': 2.3603839441535777e-05, 'epoch': 1.58}\n",
      "{'loss': 3.224, 'grad_norm': 10.422901153564453, 'learning_rate': 2.3596567771960444e-05, 'epoch': 1.58}\n",
      "{'loss': 3.2004, 'grad_norm': 9.572624206542969, 'learning_rate': 2.3589296102385108e-05, 'epoch': 1.58}\n",
      "{'loss': 3.1691, 'grad_norm': 8.75269603729248, 'learning_rate': 2.3582024432809775e-05, 'epoch': 1.59}\n",
      "{'loss': 2.9541, 'grad_norm': 9.61473274230957, 'learning_rate': 2.3574752763234438e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0023, 'grad_norm': 9.63456916809082, 'learning_rate': 2.3567481093659105e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1152, 'grad_norm': 10.093611717224121, 'learning_rate': 2.3560209424083772e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1918, 'grad_norm': 9.803084373474121, 'learning_rate': 2.3552937754508436e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0811, 'grad_norm': 8.688573837280273, 'learning_rate': 2.3545666084933102e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1387, 'grad_norm': 9.529886245727539, 'learning_rate': 2.3538394415357766e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0934, 'grad_norm': 8.7944974899292, 'learning_rate': 2.3531122745782433e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1674, 'grad_norm': 9.13862419128418, 'learning_rate': 2.35238510762071e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1457, 'grad_norm': 9.972494125366211, 'learning_rate': 2.3516579406631763e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0215, 'grad_norm': 10.240035057067871, 'learning_rate': 2.350930773705643e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0666, 'grad_norm': 8.568252563476562, 'learning_rate': 2.3502036067481094e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1193, 'grad_norm': 10.174041748046875, 'learning_rate': 2.349476439790576e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1105, 'grad_norm': 9.942152976989746, 'learning_rate': 2.3487492728330427e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0521, 'grad_norm': 8.212836265563965, 'learning_rate': 2.348022105875509e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0797, 'grad_norm': 9.390527725219727, 'learning_rate': 2.3472949389179758e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1287, 'grad_norm': 9.085617065429688, 'learning_rate': 2.3465677719604425e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0432, 'grad_norm': 9.508207321166992, 'learning_rate': 2.3458406050029088e-05, 'epoch': 1.59}\n",
      " 53%|█████████████████▌               | 36500/68760 [6:46:27<5:36:22,  1.60it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.12it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.52s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.700033999999995, 'eval_rouge-2': 7.779574, 'eval_rouge-l': 26.030079999999998, 'eval_bleu-4': 0.037019177005577525, 'eval_runtime': 17.6822, 'eval_samples_per_second': 2.828, 'eval_steps_per_second': 0.226, 'epoch': 1.59}\n",
      " 53%|█████████████████▌               | 36500/68760 [6:46:45<5:36:22,  1.60it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.37s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-36500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1617, 'grad_norm': 9.037190437316895, 'learning_rate': 2.3451134380453755e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0748, 'grad_norm': 8.850662231445312, 'learning_rate': 2.344386271087842e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1824, 'grad_norm': 9.404679298400879, 'learning_rate': 2.3436591041303086e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1803, 'grad_norm': 8.984636306762695, 'learning_rate': 2.3429319371727752e-05, 'epoch': 1.59}\n",
      "{'loss': 3.0828, 'grad_norm': 9.818424224853516, 'learning_rate': 2.3422047702152416e-05, 'epoch': 1.59}\n",
      "{'loss': 3.1061, 'grad_norm': 9.030961036682129, 'learning_rate': 2.3414776032577083e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1508, 'grad_norm': 9.339042663574219, 'learning_rate': 2.3407504363001746e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0, 'grad_norm': 9.129595756530762, 'learning_rate': 2.340023269342641e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1484, 'grad_norm': 9.42091178894043, 'learning_rate': 2.3392961023851077e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0322, 'grad_norm': 9.246886253356934, 'learning_rate': 2.338568935427574e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1396, 'grad_norm': 10.67568588256836, 'learning_rate': 2.3378417684700407e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1475, 'grad_norm': 8.492334365844727, 'learning_rate': 2.3371146015125074e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1615, 'grad_norm': 10.205225944519043, 'learning_rate': 2.3363874345549738e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0889, 'grad_norm': 8.997535705566406, 'learning_rate': 2.3356602675974404e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0572, 'grad_norm': 9.625990867614746, 'learning_rate': 2.3349331006399068e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0246, 'grad_norm': 10.052841186523438, 'learning_rate': 2.3342059336823735e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1992, 'grad_norm': 8.662877082824707, 'learning_rate': 2.3334787667248402e-05, 'epoch': 1.6}\n",
      "{'loss': 3.117, 'grad_norm': 8.90222454071045, 'learning_rate': 2.3327515997673065e-05, 'epoch': 1.6}\n",
      "{'loss': 3.3318, 'grad_norm': 8.737066268920898, 'learning_rate': 2.3320244328097732e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1213, 'grad_norm': 10.37102222442627, 'learning_rate': 2.3312972658522396e-05, 'epoch': 1.6}\n",
      "{'loss': 3.2549, 'grad_norm': 9.205349922180176, 'learning_rate': 2.3305700988947063e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0771, 'grad_norm': 8.945588111877441, 'learning_rate': 2.329842931937173e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0359, 'grad_norm': 10.275455474853516, 'learning_rate': 2.3291157649796393e-05, 'epoch': 1.6}\n",
      "{'loss': 2.9861, 'grad_norm': 9.273425102233887, 'learning_rate': 2.328388598022106e-05, 'epoch': 1.6}\n",
      "{'loss': 3.2018, 'grad_norm': 10.67069149017334, 'learning_rate': 2.3276614310645727e-05, 'epoch': 1.6}\n",
      "{'loss': 3.1314, 'grad_norm': 13.595973014831543, 'learning_rate': 2.326934264107039e-05, 'epoch': 1.6}\n",
      "{'loss': 3.2373, 'grad_norm': 9.657870292663574, 'learning_rate': 2.3262070971495057e-05, 'epoch': 1.6}\n",
      "{'loss': 3.0686, 'grad_norm': 8.894545555114746, 'learning_rate': 2.325479930191972e-05, 'epoch': 1.6}\n",
      "{'loss': 3.2227, 'grad_norm': 8.931017875671387, 'learning_rate': 2.3247527632344388e-05, 'epoch': 1.61}\n",
      "{'loss': 3.1391, 'grad_norm': 9.584564208984375, 'learning_rate': 2.3240255962769054e-05, 'epoch': 1.61}\n",
      "{'loss': 3.2051, 'grad_norm': 11.004326820373535, 'learning_rate': 2.3232984293193718e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0727, 'grad_norm': 8.366630554199219, 'learning_rate': 2.3225712623618385e-05, 'epoch': 1.61}\n",
      "{'loss': 3.085, 'grad_norm': 8.981465339660645, 'learning_rate': 2.321844095404305e-05, 'epoch': 1.61}\n",
      "{'loss': 3.1117, 'grad_norm': 9.623578071594238, 'learning_rate': 2.3211169284467715e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0354, 'grad_norm': 8.546689987182617, 'learning_rate': 2.3203897614892382e-05, 'epoch': 1.61}\n",
      "{'loss': 3.1912, 'grad_norm': 10.720474243164062, 'learning_rate': 2.3196625945317046e-05, 'epoch': 1.61}\n",
      "{'loss': 3.14, 'grad_norm': 8.919915199279785, 'learning_rate': 2.3189354275741713e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0473, 'grad_norm': 8.854867935180664, 'learning_rate': 2.3182082606166376e-05, 'epoch': 1.61}\n",
      "{'loss': 3.115, 'grad_norm': 9.172731399536133, 'learning_rate': 2.3174810936591043e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0477, 'grad_norm': 9.536770820617676, 'learning_rate': 2.316753926701571e-05, 'epoch': 1.61}\n",
      "{'loss': 3.125, 'grad_norm': 9.254048347473145, 'learning_rate': 2.3160267597440373e-05, 'epoch': 1.61}\n",
      "{'loss': 3.2756, 'grad_norm': 9.6828031539917, 'learning_rate': 2.315299592786504e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0867, 'grad_norm': 9.582165718078613, 'learning_rate': 2.3145724258289707e-05, 'epoch': 1.61}\n",
      "{'loss': 3.082, 'grad_norm': 9.90289306640625, 'learning_rate': 2.313845258871437e-05, 'epoch': 1.61}\n",
      "{'loss': 3.1096, 'grad_norm': 8.77794361114502, 'learning_rate': 2.3131180919139038e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0438, 'grad_norm': 9.014432907104492, 'learning_rate': 2.31239092495637e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0574, 'grad_norm': 9.460235595703125, 'learning_rate': 2.3116637579988368e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0816, 'grad_norm': 9.200010299682617, 'learning_rate': 2.310936591041303e-05, 'epoch': 1.61}\n",
      "{'loss': 3.0672, 'grad_norm': 9.187702178955078, 'learning_rate': 2.3102094240837695e-05, 'epoch': 1.61}\n",
      "{'loss': 3.1789, 'grad_norm': 8.93266773223877, 'learning_rate': 2.3094822571262362e-05, 'epoch': 1.61}\n",
      " 54%|█████████████████▊               | 37000/68760 [6:51:55<5:31:29,  1.60it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.00s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.37s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.629546, 'eval_rouge-2': 7.149011999999999, 'eval_rouge-l': 25.87466, 'eval_bleu-4': 0.03642881792949448, 'eval_runtime': 17.5225, 'eval_samples_per_second': 2.853, 'eval_steps_per_second': 0.228, 'epoch': 1.61}\n",
      " 54%|█████████████████▊               | 37000/68760 [6:52:13<5:31:29,  1.60it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.33s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-37000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1473, 'grad_norm': 10.019739151000977, 'learning_rate': 2.308755090168703e-05, 'epoch': 1.61}\n",
      "{'loss': 3.1635, 'grad_norm': 10.210493087768555, 'learning_rate': 2.3080279232111692e-05, 'epoch': 1.62}\n",
      "{'loss': 3.0939, 'grad_norm': 8.325860023498535, 'learning_rate': 2.307300756253636e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1961, 'grad_norm': 9.97581672668457, 'learning_rate': 2.3065735892961023e-05, 'epoch': 1.62}\n",
      "{'loss': 2.9908, 'grad_norm': 9.335615158081055, 'learning_rate': 2.305846422338569e-05, 'epoch': 1.62}\n",
      "{'loss': 3.0217, 'grad_norm': 8.872373580932617, 'learning_rate': 2.3051192553810356e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1453, 'grad_norm': 9.376587867736816, 'learning_rate': 2.304392088423502e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1846, 'grad_norm': 8.762720108032227, 'learning_rate': 2.3036649214659687e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1807, 'grad_norm': 10.089959144592285, 'learning_rate': 2.302937754508435e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1236, 'grad_norm': 9.26517391204834, 'learning_rate': 2.3022105875509017e-05, 'epoch': 1.62}\n",
      "{'loss': 3.2348, 'grad_norm': 9.122457504272461, 'learning_rate': 2.3014834205933684e-05, 'epoch': 1.62}\n",
      "{'loss': 3.2143, 'grad_norm': 9.316975593566895, 'learning_rate': 2.3007562536358348e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1078, 'grad_norm': 10.559954643249512, 'learning_rate': 2.3000290866783015e-05, 'epoch': 1.62}\n",
      "{'loss': 3.2518, 'grad_norm': 10.586036682128906, 'learning_rate': 2.2993019197207678e-05, 'epoch': 1.62}\n",
      "{'loss': 3.0906, 'grad_norm': 12.172325134277344, 'learning_rate': 2.2985747527632345e-05, 'epoch': 1.62}\n",
      "{'loss': 3.109, 'grad_norm': 9.180975914001465, 'learning_rate': 2.2978475858057012e-05, 'epoch': 1.62}\n",
      "{'loss': 3.2258, 'grad_norm': 9.039154052734375, 'learning_rate': 2.2971204188481675e-05, 'epoch': 1.62}\n",
      "{'loss': 3.0932, 'grad_norm': 11.86378002166748, 'learning_rate': 2.2963932518906342e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1266, 'grad_norm': 9.18166446685791, 'learning_rate': 2.295666084933101e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1617, 'grad_norm': 8.910003662109375, 'learning_rate': 2.2949389179755673e-05, 'epoch': 1.62}\n",
      "{'loss': 3.0604, 'grad_norm': 9.68961238861084, 'learning_rate': 2.294211751018034e-05, 'epoch': 1.62}\n",
      "{'loss': 3.0645, 'grad_norm': 9.301383972167969, 'learning_rate': 2.2934845840605003e-05, 'epoch': 1.62}\n",
      "{'loss': 3.124, 'grad_norm': 9.878632545471191, 'learning_rate': 2.292757417102967e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1922, 'grad_norm': 8.980047225952148, 'learning_rate': 2.2920302501454337e-05, 'epoch': 1.62}\n",
      "{'loss': 3.1588, 'grad_norm': 9.245862007141113, 'learning_rate': 2.2913030831879e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1066, 'grad_norm': 9.29326057434082, 'learning_rate': 2.2905759162303667e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0562, 'grad_norm': 10.09351921081543, 'learning_rate': 2.289848749272833e-05, 'epoch': 1.63}\n",
      "{'loss': 3.2146, 'grad_norm': 9.171038627624512, 'learning_rate': 2.2891215823152998e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1086, 'grad_norm': 9.995467185974121, 'learning_rate': 2.2883944153577665e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0227, 'grad_norm': 9.343221664428711, 'learning_rate': 2.2876672484002328e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1465, 'grad_norm': 8.142377853393555, 'learning_rate': 2.2869400814426995e-05, 'epoch': 1.63}\n",
      "{'loss': 3.2568, 'grad_norm': 10.074400901794434, 'learning_rate': 2.286212914485166e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1535, 'grad_norm': 9.873296737670898, 'learning_rate': 2.2854857475276325e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0404, 'grad_norm': 9.053671836853027, 'learning_rate': 2.2847585805700992e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0621, 'grad_norm': 10.806915283203125, 'learning_rate': 2.2840314136125656e-05, 'epoch': 1.63}\n",
      "{'loss': 3.2252, 'grad_norm': 9.545165061950684, 'learning_rate': 2.2833042466550323e-05, 'epoch': 1.63}\n",
      "{'loss': 2.9883, 'grad_norm': 8.697576522827148, 'learning_rate': 2.282577079697499e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0727, 'grad_norm': 9.60496997833252, 'learning_rate': 2.281849912739965e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1164, 'grad_norm': 10.192093849182129, 'learning_rate': 2.2811227457824317e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1051, 'grad_norm': 9.362110137939453, 'learning_rate': 2.2803955788248983e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1496, 'grad_norm': 8.919317245483398, 'learning_rate': 2.2796684118673647e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1375, 'grad_norm': 9.695502281188965, 'learning_rate': 2.2789412449098314e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1178, 'grad_norm': 9.129610061645508, 'learning_rate': 2.2782140779522977e-05, 'epoch': 1.63}\n",
      "{'loss': 3.1586, 'grad_norm': 9.808367729187012, 'learning_rate': 2.2774869109947644e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0273, 'grad_norm': 9.348402976989746, 'learning_rate': 2.276759744037231e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0287, 'grad_norm': 9.664911270141602, 'learning_rate': 2.2760325770796975e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0891, 'grad_norm': 9.346899032592773, 'learning_rate': 2.275305410122164e-05, 'epoch': 1.63}\n",
      "{'loss': 3.0512, 'grad_norm': 9.269128799438477, 'learning_rate': 2.2745782431646305e-05, 'epoch': 1.64}\n",
      "{'loss': 3.2012, 'grad_norm': 9.965807914733887, 'learning_rate': 2.2738510762070972e-05, 'epoch': 1.64}\n",
      "{'loss': 2.9629, 'grad_norm': 8.483804702758789, 'learning_rate': 2.273123909249564e-05, 'epoch': 1.64}\n",
      " 55%|█████████████████▉               | 37500/68760 [6:57:32<5:10:22,  1.68it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.06s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.32s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.795392, 'eval_rouge-2': 7.854333999999999, 'eval_rouge-l': 25.781218, 'eval_bleu-4': 0.039007528198441534, 'eval_runtime': 17.3896, 'eval_samples_per_second': 2.875, 'eval_steps_per_second': 0.23, 'epoch': 1.64}\n",
      " 55%|█████████████████▉               | 37500/68760 [6:57:49<5:10:22,  1.68it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.27s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-37500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0953, 'grad_norm': 10.395708084106445, 'learning_rate': 2.2723967422920302e-05, 'epoch': 1.64}\n",
      "{'loss': 3.1482, 'grad_norm': 9.900782585144043, 'learning_rate': 2.271669575334497e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0645, 'grad_norm': 10.81917953491211, 'learning_rate': 2.2709424083769633e-05, 'epoch': 1.64}\n",
      "{'loss': 3.1684, 'grad_norm': 10.531291007995605, 'learning_rate': 2.27021524141943e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0283, 'grad_norm': 9.169217109680176, 'learning_rate': 2.2694880744618967e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0785, 'grad_norm': 8.635449409484863, 'learning_rate': 2.268760907504363e-05, 'epoch': 1.64}\n",
      "{'loss': 2.9885, 'grad_norm': 9.264302253723145, 'learning_rate': 2.2680337405468297e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0848, 'grad_norm': 8.869937896728516, 'learning_rate': 2.267306573589296e-05, 'epoch': 1.64}\n",
      "{'loss': 3.1258, 'grad_norm': 9.725297927856445, 'learning_rate': 2.2665794066317627e-05, 'epoch': 1.64}\n",
      "{'loss': 3.1523, 'grad_norm': 10.701828956604004, 'learning_rate': 2.2658522396742294e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0852, 'grad_norm': 10.395514488220215, 'learning_rate': 2.2651250727166958e-05, 'epoch': 1.64}\n",
      "{'loss': 3.1807, 'grad_norm': 9.930256843566895, 'learning_rate': 2.2643979057591625e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0271, 'grad_norm': 9.195237159729004, 'learning_rate': 2.263670738801629e-05, 'epoch': 1.64}\n",
      "{'loss': 3.1385, 'grad_norm': 8.427631378173828, 'learning_rate': 2.2629435718440955e-05, 'epoch': 1.64}\n",
      "{'loss': 3.1262, 'grad_norm': 10.107735633850098, 'learning_rate': 2.2622164048865622e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0424, 'grad_norm': 10.117463111877441, 'learning_rate': 2.2614892379290285e-05, 'epoch': 1.64}\n",
      "{'loss': 3.2225, 'grad_norm': 9.749658584594727, 'learning_rate': 2.2607620709714952e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0627, 'grad_norm': 9.625297546386719, 'learning_rate': 2.260034904013962e-05, 'epoch': 1.64}\n",
      "{'loss': 3.0811, 'grad_norm': 9.21883773803711, 'learning_rate': 2.2593077370564283e-05, 'epoch': 1.64}\n",
      "{'loss': 3.2, 'grad_norm': 9.231922149658203, 'learning_rate': 2.258580570098895e-05, 'epoch': 1.64}\n",
      "{'loss': 2.8967, 'grad_norm': 10.975601196289062, 'learning_rate': 2.2578534031413613e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1621, 'grad_norm': 10.018852233886719, 'learning_rate': 2.257126236183828e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1096, 'grad_norm': 10.27661418914795, 'learning_rate': 2.2563990692262947e-05, 'epoch': 1.65}\n",
      "{'loss': 3.2305, 'grad_norm': 9.758028030395508, 'learning_rate': 2.255671902268761e-05, 'epoch': 1.65}\n",
      "{'loss': 3.0387, 'grad_norm': 9.808059692382812, 'learning_rate': 2.2549447353112277e-05, 'epoch': 1.65}\n",
      "{'loss': 3.0506, 'grad_norm': 9.025643348693848, 'learning_rate': 2.254217568353694e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1928, 'grad_norm': 10.291413307189941, 'learning_rate': 2.2534904013961608e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1934, 'grad_norm': 9.224226951599121, 'learning_rate': 2.252763234438627e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1693, 'grad_norm': 9.364877700805664, 'learning_rate': 2.2520360674810935e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1295, 'grad_norm': 9.46862506866455, 'learning_rate': 2.25130890052356e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1729, 'grad_norm': 9.818405151367188, 'learning_rate': 2.250581733566027e-05, 'epoch': 1.65}\n",
      "{'loss': 3.0799, 'grad_norm': 10.641197204589844, 'learning_rate': 2.2498545666084932e-05, 'epoch': 1.65}\n",
      "{'loss': 3.0705, 'grad_norm': 7.98777437210083, 'learning_rate': 2.24912739965096e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1098, 'grad_norm': 8.377840042114258, 'learning_rate': 2.2484002326934266e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1283, 'grad_norm': 9.098102569580078, 'learning_rate': 2.247673065735893e-05, 'epoch': 1.65}\n",
      "{'loss': 3.049, 'grad_norm': 8.848969459533691, 'learning_rate': 2.2469458987783596e-05, 'epoch': 1.65}\n",
      "{'loss': 3.2936, 'grad_norm': 9.84500789642334, 'learning_rate': 2.246218731820826e-05, 'epoch': 1.65}\n",
      "{'loss': 3.0018, 'grad_norm': 9.140351295471191, 'learning_rate': 2.2454915648632927e-05, 'epoch': 1.65}\n",
      "{'loss': 3.2205, 'grad_norm': 7.740488529205322, 'learning_rate': 2.2447643979057594e-05, 'epoch': 1.65}\n",
      "{'loss': 3.0631, 'grad_norm': 10.320696830749512, 'learning_rate': 2.2440372309482257e-05, 'epoch': 1.65}\n",
      "{'loss': 3.1158, 'grad_norm': 8.890186309814453, 'learning_rate': 2.2433100639906924e-05, 'epoch': 1.65}\n",
      "{'loss': 3.15, 'grad_norm': 8.73609447479248, 'learning_rate': 2.2425828970331587e-05, 'epoch': 1.65}\n",
      "{'loss': 3.0598, 'grad_norm': 9.429348945617676, 'learning_rate': 2.2418557300756254e-05, 'epoch': 1.65}\n",
      "{'loss': 3.2402, 'grad_norm': 9.405668258666992, 'learning_rate': 2.241128563118092e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1381, 'grad_norm': 8.664772987365723, 'learning_rate': 2.2404013961605585e-05, 'epoch': 1.66}\n",
      "{'loss': 3.2557, 'grad_norm': 9.189034461975098, 'learning_rate': 2.239674229203025e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1164, 'grad_norm': 9.616243362426758, 'learning_rate': 2.2389470622454915e-05, 'epoch': 1.66}\n",
      "{'loss': 3.0494, 'grad_norm': 8.803045272827148, 'learning_rate': 2.2382198952879582e-05, 'epoch': 1.66}\n",
      "{'loss': 3.183, 'grad_norm': 9.700092315673828, 'learning_rate': 2.237492728330425e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1363, 'grad_norm': 9.375038146972656, 'learning_rate': 2.2367655613728912e-05, 'epoch': 1.66}\n",
      " 55%|██████████████████▏              | 38000/68760 [7:03:03<5:07:20,  1.67it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.20s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.34s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.999988, 'eval_rouge-2': 8.222326, 'eval_rouge-l': 25.642856000000002, 'eval_bleu-4': 0.037711504197961744, 'eval_runtime': 17.895, 'eval_samples_per_second': 2.794, 'eval_steps_per_second': 0.224, 'epoch': 1.66}\n",
      " 55%|██████████████████▏              | 38000/68760 [7:03:21<5:07:20,  1.67it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.42s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-38000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1039, 'grad_norm': 11.390379905700684, 'learning_rate': 2.236038394415358e-05, 'epoch': 1.66}\n",
      "{'loss': 3.0428, 'grad_norm': 9.675904273986816, 'learning_rate': 2.2353112274578243e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1594, 'grad_norm': 10.106571197509766, 'learning_rate': 2.234584060500291e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1541, 'grad_norm': 9.210927963256836, 'learning_rate': 2.2338568935427577e-05, 'epoch': 1.66}\n",
      "{'loss': 3.0463, 'grad_norm': 9.868617057800293, 'learning_rate': 2.233129726585224e-05, 'epoch': 1.66}\n",
      "{'loss': 3.2559, 'grad_norm': 12.702011108398438, 'learning_rate': 2.2324025596276907e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1656, 'grad_norm': 10.024136543273926, 'learning_rate': 2.2316753926701574e-05, 'epoch': 1.66}\n",
      "{'loss': 3.2115, 'grad_norm': 9.075421333312988, 'learning_rate': 2.2309482257126237e-05, 'epoch': 1.66}\n",
      "{'loss': 3.148, 'grad_norm': 10.475043296813965, 'learning_rate': 2.2302210587550904e-05, 'epoch': 1.66}\n",
      "{'loss': 3.2301, 'grad_norm': 10.260608673095703, 'learning_rate': 2.2294938917975568e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1357, 'grad_norm': 8.94416618347168, 'learning_rate': 2.2287667248400235e-05, 'epoch': 1.66}\n",
      "{'loss': 3.2648, 'grad_norm': 9.659470558166504, 'learning_rate': 2.22803955788249e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1885, 'grad_norm': 9.00895881652832, 'learning_rate': 2.2273123909249565e-05, 'epoch': 1.66}\n",
      "{'loss': 3.0605, 'grad_norm': 8.263681411743164, 'learning_rate': 2.2265852239674232e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1986, 'grad_norm': 8.864680290222168, 'learning_rate': 2.2258580570098896e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1594, 'grad_norm': 9.404515266418457, 'learning_rate': 2.2251308900523562e-05, 'epoch': 1.66}\n",
      "{'loss': 3.1621, 'grad_norm': 8.958328247070312, 'learning_rate': 2.224403723094823e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1389, 'grad_norm': 9.352372169494629, 'learning_rate': 2.223676556137289e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1984, 'grad_norm': 9.262981414794922, 'learning_rate': 2.2229493891797556e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1443, 'grad_norm': 10.214797019958496, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1365, 'grad_norm': 8.990520477294922, 'learning_rate': 2.2214950552646887e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1002, 'grad_norm': 11.887053489685059, 'learning_rate': 2.2207678883071554e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1779, 'grad_norm': 8.704087257385254, 'learning_rate': 2.2200407213496217e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1592, 'grad_norm': 9.541129112243652, 'learning_rate': 2.2193135543920884e-05, 'epoch': 1.67}\n",
      "{'loss': 3.0502, 'grad_norm': 9.309016227722168, 'learning_rate': 2.218586387434555e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1074, 'grad_norm': 9.555181503295898, 'learning_rate': 2.2178592204770214e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1135, 'grad_norm': 9.717286109924316, 'learning_rate': 2.217132053519488e-05, 'epoch': 1.67}\n",
      "{'loss': 3.0418, 'grad_norm': 8.948355674743652, 'learning_rate': 2.2164048865619548e-05, 'epoch': 1.67}\n",
      "{'loss': 3.0695, 'grad_norm': 9.141570091247559, 'learning_rate': 2.2156777196044212e-05, 'epoch': 1.67}\n",
      "{'loss': 3.05, 'grad_norm': 9.407670974731445, 'learning_rate': 2.214950552646888e-05, 'epoch': 1.67}\n",
      "{'loss': 3.0609, 'grad_norm': 10.246253967285156, 'learning_rate': 2.2142233856893542e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1535, 'grad_norm': 10.081058502197266, 'learning_rate': 2.213496218731821e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1178, 'grad_norm': 9.682405471801758, 'learning_rate': 2.2127690517742876e-05, 'epoch': 1.67}\n",
      "{'loss': 3.1262, 'grad_norm': 9.433837890625, 'learning_rate': 2.212041884816754e-05, 'epoch': 1.67}\n",
      "{'loss': 3.0863, 'grad_norm': 9.475971221923828, 'learning_rate': 2.2113147178592206e-05, 'epoch': 1.67}\n",
      "{'loss': 3.0283, 'grad_norm': 9.023397445678711, 'learning_rate': 2.210587550901687e-05, 'epoch': 1.67}\n",
      "{'loss': 3.2141, 'grad_norm': 9.612951278686523, 'learning_rate': 2.2098603839441537e-05, 'epoch': 1.67}\n",
      "{'loss': 3.2766, 'grad_norm': 9.534610748291016, 'learning_rate': 2.2091332169866204e-05, 'epoch': 1.67}\n",
      "{'loss': 3.0428, 'grad_norm': 10.07827377319336, 'learning_rate': 2.2084060500290867e-05, 'epoch': 1.67}\n",
      "{'loss': 3.2045, 'grad_norm': 8.675987243652344, 'learning_rate': 2.2076788830715534e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1682, 'grad_norm': 9.819119453430176, 'learning_rate': 2.2069517161140198e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1803, 'grad_norm': 9.767790794372559, 'learning_rate': 2.2062245491564864e-05, 'epoch': 1.68}\n",
      "{'loss': 2.9586, 'grad_norm': 8.983739852905273, 'learning_rate': 2.205497382198953e-05, 'epoch': 1.68}\n",
      "{'loss': 3.207, 'grad_norm': 8.341596603393555, 'learning_rate': 2.2047702152414195e-05, 'epoch': 1.68}\n",
      "{'loss': 3.182, 'grad_norm': 9.627533912658691, 'learning_rate': 2.2040430482838862e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1199, 'grad_norm': 9.869365692138672, 'learning_rate': 2.2033158813263525e-05, 'epoch': 1.68}\n",
      "{'loss': 3.0336, 'grad_norm': 9.705475807189941, 'learning_rate': 2.2025887143688192e-05, 'epoch': 1.68}\n",
      "{'loss': 3.0643, 'grad_norm': 9.335670471191406, 'learning_rate': 2.201861547411286e-05, 'epoch': 1.68}\n",
      "{'loss': 3.193, 'grad_norm': 10.561735153198242, 'learning_rate': 2.2011343804537523e-05, 'epoch': 1.68}\n",
      "{'loss': 3.0926, 'grad_norm': 8.763416290283203, 'learning_rate': 2.200407213496219e-05, 'epoch': 1.68}\n",
      " 56%|██████████████████▍              | 38500/68760 [7:08:30<5:23:16,  1.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.21s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.43s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.37744599999999, 'eval_rouge-2': 8.309276, 'eval_rouge-l': 25.798412000000003, 'eval_bleu-4': 0.038520615411294694, 'eval_runtime': 12.7663, 'eval_samples_per_second': 3.917, 'eval_steps_per_second': 0.313, 'epoch': 1.68}\n",
      " 56%|██████████████████▍              | 38500/68760 [7:08:43<5:23:16,  1.56it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.29s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-38500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0902, 'grad_norm': 9.035747528076172, 'learning_rate': 2.1996800465386856e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1377, 'grad_norm': 9.124856948852539, 'learning_rate': 2.198952879581152e-05, 'epoch': 1.68}\n",
      "{'loss': 3.0984, 'grad_norm': 8.999460220336914, 'learning_rate': 2.1982257126236187e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1014, 'grad_norm': 9.05097770690918, 'learning_rate': 2.197498545666085e-05, 'epoch': 1.68}\n",
      "{'loss': 3.0676, 'grad_norm': 9.098333358764648, 'learning_rate': 2.1967713787085517e-05, 'epoch': 1.68}\n",
      "{'loss': 3.0781, 'grad_norm': 9.24567699432373, 'learning_rate': 2.1960442117510184e-05, 'epoch': 1.68}\n",
      "{'loss': 3.0686, 'grad_norm': 9.989997863769531, 'learning_rate': 2.1953170447934848e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1055, 'grad_norm': 9.251813888549805, 'learning_rate': 2.194589877835951e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1408, 'grad_norm': 13.521039962768555, 'learning_rate': 2.1938627108784178e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1975, 'grad_norm': 9.361841201782227, 'learning_rate': 2.193135543920884e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1869, 'grad_norm': 13.923150062561035, 'learning_rate': 2.192408376963351e-05, 'epoch': 1.68}\n",
      "{'loss': 3.1162, 'grad_norm': 10.228549003601074, 'learning_rate': 2.1916812100058172e-05, 'epoch': 1.68}\n",
      "{'loss': 2.999, 'grad_norm': 9.400569915771484, 'learning_rate': 2.190954043048284e-05, 'epoch': 1.69}\n",
      "{'loss': 3.0406, 'grad_norm': 11.051566123962402, 'learning_rate': 2.1902268760907506e-05, 'epoch': 1.69}\n",
      "{'loss': 3.0328, 'grad_norm': 9.372419357299805, 'learning_rate': 2.189499709133217e-05, 'epoch': 1.69}\n",
      "{'loss': 3.2441, 'grad_norm': 9.014185905456543, 'learning_rate': 2.1887725421756836e-05, 'epoch': 1.69}\n",
      "{'loss': 3.0717, 'grad_norm': 9.140559196472168, 'learning_rate': 2.18804537521815e-05, 'epoch': 1.69}\n",
      "{'loss': 3.2459, 'grad_norm': 10.007070541381836, 'learning_rate': 2.1873182082606167e-05, 'epoch': 1.69}\n",
      "{'loss': 3.1137, 'grad_norm': 9.637353897094727, 'learning_rate': 2.1865910413030833e-05, 'epoch': 1.69}\n",
      "{'loss': 3.1924, 'grad_norm': 10.183431625366211, 'learning_rate': 2.1858638743455497e-05, 'epoch': 1.69}\n",
      "{'loss': 3.1559, 'grad_norm': 13.692963600158691, 'learning_rate': 2.1851367073880164e-05, 'epoch': 1.69}\n",
      "{'loss': 3.1824, 'grad_norm': 9.602818489074707, 'learning_rate': 2.184409540430483e-05, 'epoch': 1.69}\n",
      "{'loss': 3.2463, 'grad_norm': 9.995465278625488, 'learning_rate': 2.1836823734729494e-05, 'epoch': 1.69}\n",
      "{'loss': 3.1799, 'grad_norm': 8.797563552856445, 'learning_rate': 2.182955206515416e-05, 'epoch': 1.69}\n",
      "{'loss': 3.2211, 'grad_norm': 9.23794174194336, 'learning_rate': 2.1822280395578825e-05, 'epoch': 1.69}\n",
      "{'loss': 3.0895, 'grad_norm': 9.537586212158203, 'learning_rate': 2.181500872600349e-05, 'epoch': 1.69}\n",
      "{'loss': 3.1562, 'grad_norm': 8.706839561462402, 'learning_rate': 2.180773705642816e-05, 'epoch': 1.69}\n",
      "{'loss': 3.0506, 'grad_norm': 9.691271781921387, 'learning_rate': 2.1800465386852822e-05, 'epoch': 1.69}\n",
      "{'loss': 3.2252, 'grad_norm': 8.944242477416992, 'learning_rate': 2.179319371727749e-05, 'epoch': 1.69}\n",
      "{'loss': 3.0447, 'grad_norm': 10.21063232421875, 'learning_rate': 2.1785922047702152e-05, 'epoch': 1.69}\n",
      "{'loss': 3.2107, 'grad_norm': 9.59589958190918, 'learning_rate': 2.177865037812682e-05, 'epoch': 1.69}\n",
      "{'loss': 3.1865, 'grad_norm': 9.22570514678955, 'learning_rate': 2.1771378708551486e-05, 'epoch': 1.69}\n",
      "{'loss': 3.1471, 'grad_norm': 8.793319702148438, 'learning_rate': 2.176410703897615e-05, 'epoch': 1.69}\n",
      "{'loss': 3.058, 'grad_norm': 9.223482131958008, 'learning_rate': 2.1756835369400816e-05, 'epoch': 1.69}\n",
      "{'loss': 3.0922, 'grad_norm': 9.65945053100586, 'learning_rate': 2.174956369982548e-05, 'epoch': 1.7}\n",
      "{'loss': 2.9811, 'grad_norm': 9.949991226196289, 'learning_rate': 2.1742292030250147e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1824, 'grad_norm': 9.04914665222168, 'learning_rate': 2.1735020360674814e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1166, 'grad_norm': 9.84089183807373, 'learning_rate': 2.1727748691099477e-05, 'epoch': 1.7}\n",
      "{'loss': 3.073, 'grad_norm': 9.148650169372559, 'learning_rate': 2.1720477021524144e-05, 'epoch': 1.7}\n",
      "{'loss': 3.0889, 'grad_norm': 9.016502380371094, 'learning_rate': 2.1713205351948808e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1078, 'grad_norm': 9.833951950073242, 'learning_rate': 2.1705933682373475e-05, 'epoch': 1.7}\n",
      "{'loss': 3.3506, 'grad_norm': 9.185853958129883, 'learning_rate': 2.169866201279814e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1273, 'grad_norm': 10.978705406188965, 'learning_rate': 2.1691390343222805e-05, 'epoch': 1.7}\n",
      "{'loss': 3.0697, 'grad_norm': 9.695706367492676, 'learning_rate': 2.1684118673647472e-05, 'epoch': 1.7}\n",
      "{'loss': 3.0312, 'grad_norm': 10.92447280883789, 'learning_rate': 2.167684700407214e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1398, 'grad_norm': 10.71607494354248, 'learning_rate': 2.1669575334496802e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1396, 'grad_norm': 9.320541381835938, 'learning_rate': 2.166230366492147e-05, 'epoch': 1.7}\n",
      "{'loss': 2.9941, 'grad_norm': 9.822100639343262, 'learning_rate': 2.1655031995346133e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1551, 'grad_norm': 8.681889533996582, 'learning_rate': 2.1647760325770796e-05, 'epoch': 1.7}\n",
      "{'loss': 3.0748, 'grad_norm': 8.676969528198242, 'learning_rate': 2.1640488656195463e-05, 'epoch': 1.7}\n",
      " 57%|██████████████████▋              | 39000/68760 [7:13:53<5:20:49,  1.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.35s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.55s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.15679, 'eval_rouge-2': 7.837626000000001, 'eval_rouge-l': 25.482543999999997, 'eval_bleu-4': 0.03714837422402354, 'eval_runtime': 18.0088, 'eval_samples_per_second': 2.776, 'eval_steps_per_second': 0.222, 'epoch': 1.7}\n",
      " 57%|██████████████████▋              | 39000/68760 [7:14:11<5:20:49,  1.55it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.38s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-39000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0664, 'grad_norm': 9.934395790100098, 'learning_rate': 2.1633216986620127e-05, 'epoch': 1.7}\n",
      "{'loss': 3.09, 'grad_norm': 8.73867130279541, 'learning_rate': 2.1625945317044794e-05, 'epoch': 1.7}\n",
      "{'loss': 3.0695, 'grad_norm': 8.870634078979492, 'learning_rate': 2.161867364746946e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1912, 'grad_norm': 10.044929504394531, 'learning_rate': 2.1611401977894124e-05, 'epoch': 1.7}\n",
      "{'loss': 3.1664, 'grad_norm': 9.227408409118652, 'learning_rate': 2.160413030831879e-05, 'epoch': 1.7}\n",
      "{'loss': 2.9605, 'grad_norm': 9.294292449951172, 'learning_rate': 2.1596858638743454e-05, 'epoch': 1.7}\n",
      "{'loss': 3.0254, 'grad_norm': 10.218722343444824, 'learning_rate': 2.158958696916812e-05, 'epoch': 1.7}\n",
      "{'loss': 2.9934, 'grad_norm': 10.094918251037598, 'learning_rate': 2.1582315299592788e-05, 'epoch': 1.71}\n",
      "{'loss': 3.2086, 'grad_norm': 9.537487983703613, 'learning_rate': 2.157504363001745e-05, 'epoch': 1.71}\n",
      "{'loss': 3.227, 'grad_norm': 9.530961036682129, 'learning_rate': 2.156777196044212e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1143, 'grad_norm': 9.661274909973145, 'learning_rate': 2.1560500290866782e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1141, 'grad_norm': 9.05114459991455, 'learning_rate': 2.155322862129145e-05, 'epoch': 1.71}\n",
      "{'loss': 3.0834, 'grad_norm': 8.778443336486816, 'learning_rate': 2.1545956951716116e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1514, 'grad_norm': 9.126376152038574, 'learning_rate': 2.153868528214078e-05, 'epoch': 1.71}\n",
      "{'loss': 3.0621, 'grad_norm': 9.825807571411133, 'learning_rate': 2.1531413612565446e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1543, 'grad_norm': 10.664752960205078, 'learning_rate': 2.1524141942990113e-05, 'epoch': 1.71}\n",
      "{'loss': 3.2084, 'grad_norm': 9.760137557983398, 'learning_rate': 2.1516870273414777e-05, 'epoch': 1.71}\n",
      "{'loss': 3.0693, 'grad_norm': 9.578252792358398, 'learning_rate': 2.1509598603839444e-05, 'epoch': 1.71}\n",
      "{'loss': 3.2008, 'grad_norm': 11.743779182434082, 'learning_rate': 2.1502326934264107e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1648, 'grad_norm': 10.284941673278809, 'learning_rate': 2.1495055264688774e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1904, 'grad_norm': 9.403253555297852, 'learning_rate': 2.148778359511344e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1613, 'grad_norm': 8.76181697845459, 'learning_rate': 2.1480511925538104e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1395, 'grad_norm': 8.58656120300293, 'learning_rate': 2.147324025596277e-05, 'epoch': 1.71}\n",
      "{'loss': 3.0576, 'grad_norm': 9.327688217163086, 'learning_rate': 2.1465968586387435e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1184, 'grad_norm': 9.180571556091309, 'learning_rate': 2.14586969168121e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1057, 'grad_norm': 7.734079837799072, 'learning_rate': 2.145142524723677e-05, 'epoch': 1.71}\n",
      "{'loss': 3.1148, 'grad_norm': 10.439836502075195, 'learning_rate': 2.1444153577661432e-05, 'epoch': 1.71}\n",
      "{'loss': 3.3518, 'grad_norm': 11.222867965698242, 'learning_rate': 2.14368819080861e-05, 'epoch': 1.71}\n",
      "{'loss': 3.0521, 'grad_norm': 9.086915969848633, 'learning_rate': 2.1429610238510762e-05, 'epoch': 1.71}\n",
      "{'loss': 3.0033, 'grad_norm': 9.698315620422363, 'learning_rate': 2.142233856893543e-05, 'epoch': 1.71}\n",
      "{'loss': 3.0184, 'grad_norm': 12.085966110229492, 'learning_rate': 2.1415066899360096e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1492, 'grad_norm': 10.044665336608887, 'learning_rate': 2.140779522978476e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1143, 'grad_norm': 9.698704719543457, 'learning_rate': 2.1400523560209427e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1541, 'grad_norm': 10.549415588378906, 'learning_rate': 2.139325189063409e-05, 'epoch': 1.72}\n",
      "{'loss': 3.041, 'grad_norm': 9.870048522949219, 'learning_rate': 2.1385980221058757e-05, 'epoch': 1.72}\n",
      "{'loss': 3.0836, 'grad_norm': 8.88655948638916, 'learning_rate': 2.1378708551483424e-05, 'epoch': 1.72}\n",
      "{'loss': 3.2137, 'grad_norm': 9.093149185180664, 'learning_rate': 2.1371436881908087e-05, 'epoch': 1.72}\n",
      "{'loss': 3.2002, 'grad_norm': 11.08455753326416, 'learning_rate': 2.136416521233275e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1678, 'grad_norm': 9.135090827941895, 'learning_rate': 2.1356893542757418e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1281, 'grad_norm': 10.994144439697266, 'learning_rate': 2.134962187318208e-05, 'epoch': 1.72}\n",
      "{'loss': 3.0277, 'grad_norm': 10.532455444335938, 'learning_rate': 2.1342350203606748e-05, 'epoch': 1.72}\n",
      "{'loss': 3.0396, 'grad_norm': 9.366883277893066, 'learning_rate': 2.1335078534031415e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1688, 'grad_norm': 9.55806827545166, 'learning_rate': 2.132780686445608e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1568, 'grad_norm': 9.659257888793945, 'learning_rate': 2.1320535194880746e-05, 'epoch': 1.72}\n",
      "{'loss': 3.149, 'grad_norm': 9.377826690673828, 'learning_rate': 2.131326352530541e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1219, 'grad_norm': 10.222187995910645, 'learning_rate': 2.1305991855730076e-05, 'epoch': 1.72}\n",
      "{'loss': 3.0414, 'grad_norm': 9.864068031311035, 'learning_rate': 2.1298720186154743e-05, 'epoch': 1.72}\n",
      "{'loss': 3.2549, 'grad_norm': 9.996556282043457, 'learning_rate': 2.1291448516579406e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1254, 'grad_norm': 10.245513916015625, 'learning_rate': 2.1284176847004073e-05, 'epoch': 1.72}\n",
      "{'loss': 3.1809, 'grad_norm': 9.530587196350098, 'learning_rate': 2.1276905177428737e-05, 'epoch': 1.72}\n",
      " 57%|██████████████████▉              | 39500/68760 [7:19:18<5:28:30,  1.48it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.32s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.53s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.157404, 'eval_rouge-2': 7.974729999999999, 'eval_rouge-l': 25.918608, 'eval_bleu-4': 0.0420873929404318, 'eval_runtime': 18.5152, 'eval_samples_per_second': 2.7, 'eval_steps_per_second': 0.216, 'epoch': 1.72}\n",
      " 57%|██████████████████▉              | 39500/68760 [7:19:36<5:28:30,  1.48it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.58s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-39500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1369, 'grad_norm': 9.090806007385254, 'learning_rate': 2.1269633507853404e-05, 'epoch': 1.72}\n",
      "{'loss': 3.0676, 'grad_norm': 9.06112003326416, 'learning_rate': 2.126236183827807e-05, 'epoch': 1.72}\n",
      "{'loss': 3.0678, 'grad_norm': 8.735142707824707, 'learning_rate': 2.1255090168702734e-05, 'epoch': 1.72}\n",
      "{'loss': 3.0355, 'grad_norm': 9.980145454406738, 'learning_rate': 2.12478184991274e-05, 'epoch': 1.73}\n",
      "{'loss': 2.9303, 'grad_norm': 8.84231185913086, 'learning_rate': 2.1240546829552064e-05, 'epoch': 1.73}\n",
      "{'loss': 3.0598, 'grad_norm': 10.401355743408203, 'learning_rate': 2.123327515997673e-05, 'epoch': 1.73}\n",
      "{'loss': 3.0908, 'grad_norm': 10.172895431518555, 'learning_rate': 2.1226003490401398e-05, 'epoch': 1.73}\n",
      "{'loss': 3.2938, 'grad_norm': 9.189547538757324, 'learning_rate': 2.1218731820826062e-05, 'epoch': 1.73}\n",
      "{'loss': 3.2047, 'grad_norm': 9.5802583694458, 'learning_rate': 2.121146015125073e-05, 'epoch': 1.73}\n",
      "{'loss': 3.0205, 'grad_norm': 9.144356727600098, 'learning_rate': 2.1204188481675396e-05, 'epoch': 1.73}\n",
      "{'loss': 2.9924, 'grad_norm': 9.123492240905762, 'learning_rate': 2.119691681210006e-05, 'epoch': 1.73}\n",
      "{'loss': 3.1088, 'grad_norm': 9.320798873901367, 'learning_rate': 2.1189645142524726e-05, 'epoch': 1.73}\n",
      "{'loss': 3.149, 'grad_norm': 9.333391189575195, 'learning_rate': 2.118237347294939e-05, 'epoch': 1.73}\n",
      "{'loss': 3.0598, 'grad_norm': 8.887630462646484, 'learning_rate': 2.1175101803374056e-05, 'epoch': 1.73}\n",
      "{'loss': 3.232, 'grad_norm': 9.60855770111084, 'learning_rate': 2.1167830133798723e-05, 'epoch': 1.73}\n",
      "{'loss': 3.1705, 'grad_norm': 9.928987503051758, 'learning_rate': 2.1160558464223387e-05, 'epoch': 1.73}\n",
      "{'loss': 3.073, 'grad_norm': 10.89271068572998, 'learning_rate': 2.1153286794648054e-05, 'epoch': 1.73}\n",
      "{'loss': 3.1236, 'grad_norm': 9.475570678710938, 'learning_rate': 2.1146015125072717e-05, 'epoch': 1.73}\n",
      "{'loss': 2.9676, 'grad_norm': 8.868247032165527, 'learning_rate': 2.1138743455497384e-05, 'epoch': 1.73}\n",
      "{'loss': 3.2322, 'grad_norm': 9.047060012817383, 'learning_rate': 2.113147178592205e-05, 'epoch': 1.73}\n",
      "{'loss': 3.2088, 'grad_norm': 9.236403465270996, 'learning_rate': 2.1124200116346714e-05, 'epoch': 1.73}\n",
      "{'loss': 3.207, 'grad_norm': 9.326217651367188, 'learning_rate': 2.111692844677138e-05, 'epoch': 1.73}\n",
      "{'loss': 3.1646, 'grad_norm': 9.640345573425293, 'learning_rate': 2.1109656777196045e-05, 'epoch': 1.73}\n",
      "{'loss': 3.1051, 'grad_norm': 8.540560722351074, 'learning_rate': 2.1102385107620712e-05, 'epoch': 1.73}\n",
      "{'loss': 3.1133, 'grad_norm': 9.02428913116455, 'learning_rate': 2.109511343804538e-05, 'epoch': 1.73}\n",
      "{'loss': 3.0018, 'grad_norm': 9.464573860168457, 'learning_rate': 2.1087841768470042e-05, 'epoch': 1.73}\n",
      "{'loss': 3.0465, 'grad_norm': 9.079392433166504, 'learning_rate': 2.108057009889471e-05, 'epoch': 1.74}\n",
      "{'loss': 3.0576, 'grad_norm': 10.214151382446289, 'learning_rate': 2.1073298429319373e-05, 'epoch': 1.74}\n",
      "{'loss': 3.102, 'grad_norm': 9.356066703796387, 'learning_rate': 2.1066026759744036e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1039, 'grad_norm': 8.712373733520508, 'learning_rate': 2.1058755090168703e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1658, 'grad_norm': 9.073399543762207, 'learning_rate': 2.1051483420593366e-05, 'epoch': 1.74}\n",
      "{'loss': 2.9266, 'grad_norm': 9.468141555786133, 'learning_rate': 2.1044211751018033e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1736, 'grad_norm': 9.086193084716797, 'learning_rate': 2.10369400814427e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1146, 'grad_norm': 10.36837387084961, 'learning_rate': 2.1029668411867364e-05, 'epoch': 1.74}\n",
      "{'loss': 3.2084, 'grad_norm': 27.030710220336914, 'learning_rate': 2.102239674229203e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1836, 'grad_norm': 10.135675430297852, 'learning_rate': 2.1015125072716698e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1117, 'grad_norm': 9.451906204223633, 'learning_rate': 2.100785340314136e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1074, 'grad_norm': 9.551521301269531, 'learning_rate': 2.1000581733566028e-05, 'epoch': 1.74}\n",
      "{'loss': 3.0537, 'grad_norm': 10.376226425170898, 'learning_rate': 2.099331006399069e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1605, 'grad_norm': 9.003400802612305, 'learning_rate': 2.098603839441536e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1383, 'grad_norm': 8.642434120178223, 'learning_rate': 2.0978766724840025e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1027, 'grad_norm': 9.747480392456055, 'learning_rate': 2.097149505526469e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1736, 'grad_norm': 10.068781852722168, 'learning_rate': 2.0964223385689356e-05, 'epoch': 1.74}\n",
      "{'loss': 3.0965, 'grad_norm': 9.042733192443848, 'learning_rate': 2.095695171611402e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1111, 'grad_norm': 10.022865295410156, 'learning_rate': 2.0949680046538686e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1016, 'grad_norm': 8.52517032623291, 'learning_rate': 2.0942408376963353e-05, 'epoch': 1.74}\n",
      "{'loss': 3.2885, 'grad_norm': 9.182626724243164, 'learning_rate': 2.0935136707388016e-05, 'epoch': 1.74}\n",
      "{'loss': 3.1148, 'grad_norm': 9.323806762695312, 'learning_rate': 2.0927865037812683e-05, 'epoch': 1.74}\n",
      "{'loss': 3.0041, 'grad_norm': 9.03666877746582, 'learning_rate': 2.0920593368237347e-05, 'epoch': 1.74}\n",
      "{'loss': 3.2373, 'grad_norm': 10.621960639953613, 'learning_rate': 2.0913321698662014e-05, 'epoch': 1.75}\n",
      " 58%|███████████████████▏             | 40000/68760 [7:24:55<4:45:45,  1.68it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.05it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.27s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.190934, 'eval_rouge-2': 7.783853999999999, 'eval_rouge-l': 26.002066, 'eval_bleu-4': 0.0391276810555937, 'eval_runtime': 17.6095, 'eval_samples_per_second': 2.839, 'eval_steps_per_second': 0.227, 'epoch': 1.75}\n",
      " 58%|███████████████████▏             | 40000/68760 [7:25:12<4:45:45,  1.68it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.40s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-40000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0434, 'grad_norm': 10.392295837402344, 'learning_rate': 2.090605002908668e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1957, 'grad_norm': 9.10914421081543, 'learning_rate': 2.0898778359511344e-05, 'epoch': 1.75}\n",
      "{'loss': 3.2221, 'grad_norm': 9.12226390838623, 'learning_rate': 2.089150668993601e-05, 'epoch': 1.75}\n",
      "{'loss': 3.0344, 'grad_norm': 9.662895202636719, 'learning_rate': 2.0884235020360678e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1506, 'grad_norm': 9.04499626159668, 'learning_rate': 2.087696335078534e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1045, 'grad_norm': 9.478034973144531, 'learning_rate': 2.086969168121001e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1, 'grad_norm': 10.24062442779541, 'learning_rate': 2.0862420011634672e-05, 'epoch': 1.75}\n",
      "{'loss': 3.0359, 'grad_norm': 11.147558212280273, 'learning_rate': 2.085514834205934e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1117, 'grad_norm': 9.62916374206543, 'learning_rate': 2.0847876672484006e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1631, 'grad_norm': 10.03027629852295, 'learning_rate': 2.084060500290867e-05, 'epoch': 1.75}\n",
      "{'loss': 3.0521, 'grad_norm': 9.936416625976562, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.75}\n",
      "{'loss': 3.2426, 'grad_norm': 10.37820816040039, 'learning_rate': 2.0826061663758e-05, 'epoch': 1.75}\n",
      "{'loss': 3.174, 'grad_norm': 9.31940746307373, 'learning_rate': 2.0818789994182666e-05, 'epoch': 1.75}\n",
      "{'loss': 3.0555, 'grad_norm': 10.746465682983398, 'learning_rate': 2.0811518324607333e-05, 'epoch': 1.75}\n",
      "{'loss': 3.0975, 'grad_norm': 10.330571174621582, 'learning_rate': 2.0804246655031997e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1889, 'grad_norm': 10.15992546081543, 'learning_rate': 2.0796974985456664e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1416, 'grad_norm': 9.537405014038086, 'learning_rate': 2.0789703315881327e-05, 'epoch': 1.75}\n",
      "{'loss': 3.2049, 'grad_norm': 9.701423645019531, 'learning_rate': 2.078243164630599e-05, 'epoch': 1.75}\n",
      "{'loss': 3.2908, 'grad_norm': 10.644667625427246, 'learning_rate': 2.0775159976730658e-05, 'epoch': 1.75}\n",
      "{'loss': 3.2258, 'grad_norm': 9.05728816986084, 'learning_rate': 2.076788830715532e-05, 'epoch': 1.75}\n",
      "{'loss': 3.1596, 'grad_norm': 10.214908599853516, 'learning_rate': 2.0760616637579988e-05, 'epoch': 1.75}\n",
      "{'loss': 3.027, 'grad_norm': 9.89712142944336, 'learning_rate': 2.0753344968004655e-05, 'epoch': 1.75}\n",
      "{'loss': 2.9986, 'grad_norm': 10.081697463989258, 'learning_rate': 2.074607329842932e-05, 'epoch': 1.76}\n",
      "{'loss': 3.2027, 'grad_norm': 8.9854097366333, 'learning_rate': 2.0738801628853985e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1469, 'grad_norm': 10.276473999023438, 'learning_rate': 2.073152995927865e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1328, 'grad_norm': 10.909130096435547, 'learning_rate': 2.0724258289703316e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1625, 'grad_norm': 10.65774154663086, 'learning_rate': 2.0716986620127983e-05, 'epoch': 1.76}\n",
      "{'loss': 3.0008, 'grad_norm': 9.60665512084961, 'learning_rate': 2.0709714950552646e-05, 'epoch': 1.76}\n",
      "{'loss': 2.9896, 'grad_norm': 9.214654922485352, 'learning_rate': 2.0702443280977313e-05, 'epoch': 1.76}\n",
      "{'loss': 3.0988, 'grad_norm': 9.327754974365234, 'learning_rate': 2.069517161140198e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1277, 'grad_norm': 9.372827529907227, 'learning_rate': 2.0687899941826643e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1525, 'grad_norm': 11.131209373474121, 'learning_rate': 2.068062827225131e-05, 'epoch': 1.76}\n",
      "{'loss': 3.2041, 'grad_norm': 9.280362129211426, 'learning_rate': 2.0673356602675974e-05, 'epoch': 1.76}\n",
      "{'loss': 3.0219, 'grad_norm': 9.841398239135742, 'learning_rate': 2.066608493310064e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1697, 'grad_norm': 12.005045890808105, 'learning_rate': 2.0658813263525308e-05, 'epoch': 1.76}\n",
      "{'loss': 3.0625, 'grad_norm': 8.704351425170898, 'learning_rate': 2.065154159394997e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1053, 'grad_norm': 9.727081298828125, 'learning_rate': 2.0644269924374638e-05, 'epoch': 1.76}\n",
      "{'loss': 3.068, 'grad_norm': 9.930238723754883, 'learning_rate': 2.06369982547993e-05, 'epoch': 1.76}\n",
      "{'loss': 3.19, 'grad_norm': 9.238663673400879, 'learning_rate': 2.062972658522397e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1096, 'grad_norm': 8.495978355407715, 'learning_rate': 2.0622454915648635e-05, 'epoch': 1.76}\n",
      "{'loss': 3.0971, 'grad_norm': 9.23145580291748, 'learning_rate': 2.06151832460733e-05, 'epoch': 1.76}\n",
      "{'loss': 3.0688, 'grad_norm': 10.622971534729004, 'learning_rate': 2.0607911576497966e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1191, 'grad_norm': 10.892746925354004, 'learning_rate': 2.060063990692263e-05, 'epoch': 1.76}\n",
      "{'loss': 3.0826, 'grad_norm': 10.385022163391113, 'learning_rate': 2.0593368237347296e-05, 'epoch': 1.76}\n",
      "{'loss': 3.0953, 'grad_norm': 9.358506202697754, 'learning_rate': 2.0586096567771963e-05, 'epoch': 1.76}\n",
      "{'loss': 3.1855, 'grad_norm': 9.933533668518066, 'learning_rate': 2.0578824898196627e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1055, 'grad_norm': 9.926918983459473, 'learning_rate': 2.0571553228621293e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1404, 'grad_norm': 8.54704475402832, 'learning_rate': 2.056428155904596e-05, 'epoch': 1.77}\n",
      "{'loss': 3.0615, 'grad_norm': 10.061206817626953, 'learning_rate': 2.0557009889470624e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1586, 'grad_norm': 9.612005233764648, 'learning_rate': 2.054973821989529e-05, 'epoch': 1.77}\n",
      " 59%|███████████████████▍             | 40500/68760 [7:30:21<5:13:59,  1.50it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.22s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.37s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.398920000000004, 'eval_rouge-2': 7.250814, 'eval_rouge-l': 25.643022000000002, 'eval_bleu-4': 0.035947481577107865, 'eval_runtime': 17.3308, 'eval_samples_per_second': 2.885, 'eval_steps_per_second': 0.231, 'epoch': 1.77}\n",
      " 59%|███████████████████▍             | 40500/68760 [7:30:38<5:13:59,  1.50it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.20s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-40500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1688, 'grad_norm': 9.705792427062988, 'learning_rate': 2.0542466550319954e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1246, 'grad_norm': 8.959990501403809, 'learning_rate': 2.053519488074462e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1617, 'grad_norm': 9.48527717590332, 'learning_rate': 2.0527923211169288e-05, 'epoch': 1.77}\n",
      "{'loss': 3.2061, 'grad_norm': 9.329184532165527, 'learning_rate': 2.052065154159395e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1283, 'grad_norm': 9.257978439331055, 'learning_rate': 2.051337987201862e-05, 'epoch': 1.77}\n",
      "{'loss': 3.2041, 'grad_norm': 9.131994247436523, 'learning_rate': 2.0506108202443282e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1434, 'grad_norm': 8.965558052062988, 'learning_rate': 2.049883653286795e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1252, 'grad_norm': 9.611316680908203, 'learning_rate': 2.0491564863292612e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1656, 'grad_norm': 10.448545455932617, 'learning_rate': 2.0484293193717276e-05, 'epoch': 1.77}\n",
      "{'loss': 3.0875, 'grad_norm': 9.154342651367188, 'learning_rate': 2.0477021524141943e-05, 'epoch': 1.77}\n",
      "{'loss': 3.2322, 'grad_norm': 10.279590606689453, 'learning_rate': 2.046974985456661e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1539, 'grad_norm': 9.536602020263672, 'learning_rate': 2.0462478184991273e-05, 'epoch': 1.77}\n",
      "{'loss': 3.0881, 'grad_norm': 9.450812339782715, 'learning_rate': 2.045520651541594e-05, 'epoch': 1.77}\n",
      "{'loss': 3.1863, 'grad_norm': 8.404386520385742, 'learning_rate': 2.0447934845840604e-05, 'epoch': 1.77}\n",
      "{'loss': 3.0588, 'grad_norm': 9.093823432922363, 'learning_rate': 2.044066317626527e-05, 'epoch': 1.77}\n",
      "{'loss': 3.0412, 'grad_norm': 11.001460075378418, 'learning_rate': 2.0433391506689937e-05, 'epoch': 1.77}\n",
      "{'loss': 3.3002, 'grad_norm': 9.428558349609375, 'learning_rate': 2.04261198371146e-05, 'epoch': 1.77}\n",
      "{'loss': 3.0906, 'grad_norm': 11.072147369384766, 'learning_rate': 2.0418848167539268e-05, 'epoch': 1.77}\n",
      "{'loss': 3.0914, 'grad_norm': 10.669204711914062, 'learning_rate': 2.041157649796393e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1068, 'grad_norm': 8.48268985748291, 'learning_rate': 2.0404304828388598e-05, 'epoch': 1.78}\n",
      "{'loss': 3.0434, 'grad_norm': 9.418400764465332, 'learning_rate': 2.0397033158813265e-05, 'epoch': 1.78}\n",
      "{'loss': 3.0348, 'grad_norm': 8.918262481689453, 'learning_rate': 2.038976148923793e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1678, 'grad_norm': 9.374139785766602, 'learning_rate': 2.0382489819662595e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1441, 'grad_norm': 9.04793643951416, 'learning_rate': 2.0375218150087262e-05, 'epoch': 1.78}\n",
      "{'loss': 3.0389, 'grad_norm': 11.157747268676758, 'learning_rate': 2.0367946480511926e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1941, 'grad_norm': 10.04967212677002, 'learning_rate': 2.0360674810936593e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1332, 'grad_norm': 9.086236000061035, 'learning_rate': 2.0353403141361256e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1082, 'grad_norm': 8.955094337463379, 'learning_rate': 2.0346131471785923e-05, 'epoch': 1.78}\n",
      "{'loss': 3.218, 'grad_norm': 8.770113945007324, 'learning_rate': 2.033885980221059e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1148, 'grad_norm': 10.107931137084961, 'learning_rate': 2.0331588132635254e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1762, 'grad_norm': 10.90748405456543, 'learning_rate': 2.032431646305992e-05, 'epoch': 1.78}\n",
      "{'loss': 3.0186, 'grad_norm': 10.696901321411133, 'learning_rate': 2.0317044793484584e-05, 'epoch': 1.78}\n",
      "{'loss': 2.9834, 'grad_norm': 9.91097354888916, 'learning_rate': 2.030977312390925e-05, 'epoch': 1.78}\n",
      "{'loss': 3.0488, 'grad_norm': 10.135502815246582, 'learning_rate': 2.0302501454333918e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1629, 'grad_norm': 9.20289134979248, 'learning_rate': 2.029522978475858e-05, 'epoch': 1.78}\n",
      "{'loss': 3.0859, 'grad_norm': 10.627941131591797, 'learning_rate': 2.0287958115183248e-05, 'epoch': 1.78}\n",
      "{'loss': 3.2328, 'grad_norm': 9.498477935791016, 'learning_rate': 2.028068644560791e-05, 'epoch': 1.78}\n",
      "{'loss': 3.0941, 'grad_norm': 9.26157283782959, 'learning_rate': 2.027341477603258e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1969, 'grad_norm': 15.678302764892578, 'learning_rate': 2.0266143106457245e-05, 'epoch': 1.78}\n",
      "{'loss': 2.9426, 'grad_norm': 9.636754989624023, 'learning_rate': 2.025887143688191e-05, 'epoch': 1.78}\n",
      "{'loss': 3.0, 'grad_norm': 9.710369110107422, 'learning_rate': 2.0251599767306576e-05, 'epoch': 1.78}\n",
      "{'loss': 3.1363, 'grad_norm': 10.23440933227539, 'learning_rate': 2.0244328097731243e-05, 'epoch': 1.79}\n",
      "{'loss': 3.1656, 'grad_norm': 10.833575248718262, 'learning_rate': 2.0237056428155906e-05, 'epoch': 1.79}\n",
      "{'loss': 3.1648, 'grad_norm': 9.081842422485352, 'learning_rate': 2.0229784758580573e-05, 'epoch': 1.79}\n",
      "{'loss': 3.1611, 'grad_norm': 9.785628318786621, 'learning_rate': 2.0222513089005237e-05, 'epoch': 1.79}\n",
      "{'loss': 3.1146, 'grad_norm': 10.475909233093262, 'learning_rate': 2.0215241419429904e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0627, 'grad_norm': 10.669693946838379, 'learning_rate': 2.020796974985457e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0885, 'grad_norm': 9.761262893676758, 'learning_rate': 2.020069808027923e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0988, 'grad_norm': 9.549635887145996, 'learning_rate': 2.0193426410703897e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0803, 'grad_norm': 10.407240867614746, 'learning_rate': 2.0186154741128564e-05, 'epoch': 1.79}\n",
      " 60%|███████████████████▋             | 41000/68760 [7:35:45<4:21:46,  1.77it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.05it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.17s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.798247999999994, 'eval_rouge-2': 7.2341039999999985, 'eval_rouge-l': 26.366485999999995, 'eval_bleu-4': 0.03510069928277459, 'eval_runtime': 7.6973, 'eval_samples_per_second': 6.496, 'eval_steps_per_second': 0.52, 'epoch': 1.79}\n",
      " 60%|███████████████████▋             | 41000/68760 [7:35:53<4:21:46,  1.77it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.29s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-41000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0604, 'grad_norm': 10.135579109191895, 'learning_rate': 2.0178883071553228e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0658, 'grad_norm': 9.255101203918457, 'learning_rate': 2.0171611401977895e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0764, 'grad_norm': 9.990326881408691, 'learning_rate': 2.0164339732402558e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0619, 'grad_norm': 10.016630172729492, 'learning_rate': 2.0157068062827225e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0615, 'grad_norm': 9.577014923095703, 'learning_rate': 2.0149796393251892e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0381, 'grad_norm': 10.493847846984863, 'learning_rate': 2.0142524723676556e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0014, 'grad_norm': 9.877119064331055, 'learning_rate': 2.0135253054101222e-05, 'epoch': 1.79}\n",
      "{'loss': 3.1182, 'grad_norm': 10.22673511505127, 'learning_rate': 2.0127981384525886e-05, 'epoch': 1.79}\n",
      "{'loss': 3.1936, 'grad_norm': 9.994412422180176, 'learning_rate': 2.0120709714950553e-05, 'epoch': 1.79}\n",
      "{'loss': 3.1455, 'grad_norm': 13.633585929870605, 'learning_rate': 2.011343804537522e-05, 'epoch': 1.79}\n",
      "{'loss': 2.9281, 'grad_norm': 9.9166259765625, 'learning_rate': 2.0106166375799883e-05, 'epoch': 1.79}\n",
      "{'loss': 3.1117, 'grad_norm': 10.223675727844238, 'learning_rate': 2.009889470622455e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0572, 'grad_norm': 8.890706062316895, 'learning_rate': 2.0091623036649214e-05, 'epoch': 1.79}\n",
      "{'loss': 2.9686, 'grad_norm': 10.280023574829102, 'learning_rate': 2.008435136707388e-05, 'epoch': 1.79}\n",
      "{'loss': 3.0896, 'grad_norm': 9.672201156616211, 'learning_rate': 2.0077079697498547e-05, 'epoch': 1.8}\n",
      "{'loss': 3.2184, 'grad_norm': 10.392041206359863, 'learning_rate': 2.006980802792321e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0482, 'grad_norm': 9.95057487487793, 'learning_rate': 2.0062536358347878e-05, 'epoch': 1.8}\n",
      "{'loss': 3.1369, 'grad_norm': 9.77047348022461, 'learning_rate': 2.0055264688772545e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0818, 'grad_norm': 9.609014511108398, 'learning_rate': 2.0047993019197208e-05, 'epoch': 1.8}\n",
      "{'loss': 3.1525, 'grad_norm': 10.08421802520752, 'learning_rate': 2.0040721349621875e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0584, 'grad_norm': 10.116662979125977, 'learning_rate': 2.003344968004654e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0803, 'grad_norm': 10.343877792358398, 'learning_rate': 2.0026178010471206e-05, 'epoch': 1.8}\n",
      "{'loss': 3.1406, 'grad_norm': 9.168153762817383, 'learning_rate': 2.0018906340895872e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0414, 'grad_norm': 11.465043067932129, 'learning_rate': 2.0011634671320536e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0445, 'grad_norm': 10.321313858032227, 'learning_rate': 2.0004363001745203e-05, 'epoch': 1.8}\n",
      "{'loss': 3.1418, 'grad_norm': 11.669989585876465, 'learning_rate': 1.9997091332169866e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0027, 'grad_norm': 10.029656410217285, 'learning_rate': 1.9989819662594533e-05, 'epoch': 1.8}\n",
      "{'loss': 3.2109, 'grad_norm': 8.816363334655762, 'learning_rate': 1.99825479930192e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0725, 'grad_norm': 8.937751770019531, 'learning_rate': 1.9975276323443864e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0676, 'grad_norm': 10.787376403808594, 'learning_rate': 1.996800465386853e-05, 'epoch': 1.8}\n",
      "{'loss': 3.1084, 'grad_norm': 9.47629451751709, 'learning_rate': 1.9960732984293194e-05, 'epoch': 1.8}\n",
      "{'loss': 3.0973, 'grad_norm': 9.295183181762695, 'learning_rate': 1.995346131471786e-05, 'epoch': 1.8}\n",
      "{'loss': 3.2049, 'grad_norm': 8.933599472045898, 'learning_rate': 1.9946189645142528e-05, 'epoch': 1.8}\n",
      "{'loss': 3.1721, 'grad_norm': 9.409103393554688, 'learning_rate': 1.993891797556719e-05, 'epoch': 1.8}\n",
      "{'loss': 3.2664, 'grad_norm': 9.434344291687012, 'learning_rate': 1.9931646305991858e-05, 'epoch': 1.8}\n",
      "{'loss': 3.1494, 'grad_norm': 9.079303741455078, 'learning_rate': 1.9924374636416525e-05, 'epoch': 1.8}\n",
      "{'loss': 3.035, 'grad_norm': 9.384458541870117, 'learning_rate': 1.991710296684119e-05, 'epoch': 1.8}\n",
      "{'loss': 3.1527, 'grad_norm': 9.668991088867188, 'learning_rate': 1.9909831297265852e-05, 'epoch': 1.81}\n",
      "{'loss': 3.2576, 'grad_norm': 9.431105613708496, 'learning_rate': 1.990255962769052e-05, 'epoch': 1.81}\n",
      "{'loss': 3.1908, 'grad_norm': 11.103364944458008, 'learning_rate': 1.9895287958115183e-05, 'epoch': 1.81}\n",
      "{'loss': 3.0299, 'grad_norm': 9.032607078552246, 'learning_rate': 1.988801628853985e-05, 'epoch': 1.81}\n",
      "{'loss': 2.9406, 'grad_norm': 10.016870498657227, 'learning_rate': 1.9880744618964513e-05, 'epoch': 1.81}\n",
      "{'loss': 3.2492, 'grad_norm': 9.70106315612793, 'learning_rate': 1.987347294938918e-05, 'epoch': 1.81}\n",
      "{'loss': 3.1895, 'grad_norm': 9.259641647338867, 'learning_rate': 1.9866201279813847e-05, 'epoch': 1.81}\n",
      "{'loss': 3.2092, 'grad_norm': 10.76832389831543, 'learning_rate': 1.985892961023851e-05, 'epoch': 1.81}\n",
      "{'loss': 3.0703, 'grad_norm': 10.071659088134766, 'learning_rate': 1.9851657940663177e-05, 'epoch': 1.81}\n",
      "{'loss': 2.8248, 'grad_norm': 10.367197036743164, 'learning_rate': 1.984438627108784e-05, 'epoch': 1.81}\n",
      "{'loss': 3.0756, 'grad_norm': 10.262219429016113, 'learning_rate': 1.9837114601512508e-05, 'epoch': 1.81}\n",
      "{'loss': 3.2064, 'grad_norm': 9.147444725036621, 'learning_rate': 1.9829842931937174e-05, 'epoch': 1.81}\n",
      "{'loss': 3.0328, 'grad_norm': 11.923622131347656, 'learning_rate': 1.9822571262361838e-05, 'epoch': 1.81}\n",
      " 60%|███████████████████▉             | 41500/68760 [7:41:07<4:40:20,  1.62it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.94s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.563272, 'eval_rouge-2': 7.350222000000001, 'eval_rouge-l': 25.861144, 'eval_bleu-4': 0.035499537197667665, 'eval_runtime': 9.1132, 'eval_samples_per_second': 5.487, 'eval_steps_per_second': 0.439, 'epoch': 1.81}\n",
      " 60%|███████████████████▉             | 41500/68760 [7:41:16<4:40:20,  1.62it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.72s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-41500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 2.9504, 'grad_norm': 10.704790115356445, 'learning_rate': 1.9815299592786505e-05, 'epoch': 1.81}\n",
      "{'loss': 3.0844, 'grad_norm': 9.673072814941406, 'learning_rate': 1.980802792321117e-05, 'epoch': 1.81}\n",
      "{'loss': 3.2367, 'grad_norm': 8.957915306091309, 'learning_rate': 1.9800756253635835e-05, 'epoch': 1.81}\n",
      "{'loss': 3.0586, 'grad_norm': 9.803757667541504, 'learning_rate': 1.9793484584060502e-05, 'epoch': 1.81}\n",
      "{'loss': 3.1047, 'grad_norm': 10.410465240478516, 'learning_rate': 1.9786212914485166e-05, 'epoch': 1.81}\n",
      "{'loss': 3.1562, 'grad_norm': 9.728193283081055, 'learning_rate': 1.9778941244909833e-05, 'epoch': 1.81}\n",
      "{'loss': 3.1398, 'grad_norm': 9.45604419708252, 'learning_rate': 1.9771669575334496e-05, 'epoch': 1.81}\n",
      "{'loss': 3.1572, 'grad_norm': 9.818437576293945, 'learning_rate': 1.9764397905759163e-05, 'epoch': 1.81}\n",
      "{'loss': 3.1949, 'grad_norm': 8.815086364746094, 'learning_rate': 1.975712623618383e-05, 'epoch': 1.81}\n",
      "{'loss': 3.0805, 'grad_norm': 10.910063743591309, 'learning_rate': 1.9749854566608493e-05, 'epoch': 1.82}\n",
      "{'loss': 2.9813, 'grad_norm': 9.393909454345703, 'learning_rate': 1.974258289703316e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1318, 'grad_norm': 8.990840911865234, 'learning_rate': 1.9735311227457827e-05, 'epoch': 1.82}\n",
      "{'loss': 3.218, 'grad_norm': 9.529986381530762, 'learning_rate': 1.972803955788249e-05, 'epoch': 1.82}\n",
      "{'loss': 3.0453, 'grad_norm': 10.05351734161377, 'learning_rate': 1.9720767888307158e-05, 'epoch': 1.82}\n",
      "{'loss': 3.16, 'grad_norm': 10.111296653747559, 'learning_rate': 1.971349621873182e-05, 'epoch': 1.82}\n",
      "{'loss': 3.0986, 'grad_norm': 10.004762649536133, 'learning_rate': 1.9706224549156488e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1443, 'grad_norm': 9.18676471710205, 'learning_rate': 1.9698952879581155e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1447, 'grad_norm': 9.092251777648926, 'learning_rate': 1.969168121000582e-05, 'epoch': 1.82}\n",
      "{'loss': 3.0498, 'grad_norm': 9.656510353088379, 'learning_rate': 1.9684409540430485e-05, 'epoch': 1.82}\n",
      "{'loss': 3.2057, 'grad_norm': 10.522613525390625, 'learning_rate': 1.967713787085515e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1156, 'grad_norm': 11.646313667297363, 'learning_rate': 1.9669866201279816e-05, 'epoch': 1.82}\n",
      "{'loss': 3.167, 'grad_norm': 9.912555694580078, 'learning_rate': 1.9662594531704483e-05, 'epoch': 1.82}\n",
      "{'loss': 3.0945, 'grad_norm': 10.395981788635254, 'learning_rate': 1.9655322862129146e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1699, 'grad_norm': 9.29714584350586, 'learning_rate': 1.9648051192553813e-05, 'epoch': 1.82}\n",
      "{'loss': 3.2443, 'grad_norm': 12.04464054107666, 'learning_rate': 1.9640779522978476e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1049, 'grad_norm': 9.89228630065918, 'learning_rate': 1.9633507853403143e-05, 'epoch': 1.82}\n",
      "{'loss': 3.0805, 'grad_norm': 11.008605003356934, 'learning_rate': 1.962623618382781e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1777, 'grad_norm': 10.317583084106445, 'learning_rate': 1.961896451425247e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1252, 'grad_norm': 8.942093849182129, 'learning_rate': 1.9611692844677137e-05, 'epoch': 1.82}\n",
      "{'loss': 3.0953, 'grad_norm': 10.488566398620605, 'learning_rate': 1.9604421175101804e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1729, 'grad_norm': 10.686593055725098, 'learning_rate': 1.9597149505526468e-05, 'epoch': 1.82}\n",
      "{'loss': 3.0174, 'grad_norm': 10.730774879455566, 'learning_rate': 1.9589877835951135e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1662, 'grad_norm': 9.687840461730957, 'learning_rate': 1.95826061663758e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1412, 'grad_norm': 10.06093692779541, 'learning_rate': 1.9575334496800465e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1434, 'grad_norm': 9.681145668029785, 'learning_rate': 1.9568062827225132e-05, 'epoch': 1.83}\n",
      "{'loss': 3.0145, 'grad_norm': 8.143251419067383, 'learning_rate': 1.9560791157649795e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1486, 'grad_norm': 10.86771297454834, 'learning_rate': 1.9553519488074462e-05, 'epoch': 1.83}\n",
      "{'loss': 3.2639, 'grad_norm': 9.49129581451416, 'learning_rate': 1.954624781849913e-05, 'epoch': 1.83}\n",
      "{'loss': 3.0061, 'grad_norm': 10.522050857543945, 'learning_rate': 1.9538976148923793e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1988, 'grad_norm': 10.697929382324219, 'learning_rate': 1.953170447934846e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1016, 'grad_norm': 10.763760566711426, 'learning_rate': 1.9524432809773123e-05, 'epoch': 1.83}\n",
      "{'loss': 2.9725, 'grad_norm': 8.89206600189209, 'learning_rate': 1.951716114019779e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1156, 'grad_norm': 9.27508544921875, 'learning_rate': 1.9509889470622457e-05, 'epoch': 1.83}\n",
      "{'loss': 3.0727, 'grad_norm': 9.726968765258789, 'learning_rate': 1.950261780104712e-05, 'epoch': 1.83}\n",
      "{'loss': 3.0779, 'grad_norm': 9.268962860107422, 'learning_rate': 1.9495346131471787e-05, 'epoch': 1.83}\n",
      "{'loss': 3.0707, 'grad_norm': 9.985790252685547, 'learning_rate': 1.948807446189645e-05, 'epoch': 1.83}\n",
      "{'loss': 3.241, 'grad_norm': 10.382650375366211, 'learning_rate': 1.9480802792321118e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1971, 'grad_norm': 9.72236156463623, 'learning_rate': 1.9473531122745785e-05, 'epoch': 1.83}\n",
      "{'loss': 3.0793, 'grad_norm': 8.845243453979492, 'learning_rate': 1.9466259453170448e-05, 'epoch': 1.83}\n",
      "{'loss': 3.2666, 'grad_norm': 9.148962020874023, 'learning_rate': 1.9458987783595115e-05, 'epoch': 1.83}\n",
      " 61%|████████████████████▏            | 42000/68760 [7:46:24<4:48:10,  1.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:01<00:01,  1.08it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.16s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.87809, 'eval_rouge-2': 7.342684, 'eval_rouge-l': 26.075392, 'eval_bleu-4': 0.03915228380898362, 'eval_runtime': 6.8182, 'eval_samples_per_second': 7.333, 'eval_steps_per_second': 0.587, 'epoch': 1.83}\n",
      " 61%|████████████████████▏            | 42000/68760 [7:46:31<4:48:10,  1.55it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.17s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-42000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0793, 'grad_norm': 9.915030479431152, 'learning_rate': 1.945171611401978e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1186, 'grad_norm': 10.540766716003418, 'learning_rate': 1.9444444444444445e-05, 'epoch': 1.83}\n",
      "{'loss': 3.0139, 'grad_norm': 9.583528518676758, 'learning_rate': 1.9437172774869112e-05, 'epoch': 1.83}\n",
      "{'loss': 3.1605, 'grad_norm': 10.462667465209961, 'learning_rate': 1.9429901105293776e-05, 'epoch': 1.83}\n",
      "{'loss': 3.2072, 'grad_norm': 8.928079605102539, 'learning_rate': 1.9422629435718443e-05, 'epoch': 1.83}\n",
      "{'loss': 3.0508, 'grad_norm': 11.556754112243652, 'learning_rate': 1.941535776614311e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1107, 'grad_norm': 9.361083984375, 'learning_rate': 1.9408086096567773e-05, 'epoch': 1.84}\n",
      "{'loss': 3.2021, 'grad_norm': 9.622964859008789, 'learning_rate': 1.940081442699244e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1152, 'grad_norm': 9.261640548706055, 'learning_rate': 1.9393542757417103e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1283, 'grad_norm': 8.864001274108887, 'learning_rate': 1.938627108784177e-05, 'epoch': 1.84}\n",
      "{'loss': 3.2084, 'grad_norm': 9.745057106018066, 'learning_rate': 1.9378999418266437e-05, 'epoch': 1.84}\n",
      "{'loss': 3.0445, 'grad_norm': 10.77286148071289, 'learning_rate': 1.93717277486911e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1895, 'grad_norm': 10.080697059631348, 'learning_rate': 1.9364456079115768e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1086, 'grad_norm': 9.30174446105957, 'learning_rate': 1.935718440954043e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1088, 'grad_norm': 10.94763469696045, 'learning_rate': 1.9349912739965098e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1646, 'grad_norm': 9.754315376281738, 'learning_rate': 1.9342641070389765e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1326, 'grad_norm': 10.025869369506836, 'learning_rate': 1.933536940081443e-05, 'epoch': 1.84}\n",
      "{'loss': 3.2133, 'grad_norm': 9.734004020690918, 'learning_rate': 1.9328097731239092e-05, 'epoch': 1.84}\n",
      "{'loss': 3.0664, 'grad_norm': 8.957274436950684, 'learning_rate': 1.932082606166376e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1146, 'grad_norm': 9.423234939575195, 'learning_rate': 1.9313554392088422e-05, 'epoch': 1.84}\n",
      "{'loss': 3.0465, 'grad_norm': 10.516722679138184, 'learning_rate': 1.930628272251309e-05, 'epoch': 1.84}\n",
      "{'loss': 3.0369, 'grad_norm': 9.660322189331055, 'learning_rate': 1.9299011052937753e-05, 'epoch': 1.84}\n",
      "{'loss': 3.0451, 'grad_norm': 10.287843704223633, 'learning_rate': 1.929173938336242e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1543, 'grad_norm': 10.390386581420898, 'learning_rate': 1.9284467713787087e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1566, 'grad_norm': 10.162744522094727, 'learning_rate': 1.927719604421175e-05, 'epoch': 1.84}\n",
      "{'loss': 3.2674, 'grad_norm': 10.368224143981934, 'learning_rate': 1.9269924374636417e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1834, 'grad_norm': 9.726962089538574, 'learning_rate': 1.9262652705061084e-05, 'epoch': 1.84}\n",
      "{'loss': 3.0166, 'grad_norm': 10.210282325744629, 'learning_rate': 1.9255381035485747e-05, 'epoch': 1.84}\n",
      "{'loss': 3.1672, 'grad_norm': 9.512686729431152, 'learning_rate': 1.9248109365910414e-05, 'epoch': 1.85}\n",
      "{'loss': 3.1887, 'grad_norm': 9.582444190979004, 'learning_rate': 1.9240837696335078e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0414, 'grad_norm': 8.958985328674316, 'learning_rate': 1.9233566026759745e-05, 'epoch': 1.85}\n",
      "{'loss': 3.1357, 'grad_norm': 9.715576171875, 'learning_rate': 1.922629435718441e-05, 'epoch': 1.85}\n",
      "{'loss': 3.2139, 'grad_norm': 9.841906547546387, 'learning_rate': 1.9219022687609075e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0379, 'grad_norm': 9.49246597290039, 'learning_rate': 1.9211751018033742e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0811, 'grad_norm': 10.79978084564209, 'learning_rate': 1.9204479348458405e-05, 'epoch': 1.85}\n",
      "{'loss': 3.1063, 'grad_norm': 10.400559425354004, 'learning_rate': 1.9197207678883072e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0652, 'grad_norm': 10.058513641357422, 'learning_rate': 1.918993600930774e-05, 'epoch': 1.85}\n",
      "{'loss': 3.1832, 'grad_norm': 9.641942977905273, 'learning_rate': 1.9182664339732403e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0896, 'grad_norm': 8.757647514343262, 'learning_rate': 1.917539267015707e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0557, 'grad_norm': 9.373647689819336, 'learning_rate': 1.9168121000581733e-05, 'epoch': 1.85}\n",
      "{'loss': 3.1479, 'grad_norm': 11.022321701049805, 'learning_rate': 1.91608493310064e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0287, 'grad_norm': 9.6248140335083, 'learning_rate': 1.9153577661431067e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0893, 'grad_norm': 9.87936019897461, 'learning_rate': 1.914630599185573e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0756, 'grad_norm': 10.51005744934082, 'learning_rate': 1.9139034322280397e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0936, 'grad_norm': 9.782630920410156, 'learning_rate': 1.913176265270506e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0625, 'grad_norm': 9.992986679077148, 'learning_rate': 1.9124490983129728e-05, 'epoch': 1.85}\n",
      "{'loss': 3.2504, 'grad_norm': 9.891976356506348, 'learning_rate': 1.9117219313554395e-05, 'epoch': 1.85}\n",
      "{'loss': 2.9811, 'grad_norm': 9.54056167602539, 'learning_rate': 1.9109947643979058e-05, 'epoch': 1.85}\n",
      "{'loss': 3.1113, 'grad_norm': 10.024481773376465, 'learning_rate': 1.9102675974403725e-05, 'epoch': 1.85}\n",
      "{'loss': 3.0988, 'grad_norm': 9.212894439697266, 'learning_rate': 1.9095404304828392e-05, 'epoch': 1.85}\n",
      " 62%|████████████████████▍            | 42500/68760 [7:51:39<4:33:14,  1.60it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.17s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.54s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 35.039798, 'eval_rouge-2': 8.671146, 'eval_rouge-l': 26.69891, 'eval_bleu-4': 0.0431203499690691, 'eval_runtime': 7.4809, 'eval_samples_per_second': 6.684, 'eval_steps_per_second': 0.535, 'epoch': 1.85}\n",
      " 62%|████████████████████▍            | 42500/68760 [7:51:47<4:33:14,  1.60it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.31s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-42500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 2.9936, 'grad_norm': 8.800536155700684, 'learning_rate': 1.9088132635253055e-05, 'epoch': 1.85}\n",
      "{'loss': 2.9965, 'grad_norm': 10.89215087890625, 'learning_rate': 1.9080860965677722e-05, 'epoch': 1.86}\n",
      "{'loss': 3.2102, 'grad_norm': 10.061484336853027, 'learning_rate': 1.9073589296102386e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0602, 'grad_norm': 9.765769004821777, 'learning_rate': 1.9066317626527053e-05, 'epoch': 1.86}\n",
      "{'loss': 3.183, 'grad_norm': 9.688000679016113, 'learning_rate': 1.905904595695172e-05, 'epoch': 1.86}\n",
      "{'loss': 3.1162, 'grad_norm': 9.750584602355957, 'learning_rate': 1.9051774287376383e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0525, 'grad_norm': 9.288629531860352, 'learning_rate': 1.904450261780105e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0187, 'grad_norm': 9.69735336303711, 'learning_rate': 1.9037230948225714e-05, 'epoch': 1.86}\n",
      "{'loss': 3.1279, 'grad_norm': 9.626562118530273, 'learning_rate': 1.9029959278650377e-05, 'epoch': 1.86}\n",
      "{'loss': 3.174, 'grad_norm': 9.504638671875, 'learning_rate': 1.9022687609075044e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0094, 'grad_norm': 10.27863597869873, 'learning_rate': 1.9015415939499707e-05, 'epoch': 1.86}\n",
      "{'loss': 3.058, 'grad_norm': 10.084044456481934, 'learning_rate': 1.9008144269924374e-05, 'epoch': 1.86}\n",
      "{'loss': 3.1818, 'grad_norm': 9.044267654418945, 'learning_rate': 1.900087260034904e-05, 'epoch': 1.86}\n",
      "{'loss': 2.9562, 'grad_norm': 9.706231117248535, 'learning_rate': 1.8993600930773705e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0738, 'grad_norm': 10.299616813659668, 'learning_rate': 1.898632926119837e-05, 'epoch': 1.86}\n",
      "{'loss': 3.1549, 'grad_norm': 11.190560340881348, 'learning_rate': 1.8979057591623035e-05, 'epoch': 1.86}\n",
      "{'loss': 3.1836, 'grad_norm': 9.703125, 'learning_rate': 1.8971785922047702e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0609, 'grad_norm': 9.551122665405273, 'learning_rate': 1.896451425247237e-05, 'epoch': 1.86}\n",
      "{'loss': 3.1225, 'grad_norm': 10.5304594039917, 'learning_rate': 1.8957242582897032e-05, 'epoch': 1.86}\n",
      "{'loss': 3.1527, 'grad_norm': 8.645353317260742, 'learning_rate': 1.89499709133217e-05, 'epoch': 1.86}\n",
      "{'loss': 3.3066, 'grad_norm': 9.662116050720215, 'learning_rate': 1.8942699243746366e-05, 'epoch': 1.86}\n",
      "{'loss': 3.1283, 'grad_norm': 10.030203819274902, 'learning_rate': 1.893542757417103e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0873, 'grad_norm': 10.021360397338867, 'learning_rate': 1.8928155904595697e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0812, 'grad_norm': 9.130393028259277, 'learning_rate': 1.892088423502036e-05, 'epoch': 1.86}\n",
      "{'loss': 3.0908, 'grad_norm': 11.478049278259277, 'learning_rate': 1.8913612565445027e-05, 'epoch': 1.87}\n",
      "{'loss': 3.351, 'grad_norm': 11.284988403320312, 'learning_rate': 1.8906340895869694e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0869, 'grad_norm': 12.214599609375, 'learning_rate': 1.8899069226294357e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1795, 'grad_norm': 9.427168846130371, 'learning_rate': 1.8891797556719024e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1182, 'grad_norm': 10.10565185546875, 'learning_rate': 1.8884525887143688e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0619, 'grad_norm': 9.466163635253906, 'learning_rate': 1.8877254217568355e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1402, 'grad_norm': 8.667940139770508, 'learning_rate': 1.886998254799302e-05, 'epoch': 1.87}\n",
      "{'loss': 3.123, 'grad_norm': 9.313858985900879, 'learning_rate': 1.8862710878417685e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0625, 'grad_norm': 10.010824203491211, 'learning_rate': 1.8855439208842352e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0773, 'grad_norm': 10.381247520446777, 'learning_rate': 1.8848167539267016e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0762, 'grad_norm': 9.337430000305176, 'learning_rate': 1.8840895869691682e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1834, 'grad_norm': 8.68755054473877, 'learning_rate': 1.883362420011635e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0936, 'grad_norm': 10.447532653808594, 'learning_rate': 1.8826352530541013e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1977, 'grad_norm': 10.191720008850098, 'learning_rate': 1.881908086096568e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0955, 'grad_norm': 13.05984115600586, 'learning_rate': 1.8811809191390343e-05, 'epoch': 1.87}\n",
      "{'loss': 3.2145, 'grad_norm': 9.699068069458008, 'learning_rate': 1.880453752181501e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1965, 'grad_norm': 9.247682571411133, 'learning_rate': 1.8797265852239677e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1443, 'grad_norm': 9.520380020141602, 'learning_rate': 1.878999418266434e-05, 'epoch': 1.87}\n",
      "{'loss': 2.926, 'grad_norm': 10.249632835388184, 'learning_rate': 1.8782722513089007e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0975, 'grad_norm': 10.740752220153809, 'learning_rate': 1.8775450843513674e-05, 'epoch': 1.87}\n",
      "{'loss': 3.0418, 'grad_norm': 8.851364135742188, 'learning_rate': 1.8768179173938338e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1387, 'grad_norm': 10.436379432678223, 'learning_rate': 1.8760907504363005e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1164, 'grad_norm': 10.407854080200195, 'learning_rate': 1.8753635834787668e-05, 'epoch': 1.87}\n",
      "{'loss': 3.1797, 'grad_norm': 9.915614128112793, 'learning_rate': 1.8746364165212332e-05, 'epoch': 1.88}\n",
      "{'loss': 3.123, 'grad_norm': 8.939014434814453, 'learning_rate': 1.8739092495637e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1736, 'grad_norm': 10.141654968261719, 'learning_rate': 1.8731820826061662e-05, 'epoch': 1.88}\n",
      " 63%|████████████████████▋            | 43000/68760 [7:56:57<4:41:16,  1.53it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.61s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:15<00:05,  5.90s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.480314, 'eval_rouge-2': 7.839848000000001, 'eval_rouge-l': 26.007616000000002, 'eval_bleu-4': 0.03691672036344184, 'eval_runtime': 18.5813, 'eval_samples_per_second': 2.691, 'eval_steps_per_second': 0.215, 'epoch': 1.88}\n",
      " 63%|████████████████████▋            | 43000/68760 [7:57:16<4:41:16,  1.53it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  4.07s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-43000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1568, 'grad_norm': 9.867818832397461, 'learning_rate': 1.872454915648633e-05, 'epoch': 1.88}\n",
      "{'loss': 3.2035, 'grad_norm': 9.495793342590332, 'learning_rate': 1.8717277486910996e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1898, 'grad_norm': 9.515690803527832, 'learning_rate': 1.871000581733566e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1736, 'grad_norm': 10.467326164245605, 'learning_rate': 1.8702734147760326e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1203, 'grad_norm': 9.845169067382812, 'learning_rate': 1.869546247818499e-05, 'epoch': 1.88}\n",
      "{'loss': 3.0871, 'grad_norm': 9.060699462890625, 'learning_rate': 1.8688190808609657e-05, 'epoch': 1.88}\n",
      "{'loss': 3.0611, 'grad_norm': 10.002634048461914, 'learning_rate': 1.8680919139034324e-05, 'epoch': 1.88}\n",
      "{'loss': 3.0861, 'grad_norm': 8.67721939086914, 'learning_rate': 1.8673647469458987e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1416, 'grad_norm': 11.214133262634277, 'learning_rate': 1.8666375799883654e-05, 'epoch': 1.88}\n",
      "{'loss': 3.0725, 'grad_norm': 9.27799129486084, 'learning_rate': 1.8659104130308318e-05, 'epoch': 1.88}\n",
      "{'loss': 3.084, 'grad_norm': 9.838119506835938, 'learning_rate': 1.8651832460732985e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1113, 'grad_norm': 9.653392791748047, 'learning_rate': 1.864456079115765e-05, 'epoch': 1.88}\n",
      "{'loss': 2.9969, 'grad_norm': 10.255785942077637, 'learning_rate': 1.8637289121582315e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1475, 'grad_norm': 9.552789688110352, 'learning_rate': 1.8630017452006982e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1139, 'grad_norm': 9.263540267944336, 'learning_rate': 1.862274578243165e-05, 'epoch': 1.88}\n",
      "{'loss': 3.3039, 'grad_norm': 10.569524765014648, 'learning_rate': 1.8615474112856312e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1482, 'grad_norm': 9.434337615966797, 'learning_rate': 1.860820244328098e-05, 'epoch': 1.88}\n",
      "{'loss': 2.9928, 'grad_norm': 9.144498825073242, 'learning_rate': 1.8600930773705643e-05, 'epoch': 1.88}\n",
      "{'loss': 3.0809, 'grad_norm': 10.967316627502441, 'learning_rate': 1.859365910413031e-05, 'epoch': 1.88}\n",
      "{'loss': 3.1348, 'grad_norm': 9.10483455657959, 'learning_rate': 1.8586387434554976e-05, 'epoch': 1.88}\n",
      "{'loss': 3.142, 'grad_norm': 9.707658767700195, 'learning_rate': 1.857911576497964e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0197, 'grad_norm': 9.409528732299805, 'learning_rate': 1.8571844095404307e-05, 'epoch': 1.89}\n",
      "{'loss': 3.1328, 'grad_norm': 9.976046562194824, 'learning_rate': 1.856457242582897e-05, 'epoch': 1.89}\n",
      "{'loss': 3.1453, 'grad_norm': 10.902667999267578, 'learning_rate': 1.8557300756253637e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0148, 'grad_norm': 16.818809509277344, 'learning_rate': 1.8550029086678304e-05, 'epoch': 1.89}\n",
      "{'loss': 3.1449, 'grad_norm': 9.744247436523438, 'learning_rate': 1.8542757417102968e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0107, 'grad_norm': 9.56456470489502, 'learning_rate': 1.8535485747527635e-05, 'epoch': 1.89}\n",
      "{'loss': 2.9494, 'grad_norm': 9.634692192077637, 'learning_rate': 1.8528214077952298e-05, 'epoch': 1.89}\n",
      "{'loss': 3.199, 'grad_norm': 9.876991271972656, 'learning_rate': 1.8520942408376965e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0793, 'grad_norm': 9.921636581420898, 'learning_rate': 1.8513670738801632e-05, 'epoch': 1.89}\n",
      "{'loss': 3.017, 'grad_norm': 10.285181045532227, 'learning_rate': 1.8506399069226295e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0307, 'grad_norm': 10.316567420959473, 'learning_rate': 1.8499127399650962e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0193, 'grad_norm': 10.192742347717285, 'learning_rate': 1.8491855730075626e-05, 'epoch': 1.89}\n",
      "{'loss': 3.1334, 'grad_norm': 10.22606372833252, 'learning_rate': 1.8484584060500293e-05, 'epoch': 1.89}\n",
      "{'loss': 3.1096, 'grad_norm': 9.545713424682617, 'learning_rate': 1.847731239092496e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0824, 'grad_norm': 9.82097053527832, 'learning_rate': 1.8470040721349623e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0658, 'grad_norm': 10.262948989868164, 'learning_rate': 1.846276905177429e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0785, 'grad_norm': 10.897836685180664, 'learning_rate': 1.8455497382198953e-05, 'epoch': 1.89}\n",
      "{'loss': 3.29, 'grad_norm': 10.43195629119873, 'learning_rate': 1.8448225712623617e-05, 'epoch': 1.89}\n",
      "{'loss': 3.1969, 'grad_norm': 10.436707496643066, 'learning_rate': 1.8440954043048284e-05, 'epoch': 1.89}\n",
      "{'loss': 3.1721, 'grad_norm': 9.450493812561035, 'learning_rate': 1.843368237347295e-05, 'epoch': 1.89}\n",
      "{'loss': 3.0514, 'grad_norm': 9.38175106048584, 'learning_rate': 1.8426410703897614e-05, 'epoch': 1.89}\n",
      "{'loss': 3.2814, 'grad_norm': 9.14855670928955, 'learning_rate': 1.841913903432228e-05, 'epoch': 1.89}\n",
      "{'loss': 3.2084, 'grad_norm': 9.902119636535645, 'learning_rate': 1.8411867364746945e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1797, 'grad_norm': 9.902483940124512, 'learning_rate': 1.840459569517161e-05, 'epoch': 1.9}\n",
      "{'loss': 3.0551, 'grad_norm': 8.54703426361084, 'learning_rate': 1.839732402559628e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1105, 'grad_norm': 9.651192665100098, 'learning_rate': 1.8390052356020942e-05, 'epoch': 1.9}\n",
      "{'loss': 3.0707, 'grad_norm': 10.020644187927246, 'learning_rate': 1.838278068644561e-05, 'epoch': 1.9}\n",
      "{'loss': 3.0697, 'grad_norm': 9.815464973449707, 'learning_rate': 1.8375509016870272e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1199, 'grad_norm': 9.745899200439453, 'learning_rate': 1.836823734729494e-05, 'epoch': 1.9}\n",
      " 63%|████████████████████▉            | 43500/68760 [8:02:24<4:02:48,  1.73it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.11s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.935326, 'eval_rouge-2': 8.427848, 'eval_rouge-l': 26.286941999999996, 'eval_bleu-4': 0.04124823181855723, 'eval_runtime': 7.298, 'eval_samples_per_second': 6.851, 'eval_steps_per_second': 0.548, 'epoch': 1.9}\n",
      " 63%|████████████████████▉            | 43500/68760 [8:02:32<4:02:48,  1.73it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.35s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-43500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1152, 'grad_norm': 10.658363342285156, 'learning_rate': 1.8360965677719606e-05, 'epoch': 1.9}\n",
      "{'loss': 3.049, 'grad_norm': 10.027247428894043, 'learning_rate': 1.835369400814427e-05, 'epoch': 1.9}\n",
      "{'loss': 3.191, 'grad_norm': 11.039209365844727, 'learning_rate': 1.8346422338568937e-05, 'epoch': 1.9}\n",
      "{'loss': 3.0693, 'grad_norm': 10.747442245483398, 'learning_rate': 1.83391506689936e-05, 'epoch': 1.9}\n",
      "{'loss': 3.0793, 'grad_norm': 12.050871849060059, 'learning_rate': 1.8331878999418267e-05, 'epoch': 1.9}\n",
      "{'loss': 3.0166, 'grad_norm': 10.74698543548584, 'learning_rate': 1.8324607329842934e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1037, 'grad_norm': 10.391205787658691, 'learning_rate': 1.8317335660267597e-05, 'epoch': 1.9}\n",
      "{'loss': 3.098, 'grad_norm': 11.223273277282715, 'learning_rate': 1.8310063990692264e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1705, 'grad_norm': 9.549127578735352, 'learning_rate': 1.830279232111693e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1203, 'grad_norm': 9.376974105834961, 'learning_rate': 1.8295520651541595e-05, 'epoch': 1.9}\n",
      "{'loss': 3.042, 'grad_norm': 8.89901065826416, 'learning_rate': 1.828824898196626e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1234, 'grad_norm': 9.483041763305664, 'learning_rate': 1.8280977312390925e-05, 'epoch': 1.9}\n",
      "{'loss': 3.0, 'grad_norm': 9.921138763427734, 'learning_rate': 1.8273705642815592e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1174, 'grad_norm': 10.061935424804688, 'learning_rate': 1.826643397324026e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1131, 'grad_norm': 14.089193344116211, 'learning_rate': 1.8259162303664922e-05, 'epoch': 1.9}\n",
      "{'loss': 3.134, 'grad_norm': 10.360344886779785, 'learning_rate': 1.825189063408959e-05, 'epoch': 1.9}\n",
      "{'loss': 3.1248, 'grad_norm': 10.657027244567871, 'learning_rate': 1.8244618964514253e-05, 'epoch': 1.91}\n",
      "{'loss': 3.2225, 'grad_norm': 10.506646156311035, 'learning_rate': 1.823734729493892e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1121, 'grad_norm': 10.169312477111816, 'learning_rate': 1.8230075625363587e-05, 'epoch': 1.91}\n",
      "{'loss': 3.0611, 'grad_norm': 10.330550193786621, 'learning_rate': 1.822280395578825e-05, 'epoch': 1.91}\n",
      "{'loss': 3.0869, 'grad_norm': 10.94478702545166, 'learning_rate': 1.8215532286212917e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1496, 'grad_norm': 9.653368949890137, 'learning_rate': 1.820826061663758e-05, 'epoch': 1.91}\n",
      "{'loss': 3.0854, 'grad_norm': 10.50574779510498, 'learning_rate': 1.8200988947062247e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1793, 'grad_norm': 10.39305305480957, 'learning_rate': 1.8193717277486914e-05, 'epoch': 1.91}\n",
      "{'loss': 3.0637, 'grad_norm': 10.235655784606934, 'learning_rate': 1.8186445607911578e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1658, 'grad_norm': 10.743850708007812, 'learning_rate': 1.8179173938336245e-05, 'epoch': 1.91}\n",
      "{'loss': 3.0793, 'grad_norm': 10.269248962402344, 'learning_rate': 1.8171902268760908e-05, 'epoch': 1.91}\n",
      "{'loss': 2.9559, 'grad_norm': 10.462728500366211, 'learning_rate': 1.816463059918557e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1059, 'grad_norm': 9.092769622802734, 'learning_rate': 1.815735892961024e-05, 'epoch': 1.91}\n",
      "{'loss': 3.0801, 'grad_norm': 9.477551460266113, 'learning_rate': 1.8150087260034902e-05, 'epoch': 1.91}\n",
      "{'loss': 2.9973, 'grad_norm': 9.910794258117676, 'learning_rate': 1.814281559045957e-05, 'epoch': 1.91}\n",
      "{'loss': 3.2391, 'grad_norm': 10.733383178710938, 'learning_rate': 1.8135543920884236e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1947, 'grad_norm': 9.81959342956543, 'learning_rate': 1.81282722513089e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1662, 'grad_norm': 10.247297286987305, 'learning_rate': 1.8121000581733566e-05, 'epoch': 1.91}\n",
      "{'loss': 3.2482, 'grad_norm': 9.265408515930176, 'learning_rate': 1.8113728912158233e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1051, 'grad_norm': 10.092131614685059, 'learning_rate': 1.8106457242582897e-05, 'epoch': 1.91}\n",
      "{'loss': 3.2037, 'grad_norm': 9.88962459564209, 'learning_rate': 1.8099185573007564e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1098, 'grad_norm': 9.479347229003906, 'learning_rate': 1.8091913903432227e-05, 'epoch': 1.91}\n",
      "{'loss': 3.1293, 'grad_norm': 9.081830024719238, 'learning_rate': 1.8084642233856894e-05, 'epoch': 1.91}\n",
      "{'loss': 3.0805, 'grad_norm': 10.027752876281738, 'learning_rate': 1.807737056428156e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1178, 'grad_norm': 9.244081497192383, 'learning_rate': 1.8070098894706224e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1223, 'grad_norm': 9.424147605895996, 'learning_rate': 1.806282722513089e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1035, 'grad_norm': 9.990304946899414, 'learning_rate': 1.8055555555555555e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1105, 'grad_norm': 9.693132400512695, 'learning_rate': 1.804828388598022e-05, 'epoch': 1.92}\n",
      "{'loss': 3.0926, 'grad_norm': 10.492337226867676, 'learning_rate': 1.804101221640489e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1953, 'grad_norm': 9.757856369018555, 'learning_rate': 1.8033740546829552e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1607, 'grad_norm': 10.1976957321167, 'learning_rate': 1.802646887725422e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1553, 'grad_norm': 11.340227127075195, 'learning_rate': 1.8019197207678882e-05, 'epoch': 1.92}\n",
      "{'loss': 3.0635, 'grad_norm': 10.23427963256836, 'learning_rate': 1.801192553810355e-05, 'epoch': 1.92}\n",
      "{'loss': 3.058, 'grad_norm': 10.022775650024414, 'learning_rate': 1.8004653868528216e-05, 'epoch': 1.92}\n",
      " 64%|█████████████████████            | 44000/68760 [8:07:40<4:14:19,  1.62it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.08s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:13<00:04,  4.19s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.990928, 'eval_rouge-2': 7.271859999999999, 'eval_rouge-l': 24.181496000000003, 'eval_bleu-4': 0.03293556665656842, 'eval_runtime': 27.2401, 'eval_samples_per_second': 1.836, 'eval_steps_per_second': 0.147, 'epoch': 1.92}\n",
      " 64%|█████████████████████            | 44000/68760 [8:08:07<4:14:19,  1.62it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:14<00:00,  3.05s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-44000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1732, 'grad_norm': 10.666216850280762, 'learning_rate': 1.799738219895288e-05, 'epoch': 1.92}\n",
      "{'loss': 3.2328, 'grad_norm': 9.576382637023926, 'learning_rate': 1.7990110529377547e-05, 'epoch': 1.92}\n",
      "{'loss': 3.165, 'grad_norm': 9.903491973876953, 'learning_rate': 1.7982838859802214e-05, 'epoch': 1.92}\n",
      "{'loss': 3.2074, 'grad_norm': 10.650381088256836, 'learning_rate': 1.7975567190226877e-05, 'epoch': 1.92}\n",
      "{'loss': 3.0758, 'grad_norm': 9.135968208312988, 'learning_rate': 1.7968295520651544e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1836, 'grad_norm': 9.671548843383789, 'learning_rate': 1.7961023851076207e-05, 'epoch': 1.92}\n",
      "{'loss': 3.0846, 'grad_norm': 9.680191993713379, 'learning_rate': 1.7953752181500874e-05, 'epoch': 1.92}\n",
      "{'loss': 3.1574, 'grad_norm': 9.449797630310059, 'learning_rate': 1.794648051192554e-05, 'epoch': 1.92}\n",
      "{'loss': 3.209, 'grad_norm': 9.54521369934082, 'learning_rate': 1.7939208842350205e-05, 'epoch': 1.92}\n",
      "{'loss': 3.0732, 'grad_norm': 8.9599027633667, 'learning_rate': 1.793193717277487e-05, 'epoch': 1.92}\n",
      "{'loss': 3.191, 'grad_norm': 9.937507629394531, 'learning_rate': 1.7924665503199535e-05, 'epoch': 1.92}\n",
      "{'loss': 3.0238, 'grad_norm': 8.979636192321777, 'learning_rate': 1.7917393833624202e-05, 'epoch': 1.92}\n",
      "{'loss': 3.075, 'grad_norm': 9.561760902404785, 'learning_rate': 1.791012216404887e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0076, 'grad_norm': 9.283122062683105, 'learning_rate': 1.7902850494473532e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0002, 'grad_norm': 9.714853286743164, 'learning_rate': 1.78955788248982e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0732, 'grad_norm': 10.936209678649902, 'learning_rate': 1.7888307155322863e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0227, 'grad_norm': 10.709447860717773, 'learning_rate': 1.788103548574753e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0777, 'grad_norm': 11.601508140563965, 'learning_rate': 1.7873763816172193e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0533, 'grad_norm': 9.861749649047852, 'learning_rate': 1.7866492146596857e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0193, 'grad_norm': 10.347587585449219, 'learning_rate': 1.7859220477021524e-05, 'epoch': 1.93}\n",
      "{'loss': 3.1049, 'grad_norm': 10.258408546447754, 'learning_rate': 1.785194880744619e-05, 'epoch': 1.93}\n",
      "{'loss': 3.1152, 'grad_norm': 9.809636116027832, 'learning_rate': 1.7844677137870854e-05, 'epoch': 1.93}\n",
      "{'loss': 3.1457, 'grad_norm': 10.419611930847168, 'learning_rate': 1.783740546829552e-05, 'epoch': 1.93}\n",
      "{'loss': 2.9635, 'grad_norm': 9.289690017700195, 'learning_rate': 1.7830133798720184e-05, 'epoch': 1.93}\n",
      "{'loss': 3.1977, 'grad_norm': 10.203011512756348, 'learning_rate': 1.782286212914485e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0119, 'grad_norm': 10.37612247467041, 'learning_rate': 1.7815590459569518e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0945, 'grad_norm': 10.747900009155273, 'learning_rate': 1.7808318789994182e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0293, 'grad_norm': 10.030257225036621, 'learning_rate': 1.780104712041885e-05, 'epoch': 1.93}\n",
      "{'loss': 3.2428, 'grad_norm': 13.757623672485352, 'learning_rate': 1.7793775450843516e-05, 'epoch': 1.93}\n",
      "{'loss': 3.2576, 'grad_norm': 10.378762245178223, 'learning_rate': 1.778650378126818e-05, 'epoch': 1.93}\n",
      "{'loss': 3.1271, 'grad_norm': 9.501083374023438, 'learning_rate': 1.7779232111692846e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0314, 'grad_norm': 10.804337501525879, 'learning_rate': 1.777196044211751e-05, 'epoch': 1.93}\n",
      "{'loss': 3.1822, 'grad_norm': 9.93420696258545, 'learning_rate': 1.7764688772542176e-05, 'epoch': 1.93}\n",
      "{'loss': 3.0309, 'grad_norm': 10.342753410339355, 'learning_rate': 1.7757417102966843e-05, 'epoch': 1.93}\n",
      "{'loss': 3.15, 'grad_norm': 10.966115951538086, 'learning_rate': 1.7750145433391507e-05, 'epoch': 1.93}\n",
      "{'loss': 3.1338, 'grad_norm': 10.976116180419922, 'learning_rate': 1.7742873763816174e-05, 'epoch': 1.94}\n",
      "{'loss': 3.2246, 'grad_norm': 16.729921340942383, 'learning_rate': 1.7735602094240837e-05, 'epoch': 1.94}\n",
      "{'loss': 3.1232, 'grad_norm': 10.15531063079834, 'learning_rate': 1.7728330424665504e-05, 'epoch': 1.94}\n",
      "{'loss': 3.0859, 'grad_norm': 9.26899528503418, 'learning_rate': 1.772105875509017e-05, 'epoch': 1.94}\n",
      "{'loss': 3.148, 'grad_norm': 9.895503997802734, 'learning_rate': 1.7713787085514834e-05, 'epoch': 1.94}\n",
      "{'loss': 3.0971, 'grad_norm': 11.214630126953125, 'learning_rate': 1.77065154159395e-05, 'epoch': 1.94}\n",
      "{'loss': 3.0791, 'grad_norm': 9.516005516052246, 'learning_rate': 1.7699243746364165e-05, 'epoch': 1.94}\n",
      "{'loss': 3.1779, 'grad_norm': 12.840641975402832, 'learning_rate': 1.7691972076788832e-05, 'epoch': 1.94}\n",
      "{'loss': 3.0576, 'grad_norm': 10.816844940185547, 'learning_rate': 1.76847004072135e-05, 'epoch': 1.94}\n",
      "{'loss': 3.2133, 'grad_norm': 9.94837474822998, 'learning_rate': 1.7677428737638162e-05, 'epoch': 1.94}\n",
      "{'loss': 3.24, 'grad_norm': 10.548574447631836, 'learning_rate': 1.767015706806283e-05, 'epoch': 1.94}\n",
      "{'loss': 3.1332, 'grad_norm': 9.134096145629883, 'learning_rate': 1.7662885398487496e-05, 'epoch': 1.94}\n",
      "{'loss': 3.1139, 'grad_norm': 10.69007396697998, 'learning_rate': 1.765561372891216e-05, 'epoch': 1.94}\n",
      "{'loss': 3.2828, 'grad_norm': 9.94559383392334, 'learning_rate': 1.7648342059336826e-05, 'epoch': 1.94}\n",
      "{'loss': 3.2154, 'grad_norm': 9.19250774383545, 'learning_rate': 1.764107038976149e-05, 'epoch': 1.94}\n",
      " 65%|█████████████████████▎           | 44500/68760 [8:13:22<4:12:08,  1.60it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.19s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:01,  1.99s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.179378, 'eval_rouge-2': 7.490526000000001, 'eval_rouge-l': 25.819854, 'eval_bleu-4': 0.03528294775116269, 'eval_runtime': 9.2388, 'eval_samples_per_second': 5.412, 'eval_steps_per_second': 0.433, 'epoch': 1.94}\n",
      " 65%|█████████████████████▎           | 44500/68760 [8:13:31<4:12:08,  1.60it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.69s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-44500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1178, 'grad_norm': 9.06867504119873, 'learning_rate': 1.7633798720186157e-05, 'epoch': 1.94}\n",
      "{'loss': 3.117, 'grad_norm': 9.294835090637207, 'learning_rate': 1.7626527050610824e-05, 'epoch': 1.94}\n",
      "{'loss': 3.0729, 'grad_norm': 9.86312198638916, 'learning_rate': 1.7619255381035487e-05, 'epoch': 1.94}\n",
      "{'loss': 3.2045, 'grad_norm': 11.066275596618652, 'learning_rate': 1.7611983711460154e-05, 'epoch': 1.94}\n",
      "{'loss': 3.2682, 'grad_norm': 9.716606140136719, 'learning_rate': 1.7604712041884818e-05, 'epoch': 1.94}\n",
      "{'loss': 3.1842, 'grad_norm': 9.878212928771973, 'learning_rate': 1.7597440372309484e-05, 'epoch': 1.94}\n",
      "{'loss': 3.1752, 'grad_norm': 9.482202529907227, 'learning_rate': 1.759016870273415e-05, 'epoch': 1.94}\n",
      "{'loss': 2.9771, 'grad_norm': 9.731196403503418, 'learning_rate': 1.758289703315881e-05, 'epoch': 1.95}\n",
      "{'loss': 3.2494, 'grad_norm': 9.892168045043945, 'learning_rate': 1.757562536358348e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1158, 'grad_norm': 9.152480125427246, 'learning_rate': 1.7568353694008145e-05, 'epoch': 1.95}\n",
      "{'loss': 3.209, 'grad_norm': 9.645503044128418, 'learning_rate': 1.756108202443281e-05, 'epoch': 1.95}\n",
      "{'loss': 3.0691, 'grad_norm': 9.348411560058594, 'learning_rate': 1.7553810354857476e-05, 'epoch': 1.95}\n",
      "{'loss': 3.0406, 'grad_norm': 10.225153923034668, 'learning_rate': 1.754653868528214e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1768, 'grad_norm': 11.282818794250488, 'learning_rate': 1.7539267015706806e-05, 'epoch': 1.95}\n",
      "{'loss': 3.242, 'grad_norm': 9.462472915649414, 'learning_rate': 1.7531995346131473e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1006, 'grad_norm': 11.156853675842285, 'learning_rate': 1.7524723676556136e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1707, 'grad_norm': 9.992295265197754, 'learning_rate': 1.7517452006980803e-05, 'epoch': 1.95}\n",
      "{'loss': 3.2566, 'grad_norm': 12.312530517578125, 'learning_rate': 1.7510180337405467e-05, 'epoch': 1.95}\n",
      "{'loss': 3.134, 'grad_norm': 9.582908630371094, 'learning_rate': 1.7502908667830134e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1838, 'grad_norm': 9.55628776550293, 'learning_rate': 1.74956369982548e-05, 'epoch': 1.95}\n",
      "{'loss': 2.9777, 'grad_norm': 10.203968048095703, 'learning_rate': 1.7488365328679464e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1539, 'grad_norm': 9.292247772216797, 'learning_rate': 1.748109365910413e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1111, 'grad_norm': 9.427026748657227, 'learning_rate': 1.7473821989528798e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1701, 'grad_norm': 10.11577320098877, 'learning_rate': 1.746655031995346e-05, 'epoch': 1.95}\n",
      "{'loss': 3.0615, 'grad_norm': 10.082982063293457, 'learning_rate': 1.745927865037813e-05, 'epoch': 1.95}\n",
      "{'loss': 3.2262, 'grad_norm': 10.528105735778809, 'learning_rate': 1.7452006980802792e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1068, 'grad_norm': 11.15483283996582, 'learning_rate': 1.744473531122746e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1225, 'grad_norm': 10.232690811157227, 'learning_rate': 1.7437463641652126e-05, 'epoch': 1.95}\n",
      "{'loss': 2.9742, 'grad_norm': 10.485060691833496, 'learning_rate': 1.743019197207679e-05, 'epoch': 1.95}\n",
      "{'loss': 3.0441, 'grad_norm': 9.613776206970215, 'learning_rate': 1.7422920302501456e-05, 'epoch': 1.95}\n",
      "{'loss': 3.1676, 'grad_norm': 9.270832061767578, 'learning_rate': 1.741564863292612e-05, 'epoch': 1.96}\n",
      "{'loss': 3.0537, 'grad_norm': 9.515448570251465, 'learning_rate': 1.7408376963350786e-05, 'epoch': 1.96}\n",
      "{'loss': 3.0795, 'grad_norm': 11.775094032287598, 'learning_rate': 1.7401105293775453e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1443, 'grad_norm': 10.419577598571777, 'learning_rate': 1.7393833624200117e-05, 'epoch': 1.96}\n",
      "{'loss': 3.0385, 'grad_norm': 9.497527122497559, 'learning_rate': 1.7386561954624784e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1328, 'grad_norm': 11.065940856933594, 'learning_rate': 1.7379290285049447e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1738, 'grad_norm': 10.285722732543945, 'learning_rate': 1.7372018615474114e-05, 'epoch': 1.96}\n",
      "{'loss': 3.0014, 'grad_norm': 9.512945175170898, 'learning_rate': 1.736474694589878e-05, 'epoch': 1.96}\n",
      "{'loss': 3.2439, 'grad_norm': 9.099139213562012, 'learning_rate': 1.7357475276323445e-05, 'epoch': 1.96}\n",
      "{'loss': 3.2184, 'grad_norm': 9.391719818115234, 'learning_rate': 1.735020360674811e-05, 'epoch': 1.96}\n",
      "{'loss': 3.0297, 'grad_norm': 10.14862060546875, 'learning_rate': 1.734293193717278e-05, 'epoch': 1.96}\n",
      "{'loss': 3.2371, 'grad_norm': 9.495037078857422, 'learning_rate': 1.7335660267597442e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1471, 'grad_norm': 8.998289108276367, 'learning_rate': 1.732838859802211e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1703, 'grad_norm': 10.464987754821777, 'learning_rate': 1.7321116928446772e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1824, 'grad_norm': 10.424250602722168, 'learning_rate': 1.731384525887144e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1084, 'grad_norm': 9.310580253601074, 'learning_rate': 1.7306573589296106e-05, 'epoch': 1.96}\n",
      "{'loss': 3.2047, 'grad_norm': 10.064249992370605, 'learning_rate': 1.729930191972077e-05, 'epoch': 1.96}\n",
      "{'loss': 3.0154, 'grad_norm': 9.738422393798828, 'learning_rate': 1.7292030250145433e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1295, 'grad_norm': 10.892593383789062, 'learning_rate': 1.72847585805701e-05, 'epoch': 1.96}\n",
      "{'loss': 3.073, 'grad_norm': 10.373056411743164, 'learning_rate': 1.7277486910994763e-05, 'epoch': 1.96}\n",
      " 65%|█████████████████████▌           | 45000/68760 [8:18:41<4:13:17,  1.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.11s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:03<00:01,  1.28s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.047554, 'eval_rouge-2': 7.967553999999999, 'eval_rouge-l': 27.040477999999997, 'eval_bleu-4': 0.037977049192286186, 'eval_runtime': 6.4907, 'eval_samples_per_second': 7.703, 'eval_steps_per_second': 0.616, 'epoch': 1.96}\n",
      " 65%|█████████████████████▌           | 45000/68760 [8:18:48<4:13:17,  1.56it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.20s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-45000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1104, 'grad_norm': 9.793022155761719, 'learning_rate': 1.727021524141943e-05, 'epoch': 1.96}\n",
      "{'loss': 3.0959, 'grad_norm': 13.484535217285156, 'learning_rate': 1.7262943571844094e-05, 'epoch': 1.96}\n",
      "{'loss': 3.2504, 'grad_norm': 9.683574676513672, 'learning_rate': 1.725567190226876e-05, 'epoch': 1.96}\n",
      "{'loss': 3.0408, 'grad_norm': 10.501545906066895, 'learning_rate': 1.7248400232693428e-05, 'epoch': 1.97}\n",
      "{'loss': 3.2648, 'grad_norm': 10.617559432983398, 'learning_rate': 1.724112856311809e-05, 'epoch': 1.97}\n",
      "{'loss': 2.9924, 'grad_norm': 9.857176780700684, 'learning_rate': 1.7233856893542758e-05, 'epoch': 1.97}\n",
      "{'loss': 3.0711, 'grad_norm': 9.907983779907227, 'learning_rate': 1.722658522396742e-05, 'epoch': 1.97}\n",
      "{'loss': 3.2244, 'grad_norm': 10.644243240356445, 'learning_rate': 1.721931355439209e-05, 'epoch': 1.97}\n",
      "{'loss': 3.0809, 'grad_norm': 11.160816192626953, 'learning_rate': 1.7212041884816755e-05, 'epoch': 1.97}\n",
      "{'loss': 3.1441, 'grad_norm': 9.807969093322754, 'learning_rate': 1.720477021524142e-05, 'epoch': 1.97}\n",
      "{'loss': 3.082, 'grad_norm': 10.723627090454102, 'learning_rate': 1.7197498545666086e-05, 'epoch': 1.97}\n",
      "{'loss': 3.0631, 'grad_norm': 9.301981925964355, 'learning_rate': 1.719022687609075e-05, 'epoch': 1.97}\n",
      "{'loss': 3.1111, 'grad_norm': 10.571334838867188, 'learning_rate': 1.7182955206515416e-05, 'epoch': 1.97}\n",
      "{'loss': 3.073, 'grad_norm': 9.917871475219727, 'learning_rate': 1.7175683536940083e-05, 'epoch': 1.97}\n",
      "{'loss': 3.0383, 'grad_norm': 10.154640197753906, 'learning_rate': 1.7168411867364747e-05, 'epoch': 1.97}\n",
      "{'loss': 3.0607, 'grad_norm': 10.86120319366455, 'learning_rate': 1.7161140197789413e-05, 'epoch': 1.97}\n",
      "{'loss': 3.1484, 'grad_norm': 10.097756385803223, 'learning_rate': 1.715386852821408e-05, 'epoch': 1.97}\n",
      "{'loss': 3.0539, 'grad_norm': 10.051061630249023, 'learning_rate': 1.7146596858638744e-05, 'epoch': 1.97}\n",
      "{'loss': 3.1914, 'grad_norm': 10.638662338256836, 'learning_rate': 1.713932518906341e-05, 'epoch': 1.97}\n",
      "{'loss': 3.1256, 'grad_norm': 10.452651977539062, 'learning_rate': 1.7132053519488074e-05, 'epoch': 1.97}\n",
      "{'loss': 3.1049, 'grad_norm': 9.360867500305176, 'learning_rate': 1.712478184991274e-05, 'epoch': 1.97}\n",
      "{'loss': 3.141, 'grad_norm': 9.848196029663086, 'learning_rate': 1.7117510180337408e-05, 'epoch': 1.97}\n",
      "{'loss': 3.151, 'grad_norm': 10.176950454711914, 'learning_rate': 1.711023851076207e-05, 'epoch': 1.97}\n",
      "{'loss': 3.1373, 'grad_norm': 10.019009590148926, 'learning_rate': 1.710296684118674e-05, 'epoch': 1.97}\n",
      "{'loss': 3.0043, 'grad_norm': 9.745408058166504, 'learning_rate': 1.7095695171611402e-05, 'epoch': 1.97}\n",
      "{'loss': 3.158, 'grad_norm': 10.226037979125977, 'learning_rate': 1.708842350203607e-05, 'epoch': 1.97}\n",
      "{'loss': 3.2408, 'grad_norm': 9.793625831604004, 'learning_rate': 1.7081151832460736e-05, 'epoch': 1.98}\n",
      "{'loss': 3.0854, 'grad_norm': 10.82060718536377, 'learning_rate': 1.70738801628854e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1543, 'grad_norm': 9.910937309265137, 'learning_rate': 1.7066608493310066e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1156, 'grad_norm': 11.265838623046875, 'learning_rate': 1.705933682373473e-05, 'epoch': 1.98}\n",
      "{'loss': 2.9559, 'grad_norm': 9.725924491882324, 'learning_rate': 1.7052065154159397e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1012, 'grad_norm': 9.854849815368652, 'learning_rate': 1.7044793484584063e-05, 'epoch': 1.98}\n",
      "{'loss': 3.0529, 'grad_norm': 9.329574584960938, 'learning_rate': 1.7037521815008727e-05, 'epoch': 1.98}\n",
      "{'loss': 3.2322, 'grad_norm': 8.874893188476562, 'learning_rate': 1.7030250145433394e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1463, 'grad_norm': 12.573076248168945, 'learning_rate': 1.7022978475858057e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1883, 'grad_norm': 9.838064193725586, 'learning_rate': 1.7015706806282724e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1023, 'grad_norm': 9.061456680297852, 'learning_rate': 1.700843513670739e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1074, 'grad_norm': 10.113088607788086, 'learning_rate': 1.7001163467132055e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1295, 'grad_norm': 9.612247467041016, 'learning_rate': 1.6993891797556718e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1879, 'grad_norm': 9.557117462158203, 'learning_rate': 1.6986620127981385e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1322, 'grad_norm': 10.645562171936035, 'learning_rate': 1.697934845840605e-05, 'epoch': 1.98}\n",
      "{'loss': 3.0758, 'grad_norm': 9.467853546142578, 'learning_rate': 1.6972076788830715e-05, 'epoch': 1.98}\n",
      "{'loss': 3.0049, 'grad_norm': 9.944239616394043, 'learning_rate': 1.6964805119255382e-05, 'epoch': 1.98}\n",
      "{'loss': 3.1211, 'grad_norm': 11.359864234924316, 'learning_rate': 1.6957533449680046e-05, 'epoch': 1.98}\n",
      "{'loss': 3.2516, 'grad_norm': 12.587451934814453, 'learning_rate': 1.6950261780104713e-05, 'epoch': 1.98}\n",
      "{'loss': 3.068, 'grad_norm': 9.800941467285156, 'learning_rate': 1.6942990110529376e-05, 'epoch': 1.98}\n",
      "{'loss': 3.235, 'grad_norm': 10.106294631958008, 'learning_rate': 1.6935718440954043e-05, 'epoch': 1.98}\n",
      "{'loss': 3.0973, 'grad_norm': 9.761497497558594, 'learning_rate': 1.692844677137871e-05, 'epoch': 1.98}\n",
      "{'loss': 3.0869, 'grad_norm': 9.922734260559082, 'learning_rate': 1.6921175101803374e-05, 'epoch': 1.98}\n",
      "{'loss': 3.0234, 'grad_norm': 9.540602684020996, 'learning_rate': 1.691390343222804e-05, 'epoch': 1.99}\n",
      " 66%|█████████████████████▊           | 45500/68760 [8:23:59<4:06:33,  1.57it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.11s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:04<00:01,  1.63s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.543354, 'eval_rouge-2': 8.007832, 'eval_rouge-l': 26.503188000000005, 'eval_bleu-4': 0.040246463989031034, 'eval_runtime': 7.696, 'eval_samples_per_second': 6.497, 'eval_steps_per_second': 0.52, 'epoch': 1.99}\n",
      " 66%|█████████████████████▊           | 45500/68760 [8:24:07<4:06:33,  1.57it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:05<00:00,  1.38s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-45500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.0268, 'grad_norm': 9.589637756347656, 'learning_rate': 1.6906631762652704e-05, 'epoch': 1.99}\n",
      "{'loss': 3.1105, 'grad_norm': 9.689249038696289, 'learning_rate': 1.689936009307737e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0334, 'grad_norm': 9.025501251220703, 'learning_rate': 1.6892088423502038e-05, 'epoch': 1.99}\n",
      "{'loss': 3.1236, 'grad_norm': 10.519742012023926, 'learning_rate': 1.68848167539267e-05, 'epoch': 1.99}\n",
      "{'loss': 3.1914, 'grad_norm': 11.04782772064209, 'learning_rate': 1.6877545084351368e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0811, 'grad_norm': 10.05903434753418, 'learning_rate': 1.687027341477603e-05, 'epoch': 1.99}\n",
      "{'loss': 3.1721, 'grad_norm': 9.341202735900879, 'learning_rate': 1.68630017452007e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0352, 'grad_norm': 10.12215518951416, 'learning_rate': 1.6855730075625365e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0168, 'grad_norm': 11.565537452697754, 'learning_rate': 1.684845840605003e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0277, 'grad_norm': 10.2109375, 'learning_rate': 1.6841186736474696e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0127, 'grad_norm': 9.845589637756348, 'learning_rate': 1.6833915066899363e-05, 'epoch': 1.99}\n",
      "{'loss': 3.1445, 'grad_norm': 11.56224250793457, 'learning_rate': 1.6826643397324026e-05, 'epoch': 1.99}\n",
      "{'loss': 3.2209, 'grad_norm': 9.675010681152344, 'learning_rate': 1.6819371727748693e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0955, 'grad_norm': 10.538835525512695, 'learning_rate': 1.6812100058173357e-05, 'epoch': 1.99}\n",
      "{'loss': 3.05, 'grad_norm': 9.500641822814941, 'learning_rate': 1.6804828388598024e-05, 'epoch': 1.99}\n",
      "{'loss': 3.1734, 'grad_norm': 9.86366081237793, 'learning_rate': 1.679755671902269e-05, 'epoch': 1.99}\n",
      "{'loss': 3.2188, 'grad_norm': 15.293664932250977, 'learning_rate': 1.6790285049447354e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0346, 'grad_norm': 10.416304588317871, 'learning_rate': 1.678301337987202e-05, 'epoch': 1.99}\n",
      "{'loss': 3.135, 'grad_norm': 9.550230026245117, 'learning_rate': 1.6775741710296684e-05, 'epoch': 1.99}\n",
      "{'loss': 3.1355, 'grad_norm': 10.053777694702148, 'learning_rate': 1.676847004072135e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0742, 'grad_norm': 10.239204406738281, 'learning_rate': 1.6761198371146018e-05, 'epoch': 1.99}\n",
      "{'loss': 3.0494, 'grad_norm': 10.929973602294922, 'learning_rate': 1.675392670157068e-05, 'epoch': 1.99}\n",
      "{'loss': 3.141, 'grad_norm': 13.178019523620605, 'learning_rate': 1.674665503199535e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0068, 'grad_norm': 10.27924919128418, 'learning_rate': 1.6739383362420012e-05, 'epoch': 2.0}\n",
      "{'loss': 3.1291, 'grad_norm': 9.441869735717773, 'learning_rate': 1.673211169284468e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0896, 'grad_norm': 9.849153518676758, 'learning_rate': 1.6724840023269346e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0135, 'grad_norm': 9.281451225280762, 'learning_rate': 1.671756835369401e-05, 'epoch': 2.0}\n",
      "{'loss': 3.1506, 'grad_norm': 10.229608535766602, 'learning_rate': 1.6710296684118673e-05, 'epoch': 2.0}\n",
      "{'loss': 3.2166, 'grad_norm': 10.642229080200195, 'learning_rate': 1.670302501454334e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0971, 'grad_norm': 10.70328426361084, 'learning_rate': 1.6695753344968003e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0824, 'grad_norm': 10.692634582519531, 'learning_rate': 1.668848167539267e-05, 'epoch': 2.0}\n",
      "{'loss': 3.1074, 'grad_norm': 10.649446487426758, 'learning_rate': 1.6681210005817337e-05, 'epoch': 2.0}\n",
      "{'loss': 3.141, 'grad_norm': 10.037074089050293, 'learning_rate': 1.6673938336242e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0064, 'grad_norm': 12.077834129333496, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'loss': 2.9918, 'grad_norm': 9.318371772766113, 'learning_rate': 1.665939499709133e-05, 'epoch': 2.0}\n",
      "{'loss': 3.2598, 'grad_norm': 10.000113487243652, 'learning_rate': 1.6652123327515998e-05, 'epoch': 2.0}\n",
      "{'loss': 3.1277, 'grad_norm': 10.19777774810791, 'learning_rate': 1.6644851657940665e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0541, 'grad_norm': 9.282134056091309, 'learning_rate': 1.6637579988365328e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0482, 'grad_norm': 11.25270938873291, 'learning_rate': 1.6630308318789995e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0773, 'grad_norm': 10.53459358215332, 'learning_rate': 1.662303664921466e-05, 'epoch': 2.0}\n",
      "{'loss': 2.9619, 'grad_norm': 10.223865509033203, 'learning_rate': 1.6615764979639326e-05, 'epoch': 2.0}\n",
      "{'loss': 3.1777, 'grad_norm': 10.489550590515137, 'learning_rate': 1.6608493310063992e-05, 'epoch': 2.0}\n",
      "{'loss': 3.1203, 'grad_norm': 10.388044357299805, 'learning_rate': 1.6601221640488656e-05, 'epoch': 2.0}\n",
      "{'loss': 3.1031, 'grad_norm': 10.20771598815918, 'learning_rate': 1.6593949970913323e-05, 'epoch': 2.0}\n",
      "{'loss': 3.2932, 'grad_norm': 10.204272270202637, 'learning_rate': 1.6586678301337986e-05, 'epoch': 2.0}\n",
      "{'loss': 3.0008, 'grad_norm': 10.14023494720459, 'learning_rate': 1.6579406631762653e-05, 'epoch': 2.01}\n",
      "{'loss': 3.0654, 'grad_norm': 9.516505241394043, 'learning_rate': 1.657213496218732e-05, 'epoch': 2.01}\n",
      "{'loss': 3.0033, 'grad_norm': 10.160837173461914, 'learning_rate': 1.6564863292611984e-05, 'epoch': 2.01}\n",
      "{'loss': 2.9078, 'grad_norm': 10.696428298950195, 'learning_rate': 1.655759162303665e-05, 'epoch': 2.01}\n",
      "{'loss': 3.1086, 'grad_norm': 12.226842880249023, 'learning_rate': 1.6550319953461314e-05, 'epoch': 2.01}\n",
      " 67%|██████████████████████           | 46000/68760 [8:29:21<3:59:36,  1.58it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.80s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.93s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.759042, 'eval_rouge-2': 8.417291999999998, 'eval_rouge-l': 25.432555999999998, 'eval_bleu-4': 0.0405247580283573, 'eval_runtime': 19.0001, 'eval_samples_per_second': 2.632, 'eval_steps_per_second': 0.211, 'epoch': 2.01}\n",
      " 67%|██████████████████████           | 46000/68760 [8:29:40<3:59:36,  1.58it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:06<00:00,  1.55s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-46000\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1107, 'grad_norm': 10.056709289550781, 'learning_rate': 1.654304828388598e-05, 'epoch': 2.01}\n",
      "{'loss': 3.0855, 'grad_norm': 9.561559677124023, 'learning_rate': 1.6535776614310648e-05, 'epoch': 2.01}\n",
      "{'loss': 3.1021, 'grad_norm': 9.50389289855957, 'learning_rate': 1.652850494473531e-05, 'epoch': 2.01}\n",
      "{'loss': 3.0367, 'grad_norm': 11.056468963623047, 'learning_rate': 1.6521233275159978e-05, 'epoch': 2.01}\n",
      "{'loss': 3.1148, 'grad_norm': 10.393241882324219, 'learning_rate': 1.6513961605584645e-05, 'epoch': 2.01}\n",
      "{'loss': 2.9928, 'grad_norm': 9.306492805480957, 'learning_rate': 1.650668993600931e-05, 'epoch': 2.01}\n",
      "{'loss': 3.1074, 'grad_norm': 9.83413028717041, 'learning_rate': 1.6499418266433976e-05, 'epoch': 2.01}\n",
      "{'loss': 3.0346, 'grad_norm': 9.63072395324707, 'learning_rate': 1.649214659685864e-05, 'epoch': 2.01}\n",
      "{'loss': 3.0658, 'grad_norm': 13.767951011657715, 'learning_rate': 1.6484874927283306e-05, 'epoch': 2.01}\n",
      "{'loss': 3.1277, 'grad_norm': 12.579597473144531, 'learning_rate': 1.6477603257707973e-05, 'epoch': 2.01}\n",
      "{'loss': 2.9779, 'grad_norm': 9.488197326660156, 'learning_rate': 1.6470331588132636e-05, 'epoch': 2.01}\n",
      "{'loss': 3.074, 'grad_norm': 9.98973560333252, 'learning_rate': 1.6463059918557303e-05, 'epoch': 2.01}\n",
      "{'loss': 3.1877, 'grad_norm': 10.411642074584961, 'learning_rate': 1.6455788248981967e-05, 'epoch': 2.01}\n",
      "{'loss': 3.2471, 'grad_norm': 10.258157730102539, 'learning_rate': 1.6448516579406634e-05, 'epoch': 2.01}\n",
      "{'loss': 3.1322, 'grad_norm': 8.96579647064209, 'learning_rate': 1.64412449098313e-05, 'epoch': 2.01}\n",
      "{'loss': 3.0562, 'grad_norm': 11.01329517364502, 'learning_rate': 1.6433973240255964e-05, 'epoch': 2.01}\n",
      "{'loss': 3.1533, 'grad_norm': 9.91422176361084, 'learning_rate': 1.642670157068063e-05, 'epoch': 2.01}\n",
      "{'loss': 3.0312, 'grad_norm': 11.344889640808105, 'learning_rate': 1.6419429901105294e-05, 'epoch': 2.01}\n",
      "{'loss': 3.2232, 'grad_norm': 9.873347282409668, 'learning_rate': 1.6412158231529958e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1252, 'grad_norm': 10.465629577636719, 'learning_rate': 1.6404886561954625e-05, 'epoch': 2.02}\n",
      "{'loss': 2.9787, 'grad_norm': 10.864067077636719, 'learning_rate': 1.639761489237929e-05, 'epoch': 2.02}\n",
      "{'loss': 3.2078, 'grad_norm': 10.4430570602417, 'learning_rate': 1.6390343222803955e-05, 'epoch': 2.02}\n",
      "{'loss': 3.0248, 'grad_norm': 9.525182723999023, 'learning_rate': 1.6383071553228622e-05, 'epoch': 2.02}\n",
      "{'loss': 3.09, 'grad_norm': 10.727151870727539, 'learning_rate': 1.6375799883653286e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1238, 'grad_norm': 9.892963409423828, 'learning_rate': 1.6368528214077953e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1545, 'grad_norm': 9.070403099060059, 'learning_rate': 1.636125654450262e-05, 'epoch': 2.02}\n",
      "{'loss': 3.0064, 'grad_norm': 10.796931266784668, 'learning_rate': 1.6353984874927283e-05, 'epoch': 2.02}\n",
      "{'loss': 3.0758, 'grad_norm': 9.669636726379395, 'learning_rate': 1.634671320535195e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1816, 'grad_norm': 10.79542064666748, 'learning_rate': 1.6339441535776613e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1002, 'grad_norm': 8.524417877197266, 'learning_rate': 1.633216986620128e-05, 'epoch': 2.02}\n",
      "{'loss': 3.2264, 'grad_norm': 9.438080787658691, 'learning_rate': 1.6324898196625947e-05, 'epoch': 2.02}\n",
      "{'loss': 3.0564, 'grad_norm': 11.145929336547852, 'learning_rate': 1.631762652705061e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1299, 'grad_norm': 10.875993728637695, 'learning_rate': 1.6310354857475278e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1703, 'grad_norm': 10.316885948181152, 'learning_rate': 1.630308318789994e-05, 'epoch': 2.02}\n",
      "{'loss': 3.092, 'grad_norm': 10.324285507202148, 'learning_rate': 1.6295811518324608e-05, 'epoch': 2.02}\n",
      "{'loss': 3.15, 'grad_norm': 9.69223690032959, 'learning_rate': 1.6288539848749275e-05, 'epoch': 2.02}\n",
      "{'loss': 3.0889, 'grad_norm': 9.874231338500977, 'learning_rate': 1.628126817917394e-05, 'epoch': 2.02}\n",
      "{'loss': 3.0322, 'grad_norm': 11.686020851135254, 'learning_rate': 1.6273996509598605e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1459, 'grad_norm': 11.399645805358887, 'learning_rate': 1.626672484002327e-05, 'epoch': 2.02}\n",
      "{'loss': 2.9855, 'grad_norm': 9.687768936157227, 'learning_rate': 1.6259453170447936e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1189, 'grad_norm': 10.783117294311523, 'learning_rate': 1.6252181500872603e-05, 'epoch': 2.02}\n",
      "{'loss': 3.1078, 'grad_norm': 10.175216674804688, 'learning_rate': 1.6244909831297266e-05, 'epoch': 2.03}\n",
      "{'loss': 3.1145, 'grad_norm': 9.667673110961914, 'learning_rate': 1.6237638161721933e-05, 'epoch': 2.03}\n",
      "{'loss': 2.9887, 'grad_norm': 10.207063674926758, 'learning_rate': 1.6230366492146596e-05, 'epoch': 2.03}\n",
      "{'loss': 3.1135, 'grad_norm': 9.800901412963867, 'learning_rate': 1.6223094822571263e-05, 'epoch': 2.03}\n",
      "{'loss': 3.0779, 'grad_norm': 10.709120750427246, 'learning_rate': 1.621582315299593e-05, 'epoch': 2.03}\n",
      "{'loss': 2.9545, 'grad_norm': 10.743332862854004, 'learning_rate': 1.6208551483420594e-05, 'epoch': 2.03}\n",
      "{'loss': 3.0693, 'grad_norm': 12.420190811157227, 'learning_rate': 1.620127981384526e-05, 'epoch': 2.03}\n",
      "{'loss': 3.1605, 'grad_norm': 10.827165603637695, 'learning_rate': 1.6194008144269928e-05, 'epoch': 2.03}\n",
      "{'loss': 3.0998, 'grad_norm': 10.007396697998047, 'learning_rate': 1.618673647469459e-05, 'epoch': 2.03}\n",
      " 68%|██████████████████████▎          | 46500/68760 [8:34:49<3:37:44,  1.70it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.09s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.45s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 34.516822, 'eval_rouge-2': 8.479163999999999, 'eval_rouge-l': 25.072993999999998, 'eval_bleu-4': 0.04240470782875238, 'eval_runtime': 27.8432, 'eval_samples_per_second': 1.796, 'eval_steps_per_second': 0.144, 'epoch': 2.03}\n",
      " 68%|██████████████████████▎          | 46500/68760 [8:35:17<3:37:44,  1.70it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:15<00:00,  3.20s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-46500\n",
      "/home/overman/miniconda3/envs/chatglm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/overman/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/06c7c873c843814171c51330b69c2e2a68e05178/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.1064, 'grad_norm': 9.624551773071289, 'learning_rate': 1.6179464805119258e-05, 'epoch': 2.03}\n",
      "{'loss': 3.0635, 'grad_norm': 9.447361946105957, 'learning_rate': 1.617219313554392e-05, 'epoch': 2.03}\n",
      "{'loss': 3.1936, 'grad_norm': 9.87831974029541, 'learning_rate': 1.616492146596859e-05, 'epoch': 2.03}\n",
      "{'loss': 3.1471, 'grad_norm': 9.230112075805664, 'learning_rate': 1.6157649796393255e-05, 'epoch': 2.03}\n",
      "{'loss': 3.2131, 'grad_norm': 11.437443733215332, 'learning_rate': 1.615037812681792e-05, 'epoch': 2.03}\n",
      "{'loss': 3.0428, 'grad_norm': 9.974896430969238, 'learning_rate': 1.6143106457242586e-05, 'epoch': 2.03}\n",
      "{'loss': 2.9508, 'grad_norm': 10.473822593688965, 'learning_rate': 1.613583478766725e-05, 'epoch': 2.03}\n",
      "{'loss': 3.1602, 'grad_norm': 10.881293296813965, 'learning_rate': 1.6128563118091913e-05, 'epoch': 2.03}\n",
      "{'loss': 3.0695, 'grad_norm': 10.311585426330566, 'learning_rate': 1.612129144851658e-05, 'epoch': 2.03}\n",
      "{'loss': 2.9664, 'grad_norm': 10.960185050964355, 'learning_rate': 1.6114019778941243e-05, 'epoch': 2.03}\n",
      "{'loss': 3.0479, 'grad_norm': 10.622002601623535, 'learning_rate': 1.610674810936591e-05, 'epoch': 2.03}\n",
      "{'loss': 3.2213, 'grad_norm': 9.742225646972656, 'learning_rate': 1.6099476439790577e-05, 'epoch': 2.03}\n",
      "{'loss': 3.0799, 'grad_norm': 10.285304069519043, 'learning_rate': 1.609220477021524e-05, 'epoch': 2.03}\n",
      " 68%|██████████████████████▍          | 46634/68760 [8:36:42<3:50:35,  1.60it/s]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python finetune_hf.py  data/AdvertiseGen_fix  THUDM/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060015c24e97ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5060015c24e97ae",
    "outputId": "d3f03d0d-46bf-4c74-9b00-dc0160da0e15"
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python inference_hf.py output/checkpoint-3000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "id": "18cd83087f096094",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
